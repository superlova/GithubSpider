{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch</th>\n",
       "      <th>sha</th>\n",
       "      <th>status</th>\n",
       "      <th>filename</th>\n",
       "      <th>parents_sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@ -224,12 +224,7 @@ def __init__(self,\\n     ...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@ -274,6 +274,37 @@ def test_progbar_logging(...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks_test.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@ -18,7 +18,6 @@\\n from __future__ import pri...</td>\n",
       "      <td>7d9299129c30405270813fe32f1310dbf3bab265</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/data/experimental/kernel_tes...</td>\n",
       "      <td>2c7d978a1446adaba32d8465aceef0b2c4a41cad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@ -0,0 +1,23 @@\\n+# Copyright 2019 The Tensor...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@ -33,7 +33,10 @@\\n from tensorflow.python.ke...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88281</th>\n",
       "      <td>@@ -342,15 +342,15 @@ def __init__(self,\\n    ...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/contrib/seq2seq/python/ops/attentio...</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88282</th>\n",
       "      <td>@@ -755,8 +755,8 @@ def sparse_softmax_cross_e...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/losses/losses_impl.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88283</th>\n",
       "      <td>@@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/nn_ops.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88284</th>\n",
       "      <td>@@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/kernel_tests/array_ops_test.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88285</th>\n",
       "      <td>@@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/array_ops.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88286 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patch  \\\n",
       "0      @@ -224,12 +224,7 @@ def __init__(self,\\n     ...   \n",
       "1      @@ -274,6 +274,37 @@ def test_progbar_logging(...   \n",
       "2      @@ -18,7 +18,6 @@\\n from __future__ import pri...   \n",
       "3      @@ -0,0 +1,23 @@\\n+# Copyright 2019 The Tensor...   \n",
       "4      @@ -33,7 +33,10 @@\\n from tensorflow.python.ke...   \n",
       "...                                                  ...   \n",
       "88281  @@ -342,15 +342,15 @@ def __init__(self,\\n    ...   \n",
       "88282  @@ -755,8 +755,8 @@ def sparse_softmax_cross_e...   \n",
       "88283  @@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...   \n",
       "88284  @@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...   \n",
       "88285  @@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...   \n",
       "\n",
       "                                            sha    status  \\\n",
       "0      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "1      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "2      7d9299129c30405270813fe32f1310dbf3bab265  modified   \n",
       "3      23ec33ae1418d56c3ce0294720782b636595a2b3     added   \n",
       "4      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
       "...                                         ...       ...   \n",
       "88281  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88282  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88283  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88284  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "88285  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "\n",
       "                                                filename  \\\n",
       "0                   tensorflow/python/keras/callbacks.py   \n",
       "1              tensorflow/python/keras/callbacks_test.py   \n",
       "2      tensorflow/python/data/experimental/kernel_tes...   \n",
       "3      tensorflow/python/keras/mixed_precision/experi...   \n",
       "4      tensorflow/python/keras/mixed_precision/experi...   \n",
       "...                                                  ...   \n",
       "88281  tensorflow/contrib/seq2seq/python/ops/attentio...   \n",
       "88282        tensorflow/python/ops/losses/losses_impl.py   \n",
       "88283                    tensorflow/python/ops/nn_ops.py   \n",
       "88284   tensorflow/python/kernel_tests/array_ops_test.py   \n",
       "88285                 tensorflow/python/ops/array_ops.py   \n",
       "\n",
       "                                    parents_sha  \n",
       "0      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "1      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "2      2c7d978a1446adaba32d8465aceef0b2c4a41cad  \n",
       "3      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "4      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "...                                         ...  \n",
       "88281  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88282  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88283  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88284  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "88285  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "\n",
       "[88286 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"tensorflow-tensorflow-shas.tar.bz2\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造url模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_template = \"https://api.github.com/repos/tensorflow/tensorflow/contents/{filename}?ref={sha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/data/experimental/kernel_tests/optimize_dataset_test.py?ref=7d9299129c30405270813fe32f1310dbf3bab265'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_template.format(filename=df.iloc[2]['filename'], sha=df.iloc[2]['sha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试获取相应内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(url, retry=5, params=None):\n",
    "    \"\"\"直接获取url的内容，Exception由外部处理\"\"\"\n",
    "    while retry > 0:\n",
    "        try:\n",
    "            r = requests.get(url, params, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            r.encoding = r.apparent_encoding\n",
    "            return r.text\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            logging.info(\"ReadTimeOut!\")\n",
    "            retry -= 1\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logging.info(\"ConnectionTimeOut!\")\n",
    "            retry -= 1\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            logging.info(\"HTTPError!, {}, {}\".format(type(e), str(e)))\n",
    "            retry -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfile(filename, sha):\n",
    "    url = contents_template.format(filename=filename, sha=sha)\n",
    "    text = get_html_content(url=url)\n",
    "    content = json.loads(text)\n",
    "    if content['encoding'] == 'base64':\n",
    "        decoded_content = base64.b64decode(content['content']).decode('utf8')\n",
    "    return decoded_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_file = getfile(filename=df.iloc[2]['filename'], sha=df.iloc[2]['sha'])\n",
    "parent_file = getfile(filename=df.iloc[2]['filename'], sha=df.iloc[2]['parents_sha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获得父节点的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempurl_2 = contents_template.format(filename=df.iloc[2]['filename'], sha=df.iloc[2]['parents_sha'])\n",
    "text = get_html_content(url=tempurl_2)\n",
    "content = json.loads(text)\n",
    "if content['encoding'] == 'base64':\n",
    "    decoded_content = base64.b64decode(content['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_file_lines = origin_file.splitlines()\n",
    "parent_file_lines = parent_file.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = difflib.Differ()\n",
    "diff = d.compare(parent_file_lines, origin_file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "  #\n",
      "  # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  # you may not use this file except in compliance with the License.\n",
      "  # You may obtain a copy of the License at\n",
      "  #\n",
      "  #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "  #\n",
      "  # Unless required by applicable law or agreed to in writing, software\n",
      "  # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  # See the License for the specific language governing permissions and\n",
      "  # limitations under the License.\n",
      "  # ==============================================================================\n",
      "  \"\"\"Tests for the private `_OptimizeDataset` transformation.\"\"\"\n",
      "  from __future__ import absolute_import\n",
      "  from __future__ import division\n",
      "  from __future__ import print_function\n",
      "  \n",
      "  import functools\n",
      "- import os\n",
      "  import warnings\n",
      "  \n",
      "  from absl.testing import parameterized\n",
      "  import numpy as np\n",
      "  \n",
      "  from tensorflow.python.data.experimental.ops import batching\n",
      "  from tensorflow.python.data.experimental.ops import grouping\n",
      "  from tensorflow.python.data.experimental.ops import optimization_options\n",
      "  from tensorflow.python.data.experimental.ops import scan_ops\n",
      "  from tensorflow.python.data.experimental.ops import testing\n",
      "  from tensorflow.python.data.experimental.ops import threadpool\n",
      "  from tensorflow.python.data.kernel_tests import test_base\n",
      "  from tensorflow.python.data.ops import dataset_ops\n",
      "  from tensorflow.python.framework import combinations\n",
      "  from tensorflow.python.framework import dtypes\n",
      "  from tensorflow.python.framework import errors\n",
      "  from tensorflow.python.framework import ops\n",
      "  from tensorflow.python.ops import array_ops\n",
      "  from tensorflow.python.ops import random_ops\n",
      "  from tensorflow.python.ops import variable_scope\n",
      "  from tensorflow.python.platform import test\n",
      "  \n",
      "  \n",
      "  def _captured_refvar_test_combinations():\n",
      "  \n",
      "    def make_map_dataset(var):\n",
      "      return dataset_ops.Dataset.from_tensors(0).map(lambda x: x + var)\n",
      "  \n",
      "    def make_flat_map_dataset(var):\n",
      "      return dataset_ops.Dataset.from_tensors(\n",
      "          0).flat_map(lambda _: dataset_ops.Dataset.from_tensors(var))\n",
      "  \n",
      "    def make_filter_dataset(var):\n",
      "      return dataset_ops.Dataset.from_tensors(0).filter(lambda x: x < var)\n",
      "  \n",
      "    def make_map_and_batch_dataset(var):\n",
      "  \n",
      "      def map_fn(x):\n",
      "        return x + var\n",
      "  \n",
      "      return dataset_ops.Dataset.from_tensors(0).apply(\n",
      "          batching.map_and_batch(map_fn, 1))\n",
      "  \n",
      "    def make_group_by_reducer_dataset(var):\n",
      "      reducer = grouping.Reducer(\n",
      "          init_func=lambda _: 0,\n",
      "          reduce_func=lambda x, y: x,\n",
      "          finalize_func=lambda _: var)\n",
      "      return dataset_ops.Dataset.range(5).apply(\n",
      "          grouping.group_by_reducer(lambda x: x % 2, reducer))\n",
      "  \n",
      "    def make_group_by_window_dataset(var):\n",
      "  \n",
      "      def reduce_fn(key, bucket):\n",
      "        del key, bucket\n",
      "        return dataset_ops.Dataset.from_tensors(var)\n",
      "  \n",
      "      return dataset_ops.Dataset.from_tensors(0).repeat(10).apply(\n",
      "          grouping.group_by_window(lambda _: 0, reduce_fn, 10))\n",
      "  \n",
      "    def make_scan_dataset(var):\n",
      "      return dataset_ops.Dataset.from_tensors(0).apply(\n",
      "          scan_ops.scan(\n",
      "              0, lambda old_state, elem: (old_state + 1, elem + old_state + var)))\n",
      "  \n",
      "    cases = [\n",
      "        # Core datasets\n",
      "        (\"Map\", make_map_dataset),\n",
      "        (\"FlatMap\", make_flat_map_dataset),\n",
      "        (\"Filter\", make_filter_dataset),\n",
      "        # Experimental datasets\n",
      "        (\"MapAndBatch\", make_map_and_batch_dataset),\n",
      "        (\"GroupByReducer\", make_group_by_reducer_dataset),\n",
      "        (\"GroupByWindow\", make_group_by_window_dataset),\n",
      "        (\"Scan\", make_scan_dataset)\n",
      "    ]\n",
      "  \n",
      "    def reduce_fn(x, y):\n",
      "      name, dataset_fn = y\n",
      "      return x + combinations.combine(\n",
      "          dataset_fn=combinations.NamedObject(name, dataset_fn))\n",
      "  \n",
      "    return functools.reduce(reduce_fn, cases, [])\n",
      "  \n",
      "  \n",
      "  def _disable_intra_op_parallelism_test_combinations():\n",
      "  \n",
      "    def make_tensor_dataset():\n",
      "      return dataset_ops.Dataset.from_tensors(42)\n",
      "  \n",
      "    def make_map_dataset():\n",
      "      return dataset_ops.Dataset.from_tensors(42).map(lambda x: x + 1)\n",
      "  \n",
      "    cases = [\n",
      "        (\"FromTensors\", make_tensor_dataset, [42]),\n",
      "        (\"Map\", make_map_dataset, [43]),\n",
      "    ]\n",
      "  \n",
      "    def reduce_fn(x, y):\n",
      "      name, dataset_fn, expected_output = y\n",
      "      return x + combinations.combine(\n",
      "          dataset_fn=combinations.NamedObject(name, dataset_fn),\n",
      "          expected_output=[expected_output])\n",
      "  \n",
      "    return functools.reduce(reduce_fn, cases, [])\n",
      "  \n",
      "  \n",
      "  class OptimizeDatasetTest(test_base.DatasetTestBase, parameterized.TestCase):\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationStatefulFunction(self):\n",
      "      dataset = dataset_ops.Dataset.range(\n",
      "          10).map(lambda _: random_ops.random_uniform([])).batch(10)\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      dataset = dataset.with_options(options)\n",
      "      get_next = self.getNext(dataset)\n",
      "      self.evaluate(get_next())\n",
      "  \n",
      "    # TODO(b/123902160)\n",
      "    @combinations.generate(test_base.graph_only_combinations())\n",
      "    def testOptimizationLargeInputFromTensor(self):\n",
      "      input_t = array_ops.placeholder(dtypes.int32, (None, None, None))\n",
      "      dataset = dataset_ops.Dataset.from_tensors(input_t)\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      dataset = dataset.with_options(options)\n",
      "      iterator = dataset_ops.make_initializable_iterator(dataset)\n",
      "      init_op = iterator.initializer\n",
      "      get_next = iterator.get_next()\n",
      "  \n",
      "      with self.cached_session() as sess:\n",
      "        sess.run(init_op, {input_t: np.ones([512, 1024, 1025], np.int32)})\n",
      "        self.evaluate(get_next)\n",
      "  \n",
      "    # TODO(b/123902160)\n",
      "    @combinations.generate(test_base.graph_only_combinations())\n",
      "    def testOptimizationLargeInputFromTensorSlices(self):\n",
      "      input_t = array_ops.placeholder(dtypes.int32, (None, None, None, None))\n",
      "      dataset = dataset_ops.Dataset.from_tensor_slices(input_t)\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      dataset = dataset.with_options(options)\n",
      "      iterator = dataset_ops.make_initializable_iterator(dataset)\n",
      "      init_op = iterator.initializer\n",
      "      get_next = iterator.get_next()\n",
      "  \n",
      "      with self.cached_session() as sess:\n",
      "        sess.run(init_op, {input_t: np.ones([1, 512, 1024, 1025], np.int32)})\n",
      "        self.evaluate(get_next)\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationNestedDataset(self):\n",
      "  \n",
      "      def flat_map_fn(_):\n",
      "        dataset = dataset_ops.Dataset.from_tensors(0)\n",
      "        dataset = dataset.apply(testing.assert_next([\"MemoryCacheImpl\"]))\n",
      "        dataset = dataset.skip(0)  # Should be removed by noop elimination\n",
      "        dataset = dataset.cache()\n",
      "        return dataset\n",
      "  \n",
      "      dataset = dataset_ops.Dataset.range(1)\n",
      "      dataset = dataset.flat_map(flat_map_fn)\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      options.experimental_optimization.noop_elimination = True\n",
      "      dataset = dataset.with_options(options)\n",
      "      self.assertDatasetProduces(dataset, expected_output=[0])\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationNestedDatasetWithModifiedRetval(self):\n",
      "  \n",
      "      def flat_map_fn(_):\n",
      "        dataset = dataset_ops.Dataset.from_tensors(0)\n",
      "        dataset = dataset.apply(testing.assert_next([\"MapAndBatch\"]))\n",
      "        # Should be fused by map and batch fusion\n",
      "        dataset = dataset.map(lambda x: x)\n",
      "        dataset = dataset.batch(1)\n",
      "        return dataset\n",
      "  \n",
      "      dataset = dataset_ops.Dataset.range(1)\n",
      "      dataset = dataset.flat_map(flat_map_fn)\n",
      "  \n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      options.experimental_optimization.map_and_batch_fusion = True\n",
      "      dataset = dataset.with_options(options)\n",
      "      self.assertDatasetProduces(dataset, expected_output=[[0]])\n",
      "  \n",
      "    @combinations.generate(\n",
      "        combinations.times(test_base.default_test_combinations(),\n",
      "                           _disable_intra_op_parallelism_test_combinations()))\n",
      "    def testOptimizationDisableIntraOpParallelism(self, dataset_fn,\n",
      "                                                  expected_output):\n",
      "-     os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"\n",
      "-     os.environ[\"TF_JOB_NAME\"] = \"test_job\"\n",
      "- \n",
      "      dataset = dataset_fn()\n",
      "      dataset = dataset.apply(testing.assert_next([\"MaxIntraOpParallelism\"]))\n",
      "  \n",
      "      self.assertDatasetProduces(dataset, expected_output=expected_output)\n",
      "- \n",
      "-     del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]\n",
      "-     del os.environ[\"TF_JOB_NAME\"]\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationThreadPoolDataset(self):\n",
      "      dataset = dataset_ops.Dataset.range(10).batch(10)\n",
      "  \n",
      "      dataset = threadpool.override_threadpool(\n",
      "          dataset,\n",
      "          threadpool.PrivateThreadPool(\n",
      "              2, display_name=\"private_thread_pool_%d\" % 2))\n",
      "  \n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      dataset = dataset.with_options(options)\n",
      "      self.assertDatasetProduces(\n",
      "          dataset,\n",
      "          expected_output=[list(range(10))],\n",
      "          requires_initialization=True)\n",
      "  \n",
      "    # Reference variables are not supported in eager mode.\n",
      "    @combinations.generate(\n",
      "        combinations.times(test_base.graph_only_combinations(),\n",
      "                           _captured_refvar_test_combinations()))\n",
      "    def testOptimizationWithCapturedRefVar(self, dataset_fn):\n",
      "      \"\"\"Tests that default optimizations are disabled with ref variables.\"\"\"\n",
      "      variable = variable_scope.get_variable(\n",
      "          \"v\", initializer=0, use_resource=False)\n",
      "      assign_op = variable.assign_add(1)\n",
      "  \n",
      "      # Check that warning is logged.\n",
      "      warnings.simplefilter(\"always\")\n",
      "      with warnings.catch_warnings(record=True) as w:\n",
      "        unoptimized_dataset = dataset_fn(variable)\n",
      "  \n",
      "        options = dataset_ops.Options()\n",
      "        options.experimental_optimization.apply_default_optimizations = False\n",
      "        options.experimental_optimization.noop_elimination = True\n",
      "        options.experimental_optimization.map_and_batch_fusion = True\n",
      "        optimized_dataset = unoptimized_dataset.with_options(options)\n",
      "        optimized_it = dataset_ops.make_initializable_iterator(optimized_dataset)\n",
      "  \n",
      "      self.assertGreaterEqual(len(w), 1)\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      expected = (\n",
      "          \"tf.data graph rewrites are not compatible with \"\n",
      "          \"tf.Variable. The following rewrites will be disabled: %s.\"\n",
      "          \" To enable rewrites, use resource variables instead by \"\n",
      "          \"calling `tf.enable_resource_variables()` at the start of the \"\n",
      "          \"program.\" %\n",
      "          (\", \".join(graph_rewrites.enabled + graph_rewrites.default)))\n",
      "      self.assertTrue(any(expected in str(warning) for warning in w))\n",
      "  \n",
      "      # Check that outputs are the same in the optimized and unoptimized cases,\n",
      "      # when the variable value is changing.\n",
      "      unoptimized_it = dataset_ops.make_initializable_iterator(\n",
      "          unoptimized_dataset)\n",
      "      with ops.control_dependencies([assign_op]):\n",
      "        unoptimized_output = unoptimized_it.get_next()\n",
      "        optimized_output = optimized_it.get_next()\n",
      "  \n",
      "      self.evaluate(variable.initializer)\n",
      "      self.evaluate((unoptimized_it.initializer, optimized_it.initializer))\n",
      "      while True:\n",
      "        try:\n",
      "          unoptimized, optimized = self.evaluate((unoptimized_output,\n",
      "                                                  optimized_output))\n",
      "          self.assertEqual(unoptimized, optimized)\n",
      "        except errors.OutOfRangeError:\n",
      "          break\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationDefault(self):\n",
      "      \"\"\"Tests the optimization settings by default.\"\"\"\n",
      "      options = dataset_ops.Options()\n",
      "      expected_optimizations_enabled = []\n",
      "      expected_optimizations_disabled = []\n",
      "      expected_optimizations_default = [\n",
      "          \"map_and_batch_fusion\",\n",
      "          \"noop_elimination\",\n",
      "          \"shuffle_and_repeat_fusion\",\n",
      "      ]\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      self.assertEqual(set(graph_rewrites.enabled),\n",
      "                       set(expected_optimizations_enabled))\n",
      "      self.assertEqual(set(graph_rewrites.disabled),\n",
      "                       set(expected_optimizations_disabled))\n",
      "      self.assertEqual(set(graph_rewrites.default),\n",
      "                       set(expected_optimizations_default))\n",
      "  \n",
      "      options.experimental_optimization.apply_default_optimizations = True\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      self.assertEqual(set(graph_rewrites.enabled),\n",
      "                       set(expected_optimizations_enabled))\n",
      "      self.assertEqual(set(graph_rewrites.disabled),\n",
      "                       set(expected_optimizations_disabled))\n",
      "      self.assertEqual(set(graph_rewrites.default),\n",
      "                       set(expected_optimizations_default))\n",
      "  \n",
      "      options.experimental_optimization.apply_default_optimizations = False\n",
      "      expected_optimizations_default = []\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      self.assertEqual(set(graph_rewrites.enabled),\n",
      "                       set(expected_optimizations_enabled))\n",
      "      self.assertEqual(set(graph_rewrites.disabled),\n",
      "                       set(expected_optimizations_disabled))\n",
      "      self.assertEqual(set(graph_rewrites.default),\n",
      "                       set(expected_optimizations_default))\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationEnabled(self):\n",
      "      \"\"\"Tests the optimization settings by enabling all.\"\"\"\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.filter_fusion = True\n",
      "      options.experimental_optimization.filter_with_random_uniform_fusion = True\n",
      "      options.experimental_optimization.hoist_random_uniform = True\n",
      "      options.experimental_optimization.map_and_batch_fusion = True\n",
      "      options.experimental_optimization.map_and_filter_fusion = True\n",
      "      options.experimental_optimization.map_parallelization = True\n",
      "      options.experimental_optimization.map_fusion = True\n",
      "      options.experimental_optimization.noop_elimination = True\n",
      "      options.experimental_optimization.parallel_batch = True\n",
      "      options.experimental_optimization.shuffle_and_repeat_fusion = True\n",
      "      options.experimental_optimization.map_vectorization.enabled = True\n",
      "      options.experimental_optimization.autotune_buffers = True\n",
      "      options.experimental_deterministic = False\n",
      "      options.experimental_stats.latency_all_edges = True\n",
      "      options.experimental_slack = True\n",
      "  \n",
      "      expected_optimizations_enabled = [\n",
      "          \"filter_fusion\",\n",
      "          \"filter_with_random_uniform_fusion\",\n",
      "          \"hoist_random_uniform\",\n",
      "          \"map_and_batch_fusion\",\n",
      "          \"map_and_filter_fusion\",\n",
      "          \"map_parallelization\",\n",
      "          \"map_fusion\",\n",
      "          \"noop_elimination\",\n",
      "          \"parallel_batch\",\n",
      "          \"shuffle_and_repeat_fusion\",\n",
      "          \"map_vectorization\",\n",
      "          \"inject_prefetch\",\n",
      "          \"make_sloppy\",\n",
      "          \"latency_all_edges\",\n",
      "          \"slack\",\n",
      "      ]\n",
      "      expected_optimizations_disabled = []\n",
      "      expected_optimizations_default = []\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      self.assertEqual(set(graph_rewrites.enabled),\n",
      "                       set(expected_optimizations_enabled))\n",
      "      self.assertEqual(set(graph_rewrites.disabled),\n",
      "                       set(expected_optimizations_disabled))\n",
      "      self.assertEqual(set(graph_rewrites.default),\n",
      "                       set(expected_optimizations_default))\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testOptimizationDisabled(self):\n",
      "      \"\"\"Tests the optimization settings by disabling all.\"\"\"\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.filter_fusion = False\n",
      "      options.experimental_optimization.filter_with_random_uniform_fusion = False\n",
      "      options.experimental_optimization.hoist_random_uniform = False\n",
      "      options.experimental_optimization.map_and_batch_fusion = False\n",
      "      options.experimental_optimization.map_and_filter_fusion = False\n",
      "      options.experimental_optimization.map_parallelization = False\n",
      "      options.experimental_optimization.map_fusion = False\n",
      "      options.experimental_optimization.noop_elimination = False\n",
      "      options.experimental_optimization.parallel_batch = False\n",
      "      options.experimental_optimization.shuffle_and_repeat_fusion = False\n",
      "      options.experimental_optimization.map_vectorization.enabled = False\n",
      "      options.experimental_optimization.autotune = False\n",
      "      options.experimental_deterministic = True\n",
      "      options.experimental_stats.latency_all_edges = False\n",
      "      options.experimental_slack = False\n",
      "  \n",
      "      expected_optimizations_enabled = []\n",
      "      expected_optimizations_disabled = [\n",
      "          \"filter_fusion\",\n",
      "          \"filter_with_random_uniform_fusion\",\n",
      "          \"hoist_random_uniform\",\n",
      "          \"map_and_batch_fusion\",\n",
      "          \"map_and_filter_fusion\",\n",
      "          \"map_parallelization\",\n",
      "          \"map_fusion\",\n",
      "          \"noop_elimination\",\n",
      "          \"parallel_batch\",\n",
      "          \"shuffle_and_repeat_fusion\",\n",
      "          \"map_vectorization\",\n",
      "          \"inject_prefetch\",\n",
      "          \"make_sloppy\",\n",
      "          \"latency_all_edges\",\n",
      "          \"slack\",\n",
      "      ]\n",
      "      expected_optimizations_default = []\n",
      "      graph_rewrites = options._graph_rewrites()\n",
      "      self.assertEqual(set(graph_rewrites.enabled),\n",
      "                       set(expected_optimizations_enabled))\n",
      "      self.assertEqual(set(graph_rewrites.disabled),\n",
      "                       set(expected_optimizations_disabled))\n",
      "      self.assertEqual(set(graph_rewrites.default),\n",
      "                       set(expected_optimizations_default))\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testAutotuningDefaults(self):\n",
      "      options = dataset_ops.Options()\n",
      "  \n",
      "      # Check defaults\n",
      "      autotune, algorithm, cpu_budget, ram_budget = options._autotune_settings()\n",
      "      self.assertTrue(autotune)\n",
      "      self.assertEqual(algorithm,\n",
      "                       optimization_options._AutotuneAlgorithm.HILL_CLIMB)\n",
      "      self.assertEqual(cpu_budget, 0)\n",
      "      self.assertEqual(ram_budget, 0)\n",
      "  \n",
      "    @combinations.generate(test_base.default_test_combinations())\n",
      "    def testAutotuningSettings(self):\n",
      "      options = dataset_ops.Options()\n",
      "      options.experimental_optimization.autotune_cpu_budget = 1000\n",
      "      options.experimental_optimization.autotune_ram_budget = 999999999\n",
      "      options.experimental_optimization.autotune_buffers = True\n",
      "      self.assertIn(\"inject_prefetch\", options._graph_rewrites().enabled)\n",
      "      autotune, algorithm, cpu_budget, ram_budget = options._autotune_settings()\n",
      "      self.assertTrue(autotune)\n",
      "      self.assertEqual(algorithm,\n",
      "                       optimization_options._AutotuneAlgorithm.GRADIENT_DESCENT)\n",
      "      self.assertEqual(cpu_budget, 1000)\n",
      "      self.assertEqual(ram_budget, 999999999)\n",
      "  \n",
      "  if __name__ == \"__main__\":\n",
      "    test.main()\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(list(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看原来变动paste，与比较结果进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -18,7 +18,6 @@\n",
      " from __future__ import print_function\n",
      " \n",
      " import functools\n",
      "-import os\n",
      " import warnings\n",
      " \n",
      " from absl.testing import parameterized\n",
      "@@ -213,17 +212,11 @@ def flat_map_fn(_):\n",
      "                          _disable_intra_op_parallelism_test_combinations()))\n",
      "   def testOptimizationDisableIntraOpParallelism(self, dataset_fn,\n",
      "                                                 expected_output):\n",
      "-    os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"\n",
      "-    os.environ[\"TF_JOB_NAME\"] = \"test_job\"\n",
      "-\n",
      "     dataset = dataset_fn()\n",
      "     dataset = dataset.apply(testing.assert_next([\"MaxIntraOpParallelism\"]))\n",
      " \n",
      "     self.assertDatasetProduces(dataset, expected_output=expected_output)\n",
      " \n",
      "-    del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]\n",
      "-    del os.environ[\"TF_JOB_NAME\"]\n",
      "-\n",
      "   @combinations.generate(test_base.default_test_combinations())\n",
      "   def testOptimizationThreadPoolDataset(self):\n",
      "     dataset = dataset_ops.Dataset.range(10).batch(10)\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[2]['patch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 筛选比较结果为-的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tesult = list(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_lines = []\n",
    "for line in temp_tesult:\n",
    "    if line.startswith('-') and len(line) > 2: # 排除空行，因为空行只包括两个符号：减号与空格\n",
    "        minus_lines.append(line.lstrip('- '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import os',\n",
       " 'os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"',\n",
       " 'os.environ[\"TF_JOB_NAME\"] = \"test_job\"',\n",
       " 'del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]',\n",
       " 'del os.environ[\"TF_JOB_NAME\"]']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minus_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍历df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ -224,12 +224,7 @@ def __init__(self,\n",
      "     if params:\n",
      "       self.set_params(params)\n",
      " \n",
      "-    self._queue_length = 10\n",
      "-    self._reset_batch_timing()\n",
      "-\n",
      "-    # Determines if batch-level hooks need to be called.\n",
      "-    # This is important for performance, because processing batch-level logs\n",
      "-    # will cause async eager to block on each batch.\n",
      "+    # Performance optimization: determines if batch hooks need to be called.\n",
      "     # pylint: disable=protected-access\n",
      "     self._should_call_train_batch_hooks = any(\n",
      "         cb._implements_train_batch_hooks() for cb in self.callbacks)\n",
      "@@ -239,6 +234,11 @@ def __init__(self,\n",
      "         cb._implements_predict_batch_hooks() for cb in self.callbacks)\n",
      "     # pylint: enable=protected-access\n",
      " \n",
      "+    # Performance check: Check batch hooks for slowness compared to batch time.\n",
      "+    self._timing = {}\n",
      "+    self._check_timing = False\n",
      "+    self._batch_start_time = None\n",
      "+\n",
      "   def _add_default_callbacks(self, add_history, add_progbar):\n",
      "     \"\"\"Adds `Callback`s that are always present.\"\"\"\n",
      "     self._progbar = None\n",
      "@@ -258,11 +258,6 @@ def _add_default_callbacks(self, add_history, add_progbar):\n",
      "       self._history = History()\n",
      "       self.callbacks.append(self._history)\n",
      " \n",
      "-  def _reset_batch_timing(self):\n",
      "-    self._delta_t_batch = 0.\n",
      "-    self._delta_ts = collections.defaultdict(\n",
      "-        lambda: collections.deque([], maxlen=self._queue_length))\n",
      "-\n",
      "   def append(self, callback):\n",
      "     self.callbacks.append(callback)\n",
      " \n",
      "@@ -282,33 +277,65 @@ def _call_batch_hook(self, mode, hook, batch, logs=None):\n",
      "     \"\"\"Helper function for all batch_{begin | end} methods.\"\"\"\n",
      "     if not self.callbacks:\n",
      "       return\n",
      "-    hook_name = 'on_{mode}_batch_{hook}'.format(mode=mode, hook=hook)\n",
      "+\n",
      "     if hook == 'begin':\n",
      "-      self._t_enter_batch = time.time()\n",
      "-    if hook == 'end':\n",
      "-      # Batch is ending, calculate batch time.\n",
      "-      self._delta_t_batch = time.time() - self._t_enter_batch\n",
      "+      self._call_batch_begin_hook(mode, batch, logs)\n",
      "+    elif hook == 'end':\n",
      "+      self._call_batch_end_hook(mode, batch, logs)\n",
      "+    else:\n",
      "+      raise ValueError('Unrecognized hook: {}'.format(hook))\n",
      "+\n",
      "+  def _call_batch_begin_hook(self, mode, batch, logs):\n",
      "+    \"\"\"Helper function for `on_*_batch_begin` methods.\"\"\"\n",
      "+    hook_name = 'on_{mode}_batch_begin'.format(mode=mode)\n",
      "+    self._check_timing = batch == 1 and hook_name not in self._timing\n",
      "+    self._call_batch_hook_helper(hook_name, batch, logs)\n",
      "+\n",
      "+    if self._check_timing:\n",
      "+      self._batch_start_time = time.time()\n",
      " \n",
      "+  def _call_batch_end_hook(self, mode, batch, logs):\n",
      "+    \"\"\"Helper function for `on_*_batch_end` methods.\"\"\"\n",
      "+    hook_name = 'on_{mode}_batch_end'.format(mode=mode)\n",
      "+\n",
      "+    if self._check_timing:\n",
      "+      batch_time = time.time() - self._batch_start_time\n",
      "+\n",
      "+    self._call_batch_hook_helper(hook_name, batch, logs)\n",
      "+\n",
      "+    if self._check_timing:\n",
      "+      end_hook_name = hook_name\n",
      "+      begin_hook_name = 'on_{mode}_batch_begin'.format(mode=mode)\n",
      "+\n",
      "+      threshold_time = 0.5 * batch_time\n",
      "+      warning_msg = ('Callbacks method `{hook}` is slow compared to '\n",
      "+                     'the batch time. Check your callbacks.')\n",
      "+      if self._timing[begin_hook_name] > threshold_time:\n",
      "+        logging.warning(warning_msg.format(hook=begin_hook_name))\n",
      "+      if self._timing[end_hook_name] > threshold_time:\n",
      "+        logging.warning(warning_msg.format(hook=end_hook_name))\n",
      "+\n",
      "+      self._check_timing = False\n",
      "+      self._batch_start_time = None\n",
      "+\n",
      "+  def _call_batch_hook_helper(self, hook_name, batch, logs):\n",
      "+    \"\"\"Helper function for `on_*_batch_*` methods.\"\"\"\n",
      "     logs = logs or {}\n",
      "-    t_before_callbacks = time.time()\n",
      "     numpy_logs = None\n",
      "+    if self._check_timing:\n",
      "+      start_time = time.time()\n",
      "+\n",
      "     for callback in self.callbacks:\n",
      "-      batch_hook = getattr(callback, hook_name)\n",
      "+      hook = getattr(callback, hook_name)\n",
      "       if getattr(callback, '_supports_tf_logs', False):\n",
      "-        batch_hook(batch, logs)\n",
      "+        hook(batch, logs)\n",
      "       else:\n",
      "         if numpy_logs is None:  # Only convert once.\n",
      "           numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n",
      "-        batch_hook(batch, numpy_logs)\n",
      "-    self._delta_ts[hook_name].append(time.time() - t_before_callbacks)\n",
      "+        hook(batch, numpy_logs)\n",
      " \n",
      "-    delta_t_median = np.median(self._delta_ts[hook_name])\n",
      "-    if (self._delta_t_batch > 0. and\n",
      "-        delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "-      logging.warning(\n",
      "-          'Method (%s) is slow compared '\n",
      "-          'to the batch update (%f). Check your callbacks.', hook_name,\n",
      "-          delta_t_median)\n",
      "+    if self._check_timing:\n",
      "+      self._timing[hook_name] = time.time() - start_time\n",
      " \n",
      "   def _call_begin_hook(self, mode):\n",
      "     \"\"\"Helper function for on_{train|test|predict}_begin methods.\"\"\"\n",
      "@@ -355,7 +382,6 @@ def on_epoch_begin(self, epoch, logs=None):\n",
      "         if numpy_logs is None:  # Only convert once.\n",
      "           numpy_logs = tf_utils.to_numpy_or_python_type(logs)\n",
      "         callback.on_epoch_begin(epoch, numpy_logs)\n",
      "-    self._reset_batch_timing()\n",
      " \n",
      "   def on_epoch_end(self, epoch, logs=None):\n",
      "     \"\"\"Calls the `on_epoch_end` methods of its callbacks.\n",
      "@@ -274,6 +274,37 @@ def test_progbar_logging(self):\n",
      "       model.fit(dataset, epochs=2, steps_per_epoch=10)\n",
      "       self.assertRegexpMatches(printed.contents(), expected_log)\n",
      " \n",
      "+  @keras_parameterized.run_all_keras_modes\n",
      "+  def test_callback_warning(self):\n",
      "+\n",
      "+    class SleepCallback(keras.callbacks.Callback):\n",
      "+\n",
      "+      def on_train_batch_end(self, batch, logs=None):\n",
      "+        time.sleep(1)\n",
      "+\n",
      "+    model = sequential.Sequential()\n",
      "+    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
      "+    model.compile(\n",
      "+        'sgd',\n",
      "+        loss='binary_crossentropy',\n",
      "+        run_eagerly=testing_utils.should_run_eagerly())\n",
      "+\n",
      "+    warning_messages = []\n",
      "+\n",
      "+    def warning(msg):\n",
      "+      warning_messages.append(msg)\n",
      "+\n",
      "+    with test.mock.patch.object(logging, 'warning', warning):\n",
      "+      model.fit(\n",
      "+          np.ones((10, 10), 'float32'),\n",
      "+          np.ones((10, 1), 'float32'),\n",
      "+          batch_size=5,\n",
      "+          epochs=10,\n",
      "+          callbacks=[SleepCallback()])\n",
      "+    warning_msg = ('Callbacks method `on_train_batch_end` is slow compared '\n",
      "+                   'to the batch time. Check your callbacks.')\n",
      "+    self.assertIn(warning_msg, warning_messages)\n",
      "+\n",
      "   @keras_parameterized.run_with_all_model_types(exclude_models='functional')\n",
      "   @keras_parameterized.run_all_keras_modes\n",
      "   def test_progbar_logging_deferred_model_build(self):\n",
      "@@ -18,7 +18,6 @@\n",
      " from __future__ import print_function\n",
      " \n",
      " import functools\n",
      "-import os\n",
      " import warnings\n",
      " \n",
      " from absl.testing import parameterized\n",
      "@@ -213,17 +212,11 @@ def flat_map_fn(_):\n",
      "                          _disable_intra_op_parallelism_test_combinations()))\n",
      "   def testOptimizationDisableIntraOpParallelism(self, dataset_fn,\n",
      "                                                 expected_output):\n",
      "-    os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"\n",
      "-    os.environ[\"TF_JOB_NAME\"] = \"test_job\"\n",
      "-\n",
      "     dataset = dataset_fn()\n",
      "     dataset = dataset.apply(testing.assert_next([\"MaxIntraOpParallelism\"]))\n",
      " \n",
      "     self.assertDatasetProduces(dataset, expected_output=expected_output)\n",
      " \n",
      "-    del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]\n",
      "-    del os.environ[\"TF_JOB_NAME\"]\n",
      "-\n",
      "   @combinations.generate(test_base.default_test_combinations())\n",
      "   def testOptimizationThreadPoolDataset(self):\n",
      "     dataset = dataset_ops.Dataset.range(10).batch(10)\n",
      "@@ -0,0 +1,23 @@\n",
      "+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+# ==============================================================================\n",
      "+\"\"\"Mixed precision API.\"\"\"\n",
      "+from __future__ import absolute_import\n",
      "+from __future__ import division\n",
      "+from __future__ import print_function\n",
      "+\n",
      "+from tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer import LossScaleOptimizer\n",
      "+from tensorflow.python.keras.mixed_precision.experimental.policy import global_policy\n",
      "+from tensorflow.python.keras.mixed_precision.experimental.policy import Policy\n",
      "+from tensorflow.python.keras.mixed_precision.experimental.policy import set_policy\n",
      "@@ -33,7 +33,10 @@\n",
      " from tensorflow.python.keras import models\n",
      " from tensorflow.python.keras import regularizers\n",
      " from tensorflow.python.keras.engine import base_layer\n",
      "+from tensorflow.python.keras.layers import core\n",
      "+from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer\n",
      " from tensorflow.python.keras.mixed_precision.experimental import policy\n",
      "+from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util\n",
      " from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
      " from tensorflow.python.ops import array_ops\n",
      " from tensorflow.python.ops import math_ops\n",
      "@@ -303,18 +306,24 @@ def loss_fn(y_true, y_pred):\n",
      "   }, {\n",
      "       'testcase_name': 'distribute',\n",
      "       'strategy_fn': create_mirrored_strategy,\n",
      "+  }, {\n",
      "+      'testcase_name': 'loss_scaling',\n",
      "+      'strategy_fn': create_mirrored_strategy,\n",
      "+      'use_loss_scaling': True\n",
      "   })\n",
      "   @test_util.run_in_graph_and_eager_modes\n",
      "-  def test_advanced_model(self, strategy_fn):\n",
      "+  def test_advanced_model(self, strategy_fn, use_loss_scaling=False):\n",
      " \n",
      "     # The advanced model tests mixed-precision-related features that would occur\n",
      "     # in a resnet50 model. It tests a model that has:\n",
      "     #  * Multiple layers, some which use auto-cast variables and some which do\n",
      "     #    not\n",
      "     #  * Regularization on some variables and not others.\n",
      "+    #  * Loss scaling (if use_loss_scaling is True)\n",
      " \n",
      "     strategy = strategy_fn()\n",
      "-\n",
      "+    if use_loss_scaling:\n",
      "+      loss_scale = 8.\n",
      "     learning_rate = 2 ** -14\n",
      " \n",
      "     with strategy.scope():\n",
      "@@ -332,6 +341,17 @@ def test_advanced_model(self, strategy_fn):\n",
      "         y = layer2(y)\n",
      "         y = layer3(y)\n",
      "         y = layer4(y)\n",
      "+        if use_loss_scaling:\n",
      "+          # The gradient of 'y' at this point is 1. With loss scaling, the\n",
      "+          # gradient is 'loss_scale'. The DistributionStrategy additionally\n",
      "+          # scales the gradient by 1/num_replicas in_sync. We divide by the\n",
      "+          # batch size of 2 since the loss is averaged across batch elements.\n",
      "+          expected_gradient = loss_scale / strategy.num_replicas_in_sync / 2\n",
      "+          identity_with_grad_check_fn = (\n",
      "+              mp_test_util.create_identity_with_grad_check_fn(\n",
      "+                  expected_dtype=dtypes.float16,\n",
      "+                  expected_gradient=[expected_gradient] * 2))\n",
      "+          y = core.Lambda(identity_with_grad_check_fn)(y)\n",
      "         y = math_ops.cast(y, dtypes.float32)\n",
      "         model = models.Model(inputs=x, outputs=y)\n",
      " \n",
      "@@ -341,6 +361,8 @@ def loss_fn(y_true, y_pred):\n",
      "           return math_ops.reduce_mean(y_pred)\n",
      " \n",
      "         opt = gradient_descent.SGD(learning_rate)\n",
      "+        if use_loss_scaling:\n",
      "+          opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)\n",
      "         model.compile(opt, loss=loss_fn)\n",
      " \n",
      "       x = np.ones((2, 1))\n",
      "@@ -0,0 +1,128 @@\n",
      "+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+# ==============================================================================\n",
      "+\"\"\"Contains the loss scaling optimizer class.\"\"\"\n",
      "+from __future__ import absolute_import\n",
      "+from __future__ import division\n",
      "+from __future__ import print_function\n",
      "+\n",
      "+from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
      "+from tensorflow.python.util.tf_export import keras_export\n",
      "+\n",
      "+\n",
      "+@keras_export('keras.mixed_precision.experimental.LossScaleOptimizer')\n",
      "+class LossScaleOptimizer(optimizer_v2.OptimizerV2):\n",
      "+  \"\"\"An optimizer that applies loss scaling.\n",
      "+\n",
      "+  Loss scaling is a process that multiplies the loss by a multiplier called the\n",
      "+  loss scale, and divides each gradient by the same multiplier. The pseudocode\n",
      "+  for this process is:\n",
      "+\n",
      "+  ```\n",
      "+  loss = ...\n",
      "+  loss *= loss_scale\n",
      "+  grads = gradients(loss, vars)\n",
      "+  grads /= loss_scale\n",
      "+  ```\n",
      "+\n",
      "+  Mathematically, loss scaling has no effect, but can help avoid numerical\n",
      "+  underflow in intermediate gradients when float16 tensors are used. By\n",
      "+  multiplying the loss, each intermediate gradient will have the same multiplier\n",
      "+  applied.\n",
      "+\n",
      "+  This optimizer wraps another optimizer and applies loss scaling to it. Loss\n",
      "+  scaling is applied whenever gradients are computed, either through\n",
      "+  `minimize()` or `get_gradients()`.\n",
      "+  \"\"\"\n",
      "+\n",
      "+  def __init__(self, opt, loss_scale):\n",
      "+    \"\"\"Initializes this loss scale optimizer.\n",
      "+\n",
      "+    Args:\n",
      "+      opt: The Optimizer instance to wrap.\n",
      "+      loss_scale: A float loss scale to scale loss and gradients by\n",
      "+    \"\"\"\n",
      "+    if not isinstance(opt, optimizer_v2.OptimizerV2):\n",
      "+      raise ValueError('\"opt\" must be an instance of OptimizerV2, but got: %s'\n",
      "+                       % opt)\n",
      "+    if hasattr(opt, 'clipnorm'):\n",
      "+      raise ValueError('LossScaleOptimizer does not support wrapping '\n",
      "+                       'optimizers with a clipnorm. Optimizer %s has clipnorm '\n",
      "+                       '%s' % (opt, opt.clipnorm))\n",
      "+\n",
      "+    if hasattr(opt, 'clipvalue'):\n",
      "+      raise ValueError('LossScaleOptimizer does not support wrapping '\n",
      "+                       'optimizers with a clipvalue. Optimizer %s has '\n",
      "+                       'clipvalue %s' % (opt, opt.clipvalue))\n",
      "+\n",
      "+    self._optimizer = opt\n",
      "+    self._loss_scale = float(loss_scale)\n",
      "+\n",
      "+  def _compute_gradients(self, loss, var_list, grad_loss=None):\n",
      "+    loss = self._scale_loss(loss)\n",
      "+    grads_and_vars = self._optimizer._compute_gradients(loss, var_list,  # pylint: disable=protected-access\n",
      "+                                                        grad_loss)\n",
      "+    grads = [g for g, _ in grads_and_vars]\n",
      "+    variables = [v for _, v in grads_and_vars]\n",
      "+    scaled_grads = self._scale_grads(grads)\n",
      "+    return list(zip(scaled_grads, variables))\n",
      "+\n",
      "+  def get_gradients(self, loss, params):\n",
      "+    loss = self._scale_loss(loss)\n",
      "+    grads = self._optimizer.get_gradients(loss, params)\n",
      "+    return self._scale_grads(grads)\n",
      "+\n",
      "+  def _scale_loss(self, loss):\n",
      "+    # The loss is callable for `_compute_gradients`, but not `get_gradients`.\n",
      "+    if callable(loss):\n",
      "+      return lambda: loss() * self._loss_scale\n",
      "+    else:\n",
      "+      return loss * self._loss_scale\n",
      "+\n",
      "+  def _scale_grads(self, grads):\n",
      "+    loss_scale_reciprocal = 1 / self._loss_scale\n",
      "+    return [None if g is None else g * loss_scale_reciprocal for g in grads]\n",
      "+\n",
      "+  def apply_gradients(self, grads_and_vars, name=None):\n",
      "+    return self._optimizer.apply_gradients(grads_and_vars, name)\n",
      "+\n",
      "+  @property\n",
      "+  def learning_rate(self):\n",
      "+    return self._optimizer.learning_rate\n",
      "+\n",
      "+  @learning_rate.setter\n",
      "+  def learning_rate(self, lr):\n",
      "+    self._optimizer.learning_rate = lr\n",
      "+\n",
      "+  # TODO(reedwm): Support dynamic loss scaling.\n",
      "+\n",
      "+  # TODO(reedwm): Maybe merge this class's functionality into OptimizerV2.\n",
      "+\n",
      "+  # TODO(reedwm): Maybe throw an error if mixed precision is used without this\n",
      "+  # optimizer being used.\n",
      "+\n",
      "+  # TODO(reedwm): Define __getattr__ to delegate all methods/attributes to\n",
      "+  # self._optimizer. This is tricky because the super class overrides\n",
      "+  # __getattribute__.\n",
      "+\n",
      "+  # TODO(reedwm): Implement get_config and from_config. This will first require\n",
      "+  # implementing deserialization support for OptimizerV2.\n",
      "+  def get_config(self):\n",
      "+    raise NotImplementedError('get_config() is not yet implemented for '\n",
      "+                              'LossScaleOptimizers')\n",
      "+\n",
      "+  @classmethod\n",
      "+  def from_config(cls, config, custom_objects=None):\n",
      "+    raise NotImplementedError('from_config() is not yet implemented for '\n",
      "+                              'LossScaleOptimizers')\n",
      "@@ -0,0 +1,123 @@\n",
      "+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+# ==============================================================================\n",
      "+\"\"\"Tests for LossScaleOptimizer.\"\"\"\n",
      "+\n",
      "+from __future__ import absolute_import\n",
      "+from __future__ import division\n",
      "+from __future__ import print_function\n",
      "+\n",
      "+from absl.testing import parameterized\n",
      "+\n",
      "+from tensorflow.python.distribute import mirrored_strategy\n",
      "+from tensorflow.python.distribute import one_device_strategy\n",
      "+from tensorflow.python.eager import context\n",
      "+from tensorflow.python.framework import test_util\n",
      "+from tensorflow.python.keras import optimizers\n",
      "+from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer\n",
      "+from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util\n",
      "+from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
      "+from tensorflow.python.ops import variables\n",
      "+from tensorflow.python.platform import test\n",
      "+\n",
      "+\n",
      "+def create_one_device_strategy():\n",
      "+  return one_device_strategy.OneDeviceStrategy('cpu:0')\n",
      "+\n",
      "+\n",
      "+def create_mirrored_strategy():\n",
      "+  if context.num_gpus() >= 1:\n",
      "+    return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\n",
      "+  else:\n",
      "+    return mirrored_strategy.MirroredStrategy(['cpu:0'])\n",
      "+\n",
      "+\n",
      "+TESTCASES = ({\n",
      "+    'testcase_name': 'Base',\n",
      "+    'strategy_fn': create_one_device_strategy\n",
      "+}, {\n",
      "+    'testcase_name': 'Distribute',\n",
      "+    'strategy_fn': create_mirrored_strategy\n",
      "+})\n",
      "+\n",
      "+\n",
      "+class LossScaleOptimizerTest(test.TestCase, parameterized.TestCase):\n",
      "+\n",
      "+  def _run_if_in_graph_mode(self, val):\n",
      "+    # Running only in graph mode is useful, because optimizers sometimes return\n",
      "+    # a value that, in Graph mode, is runnable with self.evaluate. But in Eager\n",
      "+    # mode, the optimizer already does the computations and the return value\n",
      "+    # cannot be run.\n",
      "+    if not context.executing_eagerly():\n",
      "+      self.evaluate(val)\n",
      "+\n",
      "+  def _run_fn_with_grad_check(self, strategy, var, opt, expected_grad):\n",
      "+    grad_check_fn = mp_test_util.create_identity_with_grad_check_fn(\n",
      "+        expected_grad)\n",
      "+    loss = lambda: grad_check_fn(var) / strategy.num_replicas_in_sync\n",
      "+    return lambda: opt.minimize(loss, var_list=[var])\n",
      "+\n",
      "+  @parameterized.named_parameters(*TESTCASES)\n",
      "+  @test_util.run_in_graph_and_eager_modes\n",
      "+  def testLossScaleAppliedToLossWithMinimize(self, strategy_fn):\n",
      "+    with strategy_fn().scope() as strategy:\n",
      "+      var = variables.Variable([5.0])\n",
      "+      opt = gradient_descent.SGD(2.0)\n",
      "+      loss_scale = 10.\n",
      "+      opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)\n",
      "+      # We need num_replicas_in_sync to divide loss_scale, otherwise loss_scale\n",
      "+      # / strategy.num_replicas_in_sync will not be exact, which could lead to\n",
      "+      # assertion failures due to rounding issues.\n",
      "+      self.assertEqual(loss_scale % strategy.num_replicas_in_sync, 0)\n",
      "+      run_fn = self._run_fn_with_grad_check(\n",
      "+          strategy, var, opt, loss_scale / strategy.num_replicas_in_sync)\n",
      "+      run_op = strategy.experimental_run(run_fn)\n",
      "+      self.evaluate(variables.global_variables_initializer())\n",
      "+      self._run_if_in_graph_mode(run_op)\n",
      "+      # The loss is the identity of the variable. Therefore the gradient is 1,\n",
      "+      # and so the variable will be init_val - grad * lr == 5 - 1 * 2 == 3\n",
      "+      self.assertAllClose([3.], self.evaluate(var))\n",
      "+\n",
      "+  @test_util.deprecated_graph_mode_only\n",
      "+  def testLossScaleAppliedToLossWithGetGradientsTest(self):\n",
      "+    var = variables.Variable([2.0])\n",
      "+    opt = gradient_descent.SGD(1.0)\n",
      "+    loss_scale = 10.\n",
      "+    opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)\n",
      "+    grad_check_fn = mp_test_util.create_identity_with_grad_check_fn(loss_scale)\n",
      "+    loss = grad_check_fn(var)\n",
      "+    run_op = opt.get_gradients(loss, [var])\n",
      "+    self.evaluate(variables.global_variables_initializer())\n",
      "+    # This will cause an assertion to run, as\n",
      "+    # mp_test_util.create_identity_with_grad_check_fn added an assertion op.\n",
      "+    self.evaluate(run_op)\n",
      "+\n",
      "+  def testInvalidConstructorArguments(self):\n",
      "+    with self.assertRaisesRegexp(ValueError,\n",
      "+                                 'must be an instance of OptimizerV2'):\n",
      "+      loss_scale_optimizer.LossScaleOptimizer(optimizers.SGD(), 10.)\n",
      "+\n",
      "+    with self.assertRaisesRegexp(ValueError, 'does not support wrapping '\n",
      "+                                             'optimizers with a clipnorm'):\n",
      "+      loss_scale_optimizer.LossScaleOptimizer(\n",
      "+          gradient_descent.SGD(1.0, clipnorm=1.0), 10.)\n",
      "+\n",
      "+    with self.assertRaisesRegexp(ValueError, 'does not support wrapping '\n",
      "+                                             'optimizers with a clipvalue'):\n",
      "+      loss_scale_optimizer.LossScaleOptimizer(\n",
      "+          gradient_descent.SGD(1.0, clipvalue=1.0), 10.)\n",
      "+\n",
      "+\n",
      "+if __name__ == '__main__':\n",
      "+  test.main()\n",
      "@@ -83,6 +83,9 @@ class Policy(object):\n",
      "       tf.keras.layers.Activation('Softmax')\n",
      "   )\n",
      "   ```\n",
      "+\n",
      "+  Note that a LossScaleOptimizer should also be used for mixed precision models\n",
      "+  to avoid numerical underflow. See `LossScaleOptimizer`.\n",
      "   \"\"\"\n",
      " \n",
      "   def __init__(self, name):\n",
      "@@ -0,0 +1,58 @@\n",
      "+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "+#\n",
      "+# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "+# you may not use this file except in compliance with the License.\n",
      "+# You may obtain a copy of the License at\n",
      "+#\n",
      "+#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "+#\n",
      "+# Unless required by applicable law or agreed to in writing, software\n",
      "+# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "+# See the License for the specific language governing permissions and\n",
      "+# limitations under the License.\n",
      "+# ==============================================================================\n",
      "+\"\"\"Contains testing utilities related to mixed precision.\"\"\"\n",
      "+from __future__ import absolute_import\n",
      "+from __future__ import division\n",
      "+from __future__ import print_function\n",
      "+\n",
      "+from tensorflow.python.framework import ops\n",
      "+from tensorflow.python.ops import array_ops\n",
      "+from tensorflow.python.ops import check_ops\n",
      "+from tensorflow.python.ops import custom_gradient\n",
      "+\n",
      "+\n",
      "+def create_identity_with_grad_check_fn(expected_gradient, expected_dtype=None):\n",
      "+  \"\"\"Returns a function that asserts it's gradient has a certain value.\n",
      "+\n",
      "+  This serves as a hook to assert intermediate gradients have a certain value.\n",
      "+  This returns an identity function. The identity's gradient function is also\n",
      "+  the identity function, except it asserts that the gradient equals\n",
      "+  `expected_gradient` and has dtype `expected_dtype`.\n",
      "+\n",
      "+  Args:\n",
      "+    expected_gradient: The gradient function asserts that the gradient is this\n",
      "+      value.\n",
      "+    expected_dtype: The gradient function asserts the gradient has this dtype.\n",
      "+\n",
      "+  Returns:\n",
      "+    An identity function whose gradient function asserts the gradient has a\n",
      "+    certain value.\n",
      "+  \"\"\"\n",
      "+  @custom_gradient.custom_gradient\n",
      "+  def identity_with_grad_check(x):\n",
      "+    \"\"\"Function that asserts it's gradient has a certain value.\"\"\"\n",
      "+    x = array_ops.identity(x)\n",
      "+    def grad(dx):\n",
      "+      if expected_dtype:\n",
      "+        assert dx.dtype == expected_dtype, (\n",
      "+            'dx.dtype should be %s but is: %s' % (expected_dtype, dx.dtype))\n",
      "+      expected_tensor = ops.convert_to_tensor(expected_gradient, dtype=dx.dtype)\n",
      "+      assert_op = check_ops.assert_equal(dx, expected_tensor, data=[dx])\n",
      "+      with ops.control_dependencies([assert_op]):\n",
      "+        dx = array_ops.identity(dx)\n",
      "+      return dx\n",
      "+    return x, grad\n",
      "+  return identity_with_grad_check\n",
      "+\n",
      "@@ -37,6 +37,7 @@\n",
      " @@Laplace\n",
      " @@LaplaceWithSoftplusScale\n",
      " @@Logistic\n",
      "+@@NegativeBinomial\n",
      " @@Normal\n",
      " @@NormalWithSoftplusScale\n",
      " @@Poisson\n",
      "@@ -107,6 +108,7 @@\n",
      " from tensorflow.contrib.distributions.python.ops.mvn_diag import *\n",
      " from tensorflow.contrib.distributions.python.ops.mvn_diag_plus_low_rank import *\n",
      " from tensorflow.contrib.distributions.python.ops.mvn_tril import *\n",
      "+from tensorflow.contrib.distributions.python.ops.negative_binomial import *\n",
      " from tensorflow.contrib.distributions.python.ops.normal import *\n",
      " from tensorflow.contrib.distributions.python.ops.normal_conjugate_posteriors import *\n",
      " from tensorflow.contrib.distributions.python.ops.onehot_categorical import *\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iloc[:10].iterrows():\n",
    "    print(row['patch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看那些df['status']!='modify'的行正不正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('added',                                                    patch  \\\n",
      "3      @@ -0,0 +1,23 @@\\n+# Copyright 2019 The Tensor...   \n",
      "5      @@ -0,0 +1,128 @@\\n+# Copyright 2019 The Tenso...   \n",
      "6      @@ -0,0 +1,123 @@\\n+# Copyright 2019 The Tenso...   \n",
      "8      @@ -0,0 +1,58 @@\\n+# Copyright 2019 The Tensor...   \n",
      "11     @@ -0,0 +1,244 @@\\n+# Copyright 2017 The Tenso...   \n",
      "...                                                  ...   \n",
      "88200  @@ -0,0 +1,559 @@\\n+# Copyright 2018 The Tenso...   \n",
      "88263  @@ -0,0 +1,429 @@\\n+# Copyright 2018 The Tenso...   \n",
      "88264  @@ -0,0 +1,286 @@\\n+# Copyright 2017 The Tenso...   \n",
      "88270  @@ -0,0 +1,21 @@\\n+# Copyright 2016 The Tensor...   \n",
      "88271  @@ -0,0 +1,50 @@\\n+# Copyright 2016 The Tensor...   \n",
      "\n",
      "                                            sha status  \\\n",
      "3      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
      "5      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
      "6      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
      "8      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
      "11     1a0742f6a7a06ff54481385b5c51094b0fef8cf3  added   \n",
      "...                                         ...    ...   \n",
      "88200  6c26c995db67a70d95974fea71103712a12128bc  added   \n",
      "88263  be15ecc9c1e692b1b562d7d23f19bd1263896eef  added   \n",
      "88264  be15ecc9c1e692b1b562d7d23f19bd1263896eef  added   \n",
      "88270  a5e51222300c0ecf644087f99f3183644b76fd00  added   \n",
      "88271  a5e51222300c0ecf644087f99f3183644b76fd00  added   \n",
      "\n",
      "                                                filename  \\\n",
      "3      tensorflow/python/keras/mixed_precision/experi...   \n",
      "5      tensorflow/python/keras/mixed_precision/experi...   \n",
      "6      tensorflow/python/keras/mixed_precision/experi...   \n",
      "8      tensorflow/python/keras/mixed_precision/experi...   \n",
      "11     tensorflow/contrib/distributions/python/kernel...   \n",
      "...                                                  ...   \n",
      "88200  tensorflow/python/ops/ragged/row_partition_tes...   \n",
      "88263  tensorflow/contrib/tpu/python/tpu/feature_colu...   \n",
      "88264  tensorflow/contrib/tpu/python/tpu/feature_colu...   \n",
      "88270            tensorflow/contrib/compiler/__init__.py   \n",
      "88271                 tensorflow/contrib/compiler/jit.py   \n",
      "\n",
      "                                    parents_sha  \n",
      "3      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "5      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "6      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "8      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "11     7314d0853e2ee7d64526ad3b1dcaa08591199bfb  \n",
      "...                                         ...  \n",
      "88200  0b0432ae2d7519aa3f8f9e9569429685024fa823  \n",
      "88263  10ef7edc881ee715eaae48656fcb431fe128441f  \n",
      "88264  10ef7edc881ee715eaae48656fcb431fe128441f  \n",
      "88270  9f2bc83209c4a4f2f57697716f4509e3aa7dde8e  \n",
      "88271  9f2bc83209c4a4f2f57697716f4509e3aa7dde8e  \n",
      "\n",
      "[6870 rows x 5 columns])\n",
      "('modified',                                                    patch  \\\n",
      "0      @@ -224,12 +224,7 @@ def __init__(self,\\n     ...   \n",
      "1      @@ -274,6 +274,37 @@ def test_progbar_logging(...   \n",
      "2      @@ -18,7 +18,6 @@\\n from __future__ import pri...   \n",
      "4      @@ -33,7 +33,10 @@\\n from tensorflow.python.ke...   \n",
      "7      @@ -83,6 +83,9 @@ class Policy(object):\\n     ...   \n",
      "...                                                  ...   \n",
      "88281  @@ -342,15 +342,15 @@ def __init__(self,\\n    ...   \n",
      "88282  @@ -755,8 +755,8 @@ def sparse_softmax_cross_e...   \n",
      "88283  @@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...   \n",
      "88284  @@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...   \n",
      "88285  @@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...   \n",
      "\n",
      "                                            sha    status  \\\n",
      "0      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
      "1      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
      "2      7d9299129c30405270813fe32f1310dbf3bab265  modified   \n",
      "4      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
      "7      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
      "...                                         ...       ...   \n",
      "88281  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
      "88282  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
      "88283  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
      "88284  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
      "88285  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
      "\n",
      "                                                filename  \\\n",
      "0                   tensorflow/python/keras/callbacks.py   \n",
      "1              tensorflow/python/keras/callbacks_test.py   \n",
      "2      tensorflow/python/data/experimental/kernel_tes...   \n",
      "4      tensorflow/python/keras/mixed_precision/experi...   \n",
      "7      tensorflow/python/keras/mixed_precision/experi...   \n",
      "...                                                  ...   \n",
      "88281  tensorflow/contrib/seq2seq/python/ops/attentio...   \n",
      "88282        tensorflow/python/ops/losses/losses_impl.py   \n",
      "88283                    tensorflow/python/ops/nn_ops.py   \n",
      "88284   tensorflow/python/kernel_tests/array_ops_test.py   \n",
      "88285                 tensorflow/python/ops/array_ops.py   \n",
      "\n",
      "                                    parents_sha  \n",
      "0      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
      "1      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
      "2      2c7d978a1446adaba32d8465aceef0b2c4a41cad  \n",
      "4      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "7      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
      "...                                         ...  \n",
      "88281  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
      "88282  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
      "88283  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
      "88284  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
      "88285  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
      "\n",
      "[78721 rows x 5 columns])\n",
      "('removed',                                                    patch  \\\n",
      "137    @@ -1,80 +0,0 @@\\n-# Copyright 2016 The Tensor...   \n",
      "181    @@ -1,58 +0,0 @@\\n-# Copyright 2016 The Tensor...   \n",
      "182    @@ -1,55 +0,0 @@\\n-# Copyright 2016 The Tensor...   \n",
      "183    @@ -1,84 +0,0 @@\\n-# Copyright 2016 The Tensor...   \n",
      "185    @@ -1,59 +0,0 @@\\n-\\n-# Copyright 2016 The Ten...   \n",
      "...                                                  ...   \n",
      "87811  @@ -1,137 +0,0 @@\\n-\\n-# Copyright 2016 The Te...   \n",
      "87941  @@ -1,32 +0,0 @@\\n-# Copyright 2018 The Tensor...   \n",
      "87942  @@ -1,142 +0,0 @@\\n-# Copyright 2018 The Tenso...   \n",
      "87943  @@ -1,90 +0,0 @@\\n-# Copyright 2018 The Tensor...   \n",
      "87944  @@ -1,24 +0,0 @@\\n-# Copyright 2018 The Tensor...   \n",
      "\n",
      "                                            sha   status  \\\n",
      "137    06b73632aab960e96847a8a57352200e9ddfdeab  removed   \n",
      "181    06b73632aab960e96847a8a57352200e9ddfdeab  removed   \n",
      "182    06b73632aab960e96847a8a57352200e9ddfdeab  removed   \n",
      "183    06b73632aab960e96847a8a57352200e9ddfdeab  removed   \n",
      "185    06b73632aab960e96847a8a57352200e9ddfdeab  removed   \n",
      "...                                         ...      ...   \n",
      "87811  f09c19255acc72279f64b58af38b78bd31cb22aa  removed   \n",
      "87941  37479ad906e89e964b2d1ff9d6dc512769d4c77b  removed   \n",
      "87942  37479ad906e89e964b2d1ff9d6dc512769d4c77b  removed   \n",
      "87943  37479ad906e89e964b2d1ff9d6dc512769d4c77b  removed   \n",
      "87944  37479ad906e89e964b2d1ff9d6dc512769d4c77b  removed   \n",
      "\n",
      "                                                filename  \\\n",
      "137    tensorflow/contrib/framework/python/ops/embedd...   \n",
      "181    tensorflow/contrib/learn/python/learn/ops/auto...   \n",
      "182    tensorflow/contrib/learn/python/learn/ops/batc...   \n",
      "183    tensorflow/contrib/learn/python/learn/ops/conv...   \n",
      "185    tensorflow/contrib/learn/python/learn/ops/drop...   \n",
      "...                                                  ...   \n",
      "87811  tensorflow/examples/image_retraining/label_ima...   \n",
      "87941             tensorflow/contrib/kinesis/__init__.py   \n",
      "87942  tensorflow/contrib/kinesis/python/kernel_tests...   \n",
      "87943  tensorflow/contrib/kinesis/python/ops/kinesis_...   \n",
      "87944  tensorflow/contrib/kinesis/python/ops/kinesis_...   \n",
      "\n",
      "                                    parents_sha  \n",
      "137    0f82cb082e1a687e85a7fc38b48c139559ab83a5  \n",
      "181    0f82cb082e1a687e85a7fc38b48c139559ab83a5  \n",
      "182    0f82cb082e1a687e85a7fc38b48c139559ab83a5  \n",
      "183    0f82cb082e1a687e85a7fc38b48c139559ab83a5  \n",
      "185    0f82cb082e1a687e85a7fc38b48c139559ab83a5  \n",
      "...                                         ...  \n",
      "87811  a6323a7b6ad8fbf6ee84c05bc991219a911c11b9  \n",
      "87941  ed9dc39b0b42430fe594f919fd9ceb239db637aa  \n",
      "87942  ed9dc39b0b42430fe594f919fd9ceb239db637aa  \n",
      "87943  ed9dc39b0b42430fe594f919fd9ceb239db637aa  \n",
      "87944  ed9dc39b0b42430fe594f919fd9ceb239db637aa  \n",
      "\n",
      "[1414 rows x 5 columns])\n",
      "('renamed',                                                    patch  \\\n",
      "436    @@ -23,7 +23,6 @@\\n from tensorflow.python.fea...   \n",
      "437    @@ -18,22 +18,25 @@\\n from __future__ import d...   \n",
      "438    @@ -18,10 +18,9 @@\\n from __future__ import di...   \n",
      "439    @@ -23,14 +23,14 @@\\n from tensorflow.python.c...   \n",
      "1024                                                None   \n",
      "...                                                  ...   \n",
      "87281                                               None   \n",
      "87282  @@ -20,6 +20,7 @@\\n \\n import numpy as np\\n im...   \n",
      "87283  @@ -19,6 +19,7 @@\\n from __future__ import pri...   \n",
      "87883                                               None   \n",
      "88226  @@ -1,3 +1,4 @@\\n+\"\"\"Gradients for XLA ops.\"\"\"...   \n",
      "\n",
      "                                            sha   status  \\\n",
      "436    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
      "437    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
      "438    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
      "439    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
      "1024   dfaf790b339c5e05ad59d85bb878813c825aa301  renamed   \n",
      "...                                         ...      ...   \n",
      "87281  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
      "87282  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
      "87283  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
      "87883  b1932a999db899743407c9d140f04e236c40fa9c  renamed   \n",
      "88226  d09a4a1cf2ef4fd0cda021ae73f942ae8b32598a  renamed   \n",
      "\n",
      "                                                filename  \\\n",
      "436    tensorflow/python/keras/feature_column/dense_f...   \n",
      "437    tensorflow/python/keras/feature_column/dense_f...   \n",
      "438    tensorflow/python/keras/feature_column/dense_f...   \n",
      "439    tensorflow/python/keras/feature_column/dense_f...   \n",
      "1024           tensorflow/python/debug/lib/debug_data.py   \n",
      "...                                                  ...   \n",
      "87281  tensorflow/contrib/training/python/training/sa...   \n",
      "87282  tensorflow/contrib/training/python/training/sa...   \n",
      "87283  tensorflow/contrib/training/python/training/sa...   \n",
      "87883  tensorflow/compiler/tests/data_format_ops_test.py   \n",
      "88226        tensorflow/compiler/jit/ops/xla_ops_grad.py   \n",
      "\n",
      "                                    parents_sha  \n",
      "436    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
      "437    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
      "438    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
      "439    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
      "1024   07bb8ea2379bd459832b23951fb20ec47f3fdbd4  \n",
      "...                                         ...  \n",
      "87281  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
      "87282  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
      "87283  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
      "87883  0aa06986aa4eb2fcb5b3f21cd444f55928eb52ff  \n",
      "88226  768b36822e0d5b988a697e0c9e3b65302b051630  \n",
      "\n",
      "[1281 rows x 5 columns])\n"
     ]
    }
   ],
   "source": [
    "for name, group in groups:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## added不行，parents_sha对应的url会报错404：HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch</th>\n",
       "      <th>sha</th>\n",
       "      <th>status</th>\n",
       "      <th>filename</th>\n",
       "      <th>parents_sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@ -0,0 +1,23 @@\\n+# Copyright 2019 The Tensor...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@@ -0,0 +1,128 @@\\n+# Copyright 2019 The Tenso...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@@ -0,0 +1,123 @@\\n+# Copyright 2019 The Tenso...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@@ -0,0 +1,58 @@\\n+# Copyright 2019 The Tensor...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@@ -0,0 +1,244 @@\\n+# Copyright 2017 The Tenso...</td>\n",
       "      <td>1a0742f6a7a06ff54481385b5c51094b0fef8cf3</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/contrib/distributions/python/kernel...</td>\n",
       "      <td>7314d0853e2ee7d64526ad3b1dcaa08591199bfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88200</th>\n",
       "      <td>@@ -0,0 +1,559 @@\\n+# Copyright 2018 The Tenso...</td>\n",
       "      <td>6c26c995db67a70d95974fea71103712a12128bc</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/python/ops/ragged/row_partition_tes...</td>\n",
       "      <td>0b0432ae2d7519aa3f8f9e9569429685024fa823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88263</th>\n",
       "      <td>@@ -0,0 +1,429 @@\\n+# Copyright 2018 The Tenso...</td>\n",
       "      <td>be15ecc9c1e692b1b562d7d23f19bd1263896eef</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/contrib/tpu/python/tpu/feature_colu...</td>\n",
       "      <td>10ef7edc881ee715eaae48656fcb431fe128441f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88264</th>\n",
       "      <td>@@ -0,0 +1,286 @@\\n+# Copyright 2017 The Tenso...</td>\n",
       "      <td>be15ecc9c1e692b1b562d7d23f19bd1263896eef</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/contrib/tpu/python/tpu/feature_colu...</td>\n",
       "      <td>10ef7edc881ee715eaae48656fcb431fe128441f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88270</th>\n",
       "      <td>@@ -0,0 +1,21 @@\\n+# Copyright 2016 The Tensor...</td>\n",
       "      <td>a5e51222300c0ecf644087f99f3183644b76fd00</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/contrib/compiler/__init__.py</td>\n",
       "      <td>9f2bc83209c4a4f2f57697716f4509e3aa7dde8e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88271</th>\n",
       "      <td>@@ -0,0 +1,50 @@\\n+# Copyright 2016 The Tensor...</td>\n",
       "      <td>a5e51222300c0ecf644087f99f3183644b76fd00</td>\n",
       "      <td>added</td>\n",
       "      <td>tensorflow/contrib/compiler/jit.py</td>\n",
       "      <td>9f2bc83209c4a4f2f57697716f4509e3aa7dde8e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patch  \\\n",
       "3      @@ -0,0 +1,23 @@\\n+# Copyright 2019 The Tensor...   \n",
       "5      @@ -0,0 +1,128 @@\\n+# Copyright 2019 The Tenso...   \n",
       "6      @@ -0,0 +1,123 @@\\n+# Copyright 2019 The Tenso...   \n",
       "8      @@ -0,0 +1,58 @@\\n+# Copyright 2019 The Tensor...   \n",
       "11     @@ -0,0 +1,244 @@\\n+# Copyright 2017 The Tenso...   \n",
       "...                                                  ...   \n",
       "88200  @@ -0,0 +1,559 @@\\n+# Copyright 2018 The Tenso...   \n",
       "88263  @@ -0,0 +1,429 @@\\n+# Copyright 2018 The Tenso...   \n",
       "88264  @@ -0,0 +1,286 @@\\n+# Copyright 2017 The Tenso...   \n",
       "88270  @@ -0,0 +1,21 @@\\n+# Copyright 2016 The Tensor...   \n",
       "88271  @@ -0,0 +1,50 @@\\n+# Copyright 2016 The Tensor...   \n",
       "\n",
       "                                            sha status  \\\n",
       "3      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
       "5      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
       "6      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
       "8      23ec33ae1418d56c3ce0294720782b636595a2b3  added   \n",
       "11     1a0742f6a7a06ff54481385b5c51094b0fef8cf3  added   \n",
       "...                                         ...    ...   \n",
       "88200  6c26c995db67a70d95974fea71103712a12128bc  added   \n",
       "88263  be15ecc9c1e692b1b562d7d23f19bd1263896eef  added   \n",
       "88264  be15ecc9c1e692b1b562d7d23f19bd1263896eef  added   \n",
       "88270  a5e51222300c0ecf644087f99f3183644b76fd00  added   \n",
       "88271  a5e51222300c0ecf644087f99f3183644b76fd00  added   \n",
       "\n",
       "                                                filename  \\\n",
       "3      tensorflow/python/keras/mixed_precision/experi...   \n",
       "5      tensorflow/python/keras/mixed_precision/experi...   \n",
       "6      tensorflow/python/keras/mixed_precision/experi...   \n",
       "8      tensorflow/python/keras/mixed_precision/experi...   \n",
       "11     tensorflow/contrib/distributions/python/kernel...   \n",
       "...                                                  ...   \n",
       "88200  tensorflow/python/ops/ragged/row_partition_tes...   \n",
       "88263  tensorflow/contrib/tpu/python/tpu/feature_colu...   \n",
       "88264  tensorflow/contrib/tpu/python/tpu/feature_colu...   \n",
       "88270            tensorflow/contrib/compiler/__init__.py   \n",
       "88271                 tensorflow/contrib/compiler/jit.py   \n",
       "\n",
       "                                    parents_sha  \n",
       "3      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "5      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "6      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "8      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "11     7314d0853e2ee7d64526ad3b1dcaa08591199bfb  \n",
       "...                                         ...  \n",
       "88200  0b0432ae2d7519aa3f8f9e9569429685024fa823  \n",
       "88263  10ef7edc881ee715eaae48656fcb431fe128441f  \n",
       "88264  10ef7edc881ee715eaae48656fcb431fe128441f  \n",
       "88270  9f2bc83209c4a4f2f57697716f4509e3aa7dde8e  \n",
       "88271  9f2bc83209c4a4f2f57697716f4509e3aa7dde8e  \n",
       "\n",
       "[6870 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_added = df[df['status']=='added']\n",
    "df_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer_test.py?ref=ff513ac54689e57ed333d62e0399a4d64a536900\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer_test.py?ref=ff513ac54689e57ed333d62e0399a4d64a536900\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer_test.py?ref=ff513ac54689e57ed333d62e0399a4d64a536900\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer_test.py?ref=ff513ac54689e57ed333d62e0399a4d64a536900\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer_test.py?ref=ff513ac54689e57ed333d62e0399a4d64a536900\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-aa1a8fcea52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morigin_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_added\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_added\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparent_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_added\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_added\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parents_sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0morigin_file_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morigin_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6ef9f640bc0e>\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(filename, sha)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'base64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdecoded_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    342\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    343\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "origin_file = getfile(filename=df_added.iloc[2]['filename'], sha=df_added.iloc[2]['sha'])\n",
    "parent_file = getfile(filename=df_added.iloc[2]['filename'], sha=df_added.iloc[2]['parents_sha'])\n",
    "\n",
    "\n",
    "origin_file_lines = origin_file.splitlines()\n",
    "parent_file_lines = parent_file.splitlines()\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(parent_file_lines, origin_file_lines)\n",
    "\n",
    "minus_lines = []\n",
    "for line in list(diff):\n",
    "    if line.startswith('-') and len(line) > 2: # 排除空行，因为空行只包括两个符号：减号与空格\n",
    "        minus_lines.append(line.lstrip('- '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import os',\n",
       " 'os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"',\n",
       " 'os.environ[\"TF_JOB_NAME\"] = \"test_job\"',\n",
       " 'del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]',\n",
       " 'del os.environ[\"TF_JOB_NAME\"]']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minus_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 只有modified可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch</th>\n",
       "      <th>sha</th>\n",
       "      <th>status</th>\n",
       "      <th>filename</th>\n",
       "      <th>parents_sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>@@ -23,7 +23,6 @@\\n from tensorflow.python.fea...</td>\n",
       "      <td>d98a0e601762228e0c0666f964530a470432bade</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/python/keras/feature_column/dense_f...</td>\n",
       "      <td>efd77d2e45f2958615a15812d225caa093f1e5af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>@@ -18,22 +18,25 @@\\n from __future__ import d...</td>\n",
       "      <td>d98a0e601762228e0c0666f964530a470432bade</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/python/keras/feature_column/dense_f...</td>\n",
       "      <td>efd77d2e45f2958615a15812d225caa093f1e5af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>@@ -18,10 +18,9 @@\\n from __future__ import di...</td>\n",
       "      <td>d98a0e601762228e0c0666f964530a470432bade</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/python/keras/feature_column/dense_f...</td>\n",
       "      <td>efd77d2e45f2958615a15812d225caa093f1e5af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>@@ -23,14 +23,14 @@\\n from tensorflow.python.c...</td>\n",
       "      <td>d98a0e601762228e0c0666f964530a470432bade</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/python/keras/feature_column/dense_f...</td>\n",
       "      <td>efd77d2e45f2958615a15812d225caa093f1e5af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>None</td>\n",
       "      <td>dfaf790b339c5e05ad59d85bb878813c825aa301</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/python/debug/lib/debug_data.py</td>\n",
       "      <td>07bb8ea2379bd459832b23951fb20ec47f3fdbd4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87281</th>\n",
       "      <td>None</td>\n",
       "      <td>33cef2740f10e1e3791f10c8e59a9d5c8d74629d</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/contrib/training/python/training/sa...</td>\n",
       "      <td>8a3107801d15bf8af36221ff5bca0b94bf44d6d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87282</th>\n",
       "      <td>@@ -20,6 +20,7 @@\\n \\n import numpy as np\\n im...</td>\n",
       "      <td>33cef2740f10e1e3791f10c8e59a9d5c8d74629d</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/contrib/training/python/training/sa...</td>\n",
       "      <td>8a3107801d15bf8af36221ff5bca0b94bf44d6d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87283</th>\n",
       "      <td>@@ -19,6 +19,7 @@\\n from __future__ import pri...</td>\n",
       "      <td>33cef2740f10e1e3791f10c8e59a9d5c8d74629d</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/contrib/training/python/training/sa...</td>\n",
       "      <td>8a3107801d15bf8af36221ff5bca0b94bf44d6d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87883</th>\n",
       "      <td>None</td>\n",
       "      <td>b1932a999db899743407c9d140f04e236c40fa9c</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/compiler/tests/data_format_ops_test.py</td>\n",
       "      <td>0aa06986aa4eb2fcb5b3f21cd444f55928eb52ff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88226</th>\n",
       "      <td>@@ -1,3 +1,4 @@\\n+\"\"\"Gradients for XLA ops.\"\"\"...</td>\n",
       "      <td>d09a4a1cf2ef4fd0cda021ae73f942ae8b32598a</td>\n",
       "      <td>renamed</td>\n",
       "      <td>tensorflow/compiler/jit/ops/xla_ops_grad.py</td>\n",
       "      <td>768b36822e0d5b988a697e0c9e3b65302b051630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1281 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patch  \\\n",
       "436    @@ -23,7 +23,6 @@\\n from tensorflow.python.fea...   \n",
       "437    @@ -18,22 +18,25 @@\\n from __future__ import d...   \n",
       "438    @@ -18,10 +18,9 @@\\n from __future__ import di...   \n",
       "439    @@ -23,14 +23,14 @@\\n from tensorflow.python.c...   \n",
       "1024                                                None   \n",
       "...                                                  ...   \n",
       "87281                                               None   \n",
       "87282  @@ -20,6 +20,7 @@\\n \\n import numpy as np\\n im...   \n",
       "87283  @@ -19,6 +19,7 @@\\n from __future__ import pri...   \n",
       "87883                                               None   \n",
       "88226  @@ -1,3 +1,4 @@\\n+\"\"\"Gradients for XLA ops.\"\"\"...   \n",
       "\n",
       "                                            sha   status  \\\n",
       "436    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
       "437    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
       "438    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
       "439    d98a0e601762228e0c0666f964530a470432bade  renamed   \n",
       "1024   dfaf790b339c5e05ad59d85bb878813c825aa301  renamed   \n",
       "...                                         ...      ...   \n",
       "87281  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
       "87282  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
       "87283  33cef2740f10e1e3791f10c8e59a9d5c8d74629d  renamed   \n",
       "87883  b1932a999db899743407c9d140f04e236c40fa9c  renamed   \n",
       "88226  d09a4a1cf2ef4fd0cda021ae73f942ae8b32598a  renamed   \n",
       "\n",
       "                                                filename  \\\n",
       "436    tensorflow/python/keras/feature_column/dense_f...   \n",
       "437    tensorflow/python/keras/feature_column/dense_f...   \n",
       "438    tensorflow/python/keras/feature_column/dense_f...   \n",
       "439    tensorflow/python/keras/feature_column/dense_f...   \n",
       "1024           tensorflow/python/debug/lib/debug_data.py   \n",
       "...                                                  ...   \n",
       "87281  tensorflow/contrib/training/python/training/sa...   \n",
       "87282  tensorflow/contrib/training/python/training/sa...   \n",
       "87283  tensorflow/contrib/training/python/training/sa...   \n",
       "87883  tensorflow/compiler/tests/data_format_ops_test.py   \n",
       "88226        tensorflow/compiler/jit/ops/xla_ops_grad.py   \n",
       "\n",
       "                                    parents_sha  \n",
       "436    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
       "437    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
       "438    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
       "439    efd77d2e45f2958615a15812d225caa093f1e5af  \n",
       "1024   07bb8ea2379bd459832b23951fb20ec47f3fdbd4  \n",
       "...                                         ...  \n",
       "87281  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
       "87282  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
       "87283  8a3107801d15bf8af36221ff5bca0b94bf44d6d3  \n",
       "87883  0aa06986aa4eb2fcb5b3f21cd444f55928eb52ff  \n",
       "88226  768b36822e0d5b988a697e0c9e3b65302b051630  \n",
       "\n",
       "[1281 rows x 5 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_renamed = df[df['status']=='renamed']\n",
    "df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/feature_column/dense_features_v2_test.py?ref=efd77d2e45f2958615a15812d225caa093f1e5af\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/feature_column/dense_features_v2_test.py?ref=efd77d2e45f2958615a15812d225caa093f1e5af\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/feature_column/dense_features_v2_test.py?ref=efd77d2e45f2958615a15812d225caa093f1e5af\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/feature_column/dense_features_v2_test.py?ref=efd77d2e45f2958615a15812d225caa093f1e5af\n",
      "INFO:root:HTTPError!, <class 'requests.exceptions.HTTPError'>, 404 Client Error: Not Found for url: https://api.github.com/repos/tensorflow/tensorflow/contents/tensorflow/python/keras/feature_column/dense_features_v2_test.py?ref=efd77d2e45f2958615a15812d225caa093f1e5af\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-69de16ad0dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morigin_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparent_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parents_sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0morigin_file_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morigin_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6ef9f640bc0e>\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(filename, sha)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'base64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdecoded_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    342\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    343\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "origin_file = getfile(filename=df_renamed.iloc[3]['filename'], sha=df_renamed.iloc[3]['sha'])\n",
    "parent_file = getfile(filename=df_renamed.iloc[3]['filename'], sha=df_renamed.iloc[3]['parents_sha'])\n",
    "\n",
    "\n",
    "origin_file_lines = origin_file.splitlines()\n",
    "parent_file_lines = parent_file.splitlines()\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(parent_file_lines, origin_file_lines)\n",
    "\n",
    "minus_lines = []\n",
    "for line in list(diff):\n",
    "    if line.startswith('-') and len(line) > 2: # 排除空行，因为空行只包括两个符号：减号与空格\n",
    "        minus_lines.append(line.lstrip('- '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把df分成两份方便并行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch</th>\n",
       "      <th>sha</th>\n",
       "      <th>status</th>\n",
       "      <th>filename</th>\n",
       "      <th>parents_sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@ -224,12 +224,7 @@ def __init__(self,\\n     ...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@ -274,6 +274,37 @@ def test_progbar_logging(...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks_test.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@ -18,7 +18,6 @@\\n from __future__ import pri...</td>\n",
       "      <td>7d9299129c30405270813fe32f1310dbf3bab265</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/data/experimental/kernel_tes...</td>\n",
       "      <td>2c7d978a1446adaba32d8465aceef0b2c4a41cad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@ -33,7 +33,10 @@\\n from tensorflow.python.ke...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@@ -83,6 +83,9 @@ class Policy(object):\\n     ...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88281</th>\n",
       "      <td>@@ -342,15 +342,15 @@ def __init__(self,\\n    ...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/contrib/seq2seq/python/ops/attentio...</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88282</th>\n",
       "      <td>@@ -755,8 +755,8 @@ def sparse_softmax_cross_e...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/losses/losses_impl.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88283</th>\n",
       "      <td>@@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/nn_ops.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88284</th>\n",
       "      <td>@@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/kernel_tests/array_ops_test.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88285</th>\n",
       "      <td>@@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/array_ops.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78721 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patch  \\\n",
       "0      @@ -224,12 +224,7 @@ def __init__(self,\\n     ...   \n",
       "1      @@ -274,6 +274,37 @@ def test_progbar_logging(...   \n",
       "2      @@ -18,7 +18,6 @@\\n from __future__ import pri...   \n",
       "4      @@ -33,7 +33,10 @@\\n from tensorflow.python.ke...   \n",
       "7      @@ -83,6 +83,9 @@ class Policy(object):\\n     ...   \n",
       "...                                                  ...   \n",
       "88281  @@ -342,15 +342,15 @@ def __init__(self,\\n    ...   \n",
       "88282  @@ -755,8 +755,8 @@ def sparse_softmax_cross_e...   \n",
       "88283  @@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...   \n",
       "88284  @@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...   \n",
       "88285  @@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...   \n",
       "\n",
       "                                            sha    status  \\\n",
       "0      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "1      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "2      7d9299129c30405270813fe32f1310dbf3bab265  modified   \n",
       "4      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
       "7      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
       "...                                         ...       ...   \n",
       "88281  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88282  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88283  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88284  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "88285  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "\n",
       "                                                filename  \\\n",
       "0                   tensorflow/python/keras/callbacks.py   \n",
       "1              tensorflow/python/keras/callbacks_test.py   \n",
       "2      tensorflow/python/data/experimental/kernel_tes...   \n",
       "4      tensorflow/python/keras/mixed_precision/experi...   \n",
       "7      tensorflow/python/keras/mixed_precision/experi...   \n",
       "...                                                  ...   \n",
       "88281  tensorflow/contrib/seq2seq/python/ops/attentio...   \n",
       "88282        tensorflow/python/ops/losses/losses_impl.py   \n",
       "88283                    tensorflow/python/ops/nn_ops.py   \n",
       "88284   tensorflow/python/kernel_tests/array_ops_test.py   \n",
       "88285                 tensorflow/python/ops/array_ops.py   \n",
       "\n",
       "                                    parents_sha  \n",
       "0      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "1      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "2      2c7d978a1446adaba32d8465aceef0b2c4a41cad  \n",
       "4      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "7      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "...                                         ...  \n",
       "88281  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88282  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88283  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88284  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "88285  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "\n",
       "[78721 rows x 5 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified = df[df['status']=='modified']\n",
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39360"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_pivot = len(df_modified) // 2\n",
    "split_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified_part1 = df_modified[:split_pivot]\n",
    "df_modified_part2 = df_modified[split_pivot:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified_part1.to_pickle(\"df_sha_part1.tar.bz2\")\n",
    "df_modified_part2.to_pickle(\"df_sha_part2.tar.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找出-开头的行还不行，要全部保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_content(filename, sha):\n",
    "    url = contents_template.format(filename=filename, sha=sha)\n",
    "    text = get_html_content(url=url)\n",
    "    content = json.loads(text)\n",
    "    if len(content) == 0:\n",
    "        return \"\"\n",
    "    return content.get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_parent_content(filename, sha):\n",
    "    return get_file_content(filename=row[3], sha=row[4])\n",
    "\n",
    "def get_file_origin_content(row):\n",
    "    return get_file_content(filename=row[3], sha=row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch</th>\n",
       "      <th>sha</th>\n",
       "      <th>status</th>\n",
       "      <th>filename</th>\n",
       "      <th>parents_sha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@ -224,12 +224,7 @@ def __init__(self,\\n     ...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@ -274,6 +274,37 @@ def test_progbar_logging(...</td>\n",
       "      <td>f21a440cde082e8b629f930c7c3f7b65ac27d4e6</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/callbacks_test.py</td>\n",
       "      <td>93d545e8b1e377926a24584a25c06dd422b7cc82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@ -18,7 +18,6 @@\\n from __future__ import pri...</td>\n",
       "      <td>7d9299129c30405270813fe32f1310dbf3bab265</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/data/experimental/kernel_tes...</td>\n",
       "      <td>2c7d978a1446adaba32d8465aceef0b2c4a41cad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@ -33,7 +33,10 @@\\n from tensorflow.python.ke...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@@ -83,6 +83,9 @@ class Policy(object):\\n     ...</td>\n",
       "      <td>23ec33ae1418d56c3ce0294720782b636595a2b3</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/keras/mixed_precision/experi...</td>\n",
       "      <td>ff513ac54689e57ed333d62e0399a4d64a536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88281</th>\n",
       "      <td>@@ -342,15 +342,15 @@ def __init__(self,\\n    ...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/contrib/seq2seq/python/ops/attentio...</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88282</th>\n",
       "      <td>@@ -755,8 +755,8 @@ def sparse_softmax_cross_e...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/losses/losses_impl.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88283</th>\n",
       "      <td>@@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...</td>\n",
       "      <td>23992bb091457f3e881ae1413d04c2aebbccfa2f</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/nn_ops.py</td>\n",
       "      <td>af14ed3f37d52220394fb9ff902ae62fd915dbe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88284</th>\n",
       "      <td>@@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/kernel_tests/array_ops_test.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88285</th>\n",
       "      <td>@@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...</td>\n",
       "      <td>caceb02f75ff80a8e48440720cec3d7d6fa3297e</td>\n",
       "      <td>modified</td>\n",
       "      <td>tensorflow/python/ops/array_ops.py</td>\n",
       "      <td>cb9d147e9c788cc60ebb255fd26971719c7e2db2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78721 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   patch  \\\n",
       "0      @@ -224,12 +224,7 @@ def __init__(self,\\n     ...   \n",
       "1      @@ -274,6 +274,37 @@ def test_progbar_logging(...   \n",
       "2      @@ -18,7 +18,6 @@\\n from __future__ import pri...   \n",
       "4      @@ -33,7 +33,10 @@\\n from tensorflow.python.ke...   \n",
       "7      @@ -83,6 +83,9 @@ class Policy(object):\\n     ...   \n",
       "...                                                  ...   \n",
       "88281  @@ -342,15 +342,15 @@ def __init__(self,\\n    ...   \n",
       "88282  @@ -755,8 +755,8 @@ def sparse_softmax_cross_e...   \n",
       "88283  @@ -1646,9 +1646,9 @@ def _swap_axis(logits, d...   \n",
       "88284  @@ -369,8 +369,7 @@ def testEllipsis(self):\\n ...   \n",
       "88285  @@ -1641,99 +1641,8 @@ def _StridedSliceGradSh...   \n",
       "\n",
       "                                            sha    status  \\\n",
       "0      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "1      f21a440cde082e8b629f930c7c3f7b65ac27d4e6  modified   \n",
       "2      7d9299129c30405270813fe32f1310dbf3bab265  modified   \n",
       "4      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
       "7      23ec33ae1418d56c3ce0294720782b636595a2b3  modified   \n",
       "...                                         ...       ...   \n",
       "88281  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88282  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88283  23992bb091457f3e881ae1413d04c2aebbccfa2f  modified   \n",
       "88284  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "88285  caceb02f75ff80a8e48440720cec3d7d6fa3297e  modified   \n",
       "\n",
       "                                                filename  \\\n",
       "0                   tensorflow/python/keras/callbacks.py   \n",
       "1              tensorflow/python/keras/callbacks_test.py   \n",
       "2      tensorflow/python/data/experimental/kernel_tes...   \n",
       "4      tensorflow/python/keras/mixed_precision/experi...   \n",
       "7      tensorflow/python/keras/mixed_precision/experi...   \n",
       "...                                                  ...   \n",
       "88281  tensorflow/contrib/seq2seq/python/ops/attentio...   \n",
       "88282        tensorflow/python/ops/losses/losses_impl.py   \n",
       "88283                    tensorflow/python/ops/nn_ops.py   \n",
       "88284   tensorflow/python/kernel_tests/array_ops_test.py   \n",
       "88285                 tensorflow/python/ops/array_ops.py   \n",
       "\n",
       "                                    parents_sha  \n",
       "0      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "1      93d545e8b1e377926a24584a25c06dd422b7cc82  \n",
       "2      2c7d978a1446adaba32d8465aceef0b2c4a41cad  \n",
       "4      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "7      ff513ac54689e57ed333d62e0399a4d64a536900  \n",
       "...                                         ...  \n",
       "88281  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88282  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88283  af14ed3f37d52220394fb9ff902ae62fd915dbe8  \n",
       "88284  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "88285  cb9d147e9c788cc60ebb255fd26971719c7e2db2  \n",
       "\n",
       "[78721 rows x 5 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-6c832f57fd6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_modified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_file_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7547\u001b[0m         )\n\u001b[0;32m-> 7548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-6c832f57fd6c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_modified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_file_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-8a7af541bcf2>\u001b[0m in \u001b[0;36mget_file_content\u001b[0;34m(filename, sha)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ba1a0d47e522>\u001b[0m in \u001b[0;36mget_html_content\u001b[0;34m(url, retry, params)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapparent_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)\u001b[0m\n\u001b[1;32m    384\u001b[0m     ) or IS_SECURETRANSPORT:\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         warnings.warn(\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/zyt_env/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_modified['origin_content'] = df_modified.apply(lambda row: get_file_content(row['filename'], row['sha']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified['parent_content'] = df_modified.apply(lambda row: get_file_content(row['filename'], row['parents_sha']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比较文本区别，删+标记-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"-   # Text\"\n",
    "test_text[1:].strip().startswith('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_file(origin_file, parent_file):\n",
    "    origin_file_lines = origin_file.splitlines()\n",
    "    parent_file_lines = parent_file.splitlines()\n",
    "    d = difflib.Differ()\n",
    "    diff = d.compare(parent_file_lines, origin_file_lines)\n",
    "    minus_lines = []\n",
    "    for line in list(diff):\n",
    "        # 因为空行只包括两个符号：减号与空格，所以筛选len()<=2的空行\n",
    "        if not line.startswith('+') and len(line) > 2:  # 新加行，或者空行都删掉，只保留原有的行与删减行\n",
    "            if line.startswith('-'): # 删减行中\n",
    "                if line[1:].strip().startswith('#'):  # 原来是不代码的，删掉\n",
    "                    continue\n",
    "                else:  # 原来是代码的，以注释代替\n",
    "                    line = line.replace('-', '#', 1).strip()\n",
    "            # 添加删减行和原有行，原有行原封不动\n",
    "            minus_lines.append(line)\n",
    "    return minus_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Copyright 2019 The TensorFlow Authors. All Rights Reserved.',\n",
       " '#',\n",
       " '# Licensed under the Apache License, Version 2.0 (the \"License\");',\n",
       " '# you may not use this file except in compliance with the License.',\n",
       " '# You may obtain a copy of the License at',\n",
       " '#',\n",
       " '#     http://www.apache.org/licenses/LICENSE-2.0',\n",
       " '#',\n",
       " '# Unless required by applicable law or agreed to in writing, software',\n",
       " '# distributed under the License is distributed on an \"AS IS\" BASIS,',\n",
       " '# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.',\n",
       " '# See the License for the specific language governing permissions and',\n",
       " '# limitations under the License.',\n",
       " '# ==============================================================================',\n",
       " '\"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\"\"\"',\n",
       " '',\n",
       " 'from __future__ import absolute_import',\n",
       " 'from __future__ import division',\n",
       " 'from __future__ import print_function',\n",
       " '',\n",
       " 'from tensorflow.python.feature_column import feature_column_v2 as fc',\n",
       " 'from tensorflow.python.framework import ops',\n",
       " 'from tensorflow.python.keras.feature_column import dense_features',\n",
       " 'from tensorflow.python.util.tf_export import keras_export',\n",
       " '',\n",
       " '',\n",
       " \"@keras_export('keras.layers.DenseFeatures', v1=[])\",\n",
       " 'class DenseFeatures(dense_features.DenseFeatures):',\n",
       " '  \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.',\n",
       " '',\n",
       " '  Generally a single example in training data is described with FeatureColumns.',\n",
       " '  At the first layer of the model, this column oriented data should be converted',\n",
       " '  to a single `Tensor`.',\n",
       " '',\n",
       " '  This layer can be called multiple times with different features.',\n",
       " '',\n",
       " '  This is the V2 version of this layer that uses name_scopes to create',\n",
       " '  variables instead of variable_scopes. But this approach currently lacks',\n",
       " '  support for partitioned variables. In that case, use the V1 version instead.',\n",
       " '',\n",
       " '  Example:',\n",
       " '',\n",
       " '  ```python',\n",
       " \"  price = tf.feature_column.numeric_column('price')\",\n",
       " '  keywords_embedded = tf.feature_column.embedding_column(',\n",
       " '      tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),',\n",
       " '      dimensions=16)',\n",
       " '  columns = [price, keywords_embedded, ...]',\n",
       " '  feature_layer = tf.keras.layers.DenseFeatures(columns)',\n",
       " '',\n",
       " '  features = tf.io.parse_example(',\n",
       " '      ..., features=tf.feature_column.make_parse_example_spec(columns))',\n",
       " '  dense_tensor = feature_layer(features)',\n",
       " '  for units in [128, 64, 32]:',\n",
       " \"    dense_tensor = tf.keras.layers.Dense(units, activation='relu')(dense_tensor)\",\n",
       " '  prediction = tf.keras.layers.Dense(1)(dense_tensor)',\n",
       " '  ```',\n",
       " '  \"\"\"',\n",
       " '',\n",
       " '  def __init__(self,',\n",
       " '               feature_columns,',\n",
       " '               trainable=True,',\n",
       " '               name=None,',\n",
       " '               **kwargs):',\n",
       " '    \"\"\"Creates a DenseFeatures object.',\n",
       " '',\n",
       " '    Args:',\n",
       " '      feature_columns: An iterable containing the FeatureColumns to use as',\n",
       " '        inputs to your model. All items should be instances of classes derived',\n",
       " '        from `DenseColumn` such as `numeric_column`, `embedding_column`,',\n",
       " '        `bucketized_column`, `indicator_column`. If you have categorical',\n",
       " '        features, you can wrap them with an `embedding_column` or',\n",
       " '        `indicator_column`.',\n",
       " \"      trainable:  Boolean, whether the layer's variables will be updated via\",\n",
       " '        gradient descent during training.',\n",
       " '      name: Name to give to the DenseFeatures.',\n",
       " '      **kwargs: Keyword arguments to construct a layer.',\n",
       " '',\n",
       " '    Raises:',\n",
       " '      ValueError: if an item in `feature_columns` is not a `DenseColumn`.',\n",
       " '    \"\"\"',\n",
       " '    super(DenseFeatures, self).__init__(',\n",
       " '        feature_columns=feature_columns,',\n",
       " '        trainable=trainable,',\n",
       " '        name=name,',\n",
       " '        **kwargs)',\n",
       " '    self._state_manager = fc._StateManagerImplV2(self, self.trainable)  # pylint: disable=protected-access',\n",
       " '',\n",
       " '  def build(self, _):',\n",
       " '    for column in self._feature_columns:',\n",
       " '      with ops.name_scope(column.name):',\n",
       " '        column.create_state(self._state_manager)',\n",
       " '    # We would like to call Layer.build and not _DenseFeaturesHelper.build.',\n",
       " '    # pylint: disable=protected-access',\n",
       " '    super(fc._BaseFeaturesLayer, self).build(None)  # pylint: disable=bad-super-call']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_file.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- # Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
      "?                ^\n",
      "\n",
      "+ # Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "?                ^\n",
      "\n",
      "  #\n",
      "  # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  # you may not use this file except in compliance with the License.\n",
      "  # You may obtain a copy of the License at\n",
      "  #\n",
      "  #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "  #\n",
      "  # Unless required by applicable law or agreed to in writing, software\n",
      "  # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  # See the License for the specific language governing permissions and\n",
      "  # limitations under the License.\n",
      "  # ==============================================================================\n",
      "- \"\"\"Tests for the private `_OptimizeDataset` transformation.\"\"\"\n",
      "+ \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\"\"\"\n",
      "+ \n",
      "  from __future__ import absolute_import\n",
      "  from __future__ import division\n",
      "  from __future__ import print_function\n",
      "  \n",
      "+ from tensorflow.python.feature_column import feature_column_v2 as fc\n",
      "- import functools\n",
      "- import os\n",
      "- import warnings\n",
      "- \n",
      "- from absl.testing import parameterized\n",
      "- import numpy as np\n",
      "- \n",
      "- from tensorflow.python.data.experimental.ops import batching\n",
      "- from tensorflow.python.data.experimental.ops import grouping\n",
      "- from tensorflow.python.data.experimental.ops import optimization_options\n",
      "- from tensorflow.python.data.experimental.ops import scan_ops\n",
      "- from tensorflow.python.data.experimental.ops import testing\n",
      "- from tensorflow.python.data.experimental.ops import threadpool\n",
      "- from tensorflow.python.data.kernel_tests import test_base\n",
      "- from tensorflow.python.data.ops import dataset_ops\n",
      "- from tensorflow.python.framework import combinations\n",
      "- from tensorflow.python.framework import dtypes\n",
      "- from tensorflow.python.framework import errors\n",
      "  from tensorflow.python.framework import ops\n",
      "+ from tensorflow.python.keras.feature_column import dense_features\n",
      "+ from tensorflow.python.util.tf_export import keras_export\n",
      "- from tensorflow.python.ops import array_ops\n",
      "- from tensorflow.python.ops import random_ops\n",
      "- from tensorflow.python.ops import variable_scope\n",
      "- from tensorflow.python.platform import test\n",
      "  \n",
      "  \n",
      "- def _captured_refvar_test_combinations():\n",
      "+ @keras_export('keras.layers.DenseFeatures', v1=[])\n",
      "+ class DenseFeatures(dense_features.DenseFeatures):\n",
      "+   \"\"\"A layer that produces a dense `Tensor` based on given `feature_columns`.\n",
      "  \n",
      "-   def make_map_dataset(var):\n",
      "-     return dataset_ops.Dataset.from_tensors(0).map(lambda x: x + var)\n",
      "+   Generally a single example in training data is described with FeatureColumns.\n",
      "+   At the first layer of the model, this column oriented data should be converted\n",
      "+   to a single `Tensor`.\n",
      "  \n",
      "+   This layer can be called multiple times with different features.\n",
      "-   def make_flat_map_dataset(var):\n",
      "-     return dataset_ops.Dataset.from_tensors(\n",
      "-         0).flat_map(lambda _: dataset_ops.Dataset.from_tensors(var))\n",
      "  \n",
      "-   def make_filter_dataset(var):\n",
      "-     return dataset_ops.Dataset.from_tensors(0).filter(lambda x: x < var)\n",
      "+   This is the V2 version of this layer that uses name_scopes to create\n",
      "+   variables instead of variable_scopes. But this approach currently lacks\n",
      "+   support for partitioned variables. In that case, use the V1 version instead.\n",
      "  \n",
      "-   def make_map_and_batch_dataset(var):\n",
      "+   Example:\n",
      "  \n",
      "-     def map_fn(x):\n",
      "-       return x + var\n",
      "+   ```python\n",
      "+   price = tf.feature_column.numeric_column('price')\n",
      "+   keywords_embedded = tf.feature_column.embedding_column(\n",
      "+       tf.feature_column.categorical_column_with_hash_bucket(\"keywords\", 10K),\n",
      "+       dimensions=16)\n",
      "+   columns = [price, keywords_embedded, ...]\n",
      "+   feature_layer = tf.keras.layers.DenseFeatures(columns)\n",
      "  \n",
      "-     return dataset_ops.Dataset.from_tensors(0).apply(\n",
      "-         batching.map_and_batch(map_fn, 1))\n",
      "+   features = tf.io.parse_example(\n",
      "+       ..., features=tf.feature_column.make_parse_example_spec(columns))\n",
      "+   dense_tensor = feature_layer(features)\n",
      "+   for units in [128, 64, 32]:\n",
      "+     dense_tensor = tf.keras.layers.Dense(units, activation='relu')(dense_tensor)\n",
      "+   prediction = tf.keras.layers.Dense(1)(dense_tensor)\n",
      "+   ```\n",
      "+   \"\"\"\n",
      "  \n",
      "+   def __init__(self,\n",
      "+                feature_columns,\n",
      "+                trainable=True,\n",
      "+                name=None,\n",
      "+                **kwargs):\n",
      "+     \"\"\"Creates a DenseFeatures object.\n",
      "-   def make_group_by_reducer_dataset(var):\n",
      "-     reducer = grouping.Reducer(\n",
      "-         init_func=lambda _: 0,\n",
      "-         reduce_func=lambda x, y: x,\n",
      "-         finalize_func=lambda _: var)\n",
      "-     return dataset_ops.Dataset.range(5).apply(\n",
      "-         grouping.group_by_reducer(lambda x: x % 2, reducer))\n",
      "  \n",
      "-   def make_group_by_window_dataset(var):\n",
      "+     Args:\n",
      "+       feature_columns: An iterable containing the FeatureColumns to use as\n",
      "+         inputs to your model. All items should be instances of classes derived\n",
      "+         from `DenseColumn` such as `numeric_column`, `embedding_column`,\n",
      "+         `bucketized_column`, `indicator_column`. If you have categorical\n",
      "+         features, you can wrap them with an `embedding_column` or\n",
      "+         `indicator_column`.\n",
      "+       trainable:  Boolean, whether the layer's variables will be updated via\n",
      "+         gradient descent during training.\n",
      "+       name: Name to give to the DenseFeatures.\n",
      "+       **kwargs: Keyword arguments to construct a layer.\n",
      "  \n",
      "-     def reduce_fn(key, bucket):\n",
      "-       del key, bucket\n",
      "-       return dataset_ops.Dataset.from_tensors(var)\n",
      "+     Raises:\n",
      "+       ValueError: if an item in `feature_columns` is not a `DenseColumn`.\n",
      "+     \"\"\"\n",
      "+     super(DenseFeatures, self).__init__(\n",
      "+         feature_columns=feature_columns,\n",
      "+         trainable=trainable,\n",
      "+         name=name,\n",
      "+         **kwargs)\n",
      "+     self._state_manager = fc._StateManagerImplV2(self, self.trainable)  # pylint: disable=protected-access\n",
      "  \n",
      "+   def build(self, _):\n",
      "+     for column in self._feature_columns:\n",
      "+       with ops.name_scope(column.name):\n",
      "+         column.create_state(self._state_manager)\n",
      "+     # We would like to call Layer.build and not _DenseFeaturesHelper.build.\n",
      "+     # pylint: disable=protected-access\n",
      "+     super(fc._BaseFeaturesLayer, self).build(None)  # pylint: disable=bad-super-call\n",
      "-     return dataset_ops.Dataset.from_tensors(0).repeat(10).apply(\n",
      "-         grouping.group_by_window(lambda _: 0, reduce_fn, 10))\n",
      "- \n",
      "-   def make_scan_dataset(var):\n",
      "-     return dataset_ops.Dataset.from_tensors(0).apply(\n",
      "-         scan_ops.scan(\n",
      "-             0, lambda old_state, elem: (old_state + 1, elem + old_state + var)))\n",
      "- \n",
      "-   cases = [\n",
      "-       # Core datasets\n",
      "-       (\"Map\", make_map_dataset),\n",
      "-       (\"FlatMap\", make_flat_map_dataset),\n",
      "-       (\"Filter\", make_filter_dataset),\n",
      "-       # Experimental datasets\n",
      "-       (\"MapAndBatch\", make_map_and_batch_dataset),\n",
      "-       (\"GroupByReducer\", make_group_by_reducer_dataset),\n",
      "-       (\"GroupByWindow\", make_group_by_window_dataset),\n",
      "-       (\"Scan\", make_scan_dataset)\n",
      "-   ]\n",
      "- \n",
      "-   def reduce_fn(x, y):\n",
      "-     name, dataset_fn = y\n",
      "-     return x + combinations.combine(\n",
      "-         dataset_fn=combinations.NamedObject(name, dataset_fn))\n",
      "- \n",
      "-   return functools.reduce(reduce_fn, cases, [])\n",
      "- \n",
      "- \n",
      "- def _disable_intra_op_parallelism_test_combinations():\n",
      "- \n",
      "-   def make_tensor_dataset():\n",
      "-     return dataset_ops.Dataset.from_tensors(42)\n",
      "- \n",
      "-   def make_map_dataset():\n",
      "-     return dataset_ops.Dataset.from_tensors(42).map(lambda x: x + 1)\n",
      "- \n",
      "-   cases = [\n",
      "-       (\"FromTensors\", make_tensor_dataset, [42]),\n",
      "-       (\"Map\", make_map_dataset, [43]),\n",
      "-   ]\n",
      "- \n",
      "-   def reduce_fn(x, y):\n",
      "-     name, dataset_fn, expected_output = y\n",
      "-     return x + combinations.combine(\n",
      "-         dataset_fn=combinations.NamedObject(name, dataset_fn),\n",
      "-         expected_output=[expected_output])\n",
      "- \n",
      "-   return functools.reduce(reduce_fn, cases, [])\n",
      "- \n",
      "- \n",
      "- class OptimizeDatasetTest(test_base.DatasetTestBase, parameterized.TestCase):\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationStatefulFunction(self):\n",
      "-     dataset = dataset_ops.Dataset.range(\n",
      "-         10).map(lambda _: random_ops.random_uniform([])).batch(10)\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     get_next = self.getNext(dataset)\n",
      "-     self.evaluate(get_next())\n",
      "- \n",
      "-   # TODO(b/123902160)\n",
      "-   @combinations.generate(test_base.graph_only_combinations())\n",
      "-   def testOptimizationLargeInputFromTensor(self):\n",
      "-     input_t = array_ops.placeholder(dtypes.int32, (None, None, None))\n",
      "-     dataset = dataset_ops.Dataset.from_tensors(input_t)\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     iterator = dataset_ops.make_initializable_iterator(dataset)\n",
      "-     init_op = iterator.initializer\n",
      "-     get_next = iterator.get_next()\n",
      "- \n",
      "-     with self.cached_session() as sess:\n",
      "-       sess.run(init_op, {input_t: np.ones([512, 1024, 1025], np.int32)})\n",
      "-       self.evaluate(get_next)\n",
      "- \n",
      "-   # TODO(b/123902160)\n",
      "-   @combinations.generate(test_base.graph_only_combinations())\n",
      "-   def testOptimizationLargeInputFromTensorSlices(self):\n",
      "-     input_t = array_ops.placeholder(dtypes.int32, (None, None, None, None))\n",
      "-     dataset = dataset_ops.Dataset.from_tensor_slices(input_t)\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     iterator = dataset_ops.make_initializable_iterator(dataset)\n",
      "-     init_op = iterator.initializer\n",
      "-     get_next = iterator.get_next()\n",
      "- \n",
      "-     with self.cached_session() as sess:\n",
      "-       sess.run(init_op, {input_t: np.ones([1, 512, 1024, 1025], np.int32)})\n",
      "-       self.evaluate(get_next)\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationNestedDataset(self):\n",
      "- \n",
      "-     def flat_map_fn(_):\n",
      "-       dataset = dataset_ops.Dataset.from_tensors(0)\n",
      "-       dataset = dataset.apply(testing.assert_next([\"MemoryCacheImpl\"]))\n",
      "-       dataset = dataset.skip(0)  # Should be removed by noop elimination\n",
      "-       dataset = dataset.cache()\n",
      "-       return dataset\n",
      "- \n",
      "-     dataset = dataset_ops.Dataset.range(1)\n",
      "-     dataset = dataset.flat_map(flat_map_fn)\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     options.experimental_optimization.noop_elimination = True\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     self.assertDatasetProduces(dataset, expected_output=[0])\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationNestedDatasetWithModifiedRetval(self):\n",
      "- \n",
      "-     def flat_map_fn(_):\n",
      "-       dataset = dataset_ops.Dataset.from_tensors(0)\n",
      "-       dataset = dataset.apply(testing.assert_next([\"MapAndBatch\"]))\n",
      "-       # Should be fused by map and batch fusion\n",
      "-       dataset = dataset.map(lambda x: x)\n",
      "-       dataset = dataset.batch(1)\n",
      "-       return dataset\n",
      "- \n",
      "-     dataset = dataset_ops.Dataset.range(1)\n",
      "-     dataset = dataset.flat_map(flat_map_fn)\n",
      "- \n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     options.experimental_optimization.map_and_batch_fusion = True\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     self.assertDatasetProduces(dataset, expected_output=[[0]])\n",
      "- \n",
      "-   @combinations.generate(\n",
      "-       combinations.times(test_base.default_test_combinations(),\n",
      "-                          _disable_intra_op_parallelism_test_combinations()))\n",
      "-   def testOptimizationDisableIntraOpParallelism(self, dataset_fn,\n",
      "-                                                 expected_output):\n",
      "-     os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"] = \"disable_intra_op_parallelism\"\n",
      "-     os.environ[\"TF_JOB_NAME\"] = \"test_job\"\n",
      "- \n",
      "-     dataset = dataset_fn()\n",
      "-     dataset = dataset.apply(testing.assert_next([\"MaxIntraOpParallelism\"]))\n",
      "- \n",
      "-     self.assertDatasetProduces(dataset, expected_output=expected_output)\n",
      "- \n",
      "-     del os.environ[\"TF_DATA_EXPERIMENT_OPT_IN\"]\n",
      "-     del os.environ[\"TF_JOB_NAME\"]\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationThreadPoolDataset(self):\n",
      "-     dataset = dataset_ops.Dataset.range(10).batch(10)\n",
      "- \n",
      "-     dataset = threadpool.override_threadpool(\n",
      "-         dataset,\n",
      "-         threadpool.PrivateThreadPool(\n",
      "-             2, display_name=\"private_thread_pool_%d\" % 2))\n",
      "- \n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     dataset = dataset.with_options(options)\n",
      "-     self.assertDatasetProduces(\n",
      "-         dataset,\n",
      "-         expected_output=[list(range(10))],\n",
      "-         requires_initialization=True)\n",
      "- \n",
      "-   # Reference variables are not supported in eager mode.\n",
      "-   @combinations.generate(\n",
      "-       combinations.times(test_base.graph_only_combinations(),\n",
      "-                          _captured_refvar_test_combinations()))\n",
      "-   def testOptimizationWithCapturedRefVar(self, dataset_fn):\n",
      "-     \"\"\"Tests that default optimizations are disabled with ref variables.\"\"\"\n",
      "-     variable = variable_scope.get_variable(\n",
      "-         \"v\", initializer=0, use_resource=False)\n",
      "-     assign_op = variable.assign_add(1)\n",
      "- \n",
      "-     # Check that warning is logged.\n",
      "-     warnings.simplefilter(\"always\")\n",
      "-     with warnings.catch_warnings(record=True) as w:\n",
      "-       unoptimized_dataset = dataset_fn(variable)\n",
      "- \n",
      "-       options = dataset_ops.Options()\n",
      "-       options.experimental_optimization.apply_default_optimizations = False\n",
      "-       options.experimental_optimization.noop_elimination = True\n",
      "-       options.experimental_optimization.map_and_batch_fusion = True\n",
      "-       optimized_dataset = unoptimized_dataset.with_options(options)\n",
      "-       optimized_it = dataset_ops.make_initializable_iterator(optimized_dataset)\n",
      "- \n",
      "-     self.assertGreaterEqual(len(w), 1)\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     expected = (\n",
      "-         \"tf.data graph rewrites are not compatible with \"\n",
      "-         \"tf.Variable. The following rewrites will be disabled: %s.\"\n",
      "-         \" To enable rewrites, use resource variables instead by \"\n",
      "-         \"calling `tf.enable_resource_variables()` at the start of the \"\n",
      "-         \"program.\" %\n",
      "-         (\", \".join(graph_rewrites.enabled + graph_rewrites.default)))\n",
      "-     self.assertTrue(any(expected in str(warning) for warning in w))\n",
      "- \n",
      "-     # Check that outputs are the same in the optimized and unoptimized cases,\n",
      "-     # when the variable value is changing.\n",
      "-     unoptimized_it = dataset_ops.make_initializable_iterator(\n",
      "-         unoptimized_dataset)\n",
      "-     with ops.control_dependencies([assign_op]):\n",
      "-       unoptimized_output = unoptimized_it.get_next()\n",
      "-       optimized_output = optimized_it.get_next()\n",
      "- \n",
      "-     self.evaluate(variable.initializer)\n",
      "-     self.evaluate((unoptimized_it.initializer, optimized_it.initializer))\n",
      "-     while True:\n",
      "-       try:\n",
      "-         unoptimized, optimized = self.evaluate((unoptimized_output,\n",
      "-                                                 optimized_output))\n",
      "-         self.assertEqual(unoptimized, optimized)\n",
      "-       except errors.OutOfRangeError:\n",
      "-         break\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationDefault(self):\n",
      "-     \"\"\"Tests the optimization settings by default.\"\"\"\n",
      "-     options = dataset_ops.Options()\n",
      "-     expected_optimizations_enabled = []\n",
      "-     expected_optimizations_disabled = []\n",
      "-     expected_optimizations_default = [\n",
      "-         \"map_and_batch_fusion\",\n",
      "-         \"noop_elimination\",\n",
      "-         \"shuffle_and_repeat_fusion\",\n",
      "-     ]\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     self.assertEqual(set(graph_rewrites.enabled),\n",
      "-                      set(expected_optimizations_enabled))\n",
      "-     self.assertEqual(set(graph_rewrites.disabled),\n",
      "-                      set(expected_optimizations_disabled))\n",
      "-     self.assertEqual(set(graph_rewrites.default),\n",
      "-                      set(expected_optimizations_default))\n",
      "- \n",
      "-     options.experimental_optimization.apply_default_optimizations = True\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     self.assertEqual(set(graph_rewrites.enabled),\n",
      "-                      set(expected_optimizations_enabled))\n",
      "-     self.assertEqual(set(graph_rewrites.disabled),\n",
      "-                      set(expected_optimizations_disabled))\n",
      "-     self.assertEqual(set(graph_rewrites.default),\n",
      "-                      set(expected_optimizations_default))\n",
      "- \n",
      "-     options.experimental_optimization.apply_default_optimizations = False\n",
      "-     expected_optimizations_default = []\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     self.assertEqual(set(graph_rewrites.enabled),\n",
      "-                      set(expected_optimizations_enabled))\n",
      "-     self.assertEqual(set(graph_rewrites.disabled),\n",
      "-                      set(expected_optimizations_disabled))\n",
      "-     self.assertEqual(set(graph_rewrites.default),\n",
      "-                      set(expected_optimizations_default))\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationEnabled(self):\n",
      "-     \"\"\"Tests the optimization settings by enabling all.\"\"\"\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.filter_fusion = True\n",
      "-     options.experimental_optimization.filter_with_random_uniform_fusion = True\n",
      "-     options.experimental_optimization.hoist_random_uniform = True\n",
      "-     options.experimental_optimization.map_and_batch_fusion = True\n",
      "-     options.experimental_optimization.map_and_filter_fusion = True\n",
      "-     options.experimental_optimization.map_parallelization = True\n",
      "-     options.experimental_optimization.map_fusion = True\n",
      "-     options.experimental_optimization.noop_elimination = True\n",
      "-     options.experimental_optimization.parallel_batch = True\n",
      "-     options.experimental_optimization.shuffle_and_repeat_fusion = True\n",
      "-     options.experimental_optimization.map_vectorization.enabled = True\n",
      "-     options.experimental_optimization.autotune_buffers = True\n",
      "-     options.experimental_deterministic = False\n",
      "-     options.experimental_stats.latency_all_edges = True\n",
      "-     options.experimental_slack = True\n",
      "- \n",
      "-     expected_optimizations_enabled = [\n",
      "-         \"filter_fusion\",\n",
      "-         \"filter_with_random_uniform_fusion\",\n",
      "-         \"hoist_random_uniform\",\n",
      "-         \"map_and_batch_fusion\",\n",
      "-         \"map_and_filter_fusion\",\n",
      "-         \"map_parallelization\",\n",
      "-         \"map_fusion\",\n",
      "-         \"noop_elimination\",\n",
      "-         \"parallel_batch\",\n",
      "-         \"shuffle_and_repeat_fusion\",\n",
      "-         \"map_vectorization\",\n",
      "-         \"inject_prefetch\",\n",
      "-         \"make_sloppy\",\n",
      "-         \"latency_all_edges\",\n",
      "-         \"slack\",\n",
      "-     ]\n",
      "-     expected_optimizations_disabled = []\n",
      "-     expected_optimizations_default = []\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     self.assertEqual(set(graph_rewrites.enabled),\n",
      "-                      set(expected_optimizations_enabled))\n",
      "-     self.assertEqual(set(graph_rewrites.disabled),\n",
      "-                      set(expected_optimizations_disabled))\n",
      "-     self.assertEqual(set(graph_rewrites.default),\n",
      "-                      set(expected_optimizations_default))\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testOptimizationDisabled(self):\n",
      "-     \"\"\"Tests the optimization settings by disabling all.\"\"\"\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.filter_fusion = False\n",
      "-     options.experimental_optimization.filter_with_random_uniform_fusion = False\n",
      "-     options.experimental_optimization.hoist_random_uniform = False\n",
      "-     options.experimental_optimization.map_and_batch_fusion = False\n",
      "-     options.experimental_optimization.map_and_filter_fusion = False\n",
      "-     options.experimental_optimization.map_parallelization = False\n",
      "-     options.experimental_optimization.map_fusion = False\n",
      "-     options.experimental_optimization.noop_elimination = False\n",
      "-     options.experimental_optimization.parallel_batch = False\n",
      "-     options.experimental_optimization.shuffle_and_repeat_fusion = False\n",
      "-     options.experimental_optimization.map_vectorization.enabled = False\n",
      "-     options.experimental_optimization.autotune = False\n",
      "-     options.experimental_deterministic = True\n",
      "-     options.experimental_stats.latency_all_edges = False\n",
      "-     options.experimental_slack = False\n",
      "- \n",
      "-     expected_optimizations_enabled = []\n",
      "-     expected_optimizations_disabled = [\n",
      "-         \"filter_fusion\",\n",
      "-         \"filter_with_random_uniform_fusion\",\n",
      "-         \"hoist_random_uniform\",\n",
      "-         \"map_and_batch_fusion\",\n",
      "-         \"map_and_filter_fusion\",\n",
      "-         \"map_parallelization\",\n",
      "-         \"map_fusion\",\n",
      "-         \"noop_elimination\",\n",
      "-         \"parallel_batch\",\n",
      "-         \"shuffle_and_repeat_fusion\",\n",
      "-         \"map_vectorization\",\n",
      "-         \"inject_prefetch\",\n",
      "-         \"make_sloppy\",\n",
      "-         \"latency_all_edges\",\n",
      "-         \"slack\",\n",
      "-     ]\n",
      "-     expected_optimizations_default = []\n",
      "-     graph_rewrites = options._graph_rewrites()\n",
      "-     self.assertEqual(set(graph_rewrites.enabled),\n",
      "-                      set(expected_optimizations_enabled))\n",
      "-     self.assertEqual(set(graph_rewrites.disabled),\n",
      "-                      set(expected_optimizations_disabled))\n",
      "-     self.assertEqual(set(graph_rewrites.default),\n",
      "-                      set(expected_optimizations_default))\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testAutotuningDefaults(self):\n",
      "-     options = dataset_ops.Options()\n",
      "- \n",
      "-     # Check defaults\n",
      "-     autotune, algorithm, cpu_budget, ram_budget = options._autotune_settings()\n",
      "-     self.assertTrue(autotune)\n",
      "-     self.assertEqual(algorithm,\n",
      "-                      optimization_options._AutotuneAlgorithm.HILL_CLIMB)\n",
      "-     self.assertEqual(cpu_budget, 0)\n",
      "-     self.assertEqual(ram_budget, 0)\n",
      "- \n",
      "-   @combinations.generate(test_base.default_test_combinations())\n",
      "-   def testAutotuningSettings(self):\n",
      "-     options = dataset_ops.Options()\n",
      "-     options.experimental_optimization.autotune_cpu_budget = 1000\n",
      "-     options.experimental_optimization.autotune_ram_budget = 999999999\n",
      "-     options.experimental_optimization.autotune_buffers = True\n",
      "-     self.assertIn(\"inject_prefetch\", options._graph_rewrites().enabled)\n",
      "-     autotune, algorithm, cpu_budget, ram_budget = options._autotune_settings()\n",
      "-     self.assertTrue(autotune)\n",
      "-     self.assertEqual(algorithm,\n",
      "-                      optimization_options._AutotuneAlgorithm.GRADIENT_DESCENT)\n",
      "-     self.assertEqual(cpu_budget, 1000)\n",
      "-     self.assertEqual(ram_budget, 999999999)\n",
      "- \n",
      "- if __name__ == \"__main__\":\n",
      "-   test.main()\n"
     ]
    }
   ],
   "source": [
    "origin_file_lines = origin_file.splitlines()\n",
    "parent_file_lines = parent_file.splitlines()\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(parent_file_lines, origin_file_lines)\n",
    "print('\\n'.join(list(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_file = getfile(filename=df_modified.iloc[3]['filename'], sha=df_modified.iloc[3]['sha'])\n",
    "parent_file = getfile(filename=df_modified.iloc[3]['filename'], sha=df_modified.iloc[3]['parents_sha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "\n",
      "from absl.testing import parameterized\n",
      "import numpy as np\n",
      "\n",
      "from tensorflow.python.data.ops import dataset_ops\n",
      "from tensorflow.python.distribute import mirrored_strategy\n",
      "from tensorflow.python.distribute import one_device_strategy\n",
      "from tensorflow.python.eager import backprop\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.framework import dtypes\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.keras import backend\n",
      "from tensorflow.python.keras import layers\n",
      "from tensorflow.python.keras import models\n",
      "from tensorflow.python.keras import regularizers\n",
      "from tensorflow.python.keras.engine import base_layer\n",
      "from tensorflow.python.keras.layers import core\n",
      "from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer\n",
      "from tensorflow.python.keras.mixed_precision.experimental import policy\n",
      "from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util\n",
      "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.ops import variables\n",
      "from tensorflow.python.platform import test\n",
      "from tensorflow.python.util import nest\n",
      "\n",
      "\n",
      "class AssertTypeLayer(base_layer.Layer):\n",
      "  \"\"\"A layer which asserts it's inputs are a certain type.\"\"\"\n",
      "\n",
      "  def __init__(self, assert_type=None, **kwargs):\n",
      "    self._assert_type = assert_type\n",
      "    super(AssertTypeLayer, self).__init__(**kwargs)\n",
      "\n",
      "  def assert_input_types(self, inputs):\n",
      "    \"\"\"Asserts `inputs` are of the correct type. Should be called in call().\"\"\"\n",
      "    if self._assert_type:\n",
      "      inputs_flattened = nest.flatten(inputs)\n",
      "      for inp in inputs_flattened:\n",
      "        assert inp.dtype.base_dtype == self._assert_type, (\n",
      "            'Input tensor has type %s which does not match assert type %s' %\n",
      "            (inp.dtype.name, self._assert_type.name))\n",
      "\n",
      "\n",
      "class AddLayer(AssertTypeLayer):\n",
      "  \"\"\"A layer which adds it's input to a scalar variable.\"\"\"\n",
      "\n",
      "  def __init__(self, regularizer=None, use_operator=False, **kwargs):\n",
      "    \"\"\"Initializes the AddLayer.\n",
      "\n",
      "    Args:\n",
      "      regularizer: The regularizer on the scalar variable.\n",
      "      use_operator: If True, add using the + operator. If False, add using\n",
      "        tf.add.\n",
      "      **kwargs: Passed to AssertTypeLayer constructor.\n",
      "    \"\"\"\n",
      "    self._regularizer = regularizer\n",
      "    self._use_operator = use_operator\n",
      "    super(AddLayer, self).__init__(**kwargs)\n",
      "\n",
      "  def build(self, _):\n",
      "    self.v = self.add_weight('v', (), initializer='ones',\n",
      "                             regularizer=self._regularizer)\n",
      "    self.built = True\n",
      "\n",
      "  def call(self, inputs):\n",
      "    self.assert_input_types(inputs)\n",
      "    assert inputs.dtype == self.v.dtype\n",
      "    return self._add(inputs, self.v)\n",
      "\n",
      "  def _add(self, x, y):\n",
      "    if self._use_operator:\n",
      "      return x + y\n",
      "    else:\n",
      "      return math_ops.add(x, y)\n",
      "\n",
      "\n",
      "class AddLayerWithoutAutoCast(AddLayer):\n",
      "  \"\"\"Same as AddLayer, but does not use AutoCastVariables.\"\"\"\n",
      "\n",
      "  def build(self, _):\n",
      "    dtype = self.dtype\n",
      "    if dtype in ('float16', 'bfloat16'):\n",
      "      dtype = 'float32'\n",
      "    self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\n",
      "                             experimental_autocast=False,\n",
      "                             regularizer=self._regularizer)\n",
      "    self.built = True\n",
      "\n",
      "  def call(self, inputs):\n",
      "    self.assert_input_types(inputs)\n",
      "    assert self.v.dtype in (dtypes.float32, dtypes.float64)\n",
      "    return self._add(inputs, math_ops.cast(self.v, inputs.dtype))\n",
      "\n",
      "\n",
      "class IdentityRegularizer(regularizers.Regularizer):\n",
      "\n",
      "  def __call__(self, x):\n",
      "    assert x.dtype == dtypes.float32\n",
      "    return array_ops.identity(x)\n",
      "\n",
      "\n",
      "def create_one_device_strategy():\n",
      "  return one_device_strategy.OneDeviceStrategy('cpu:0')\n",
      "\n",
      "\n",
      "def create_mirrored_strategy():\n",
      "  if context.num_gpus() >= 1:\n",
      "    return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\n",
      "  else:\n",
      "    return mirrored_strategy.MirroredStrategy(['cpu:0'])\n",
      "\n",
      "\n",
      "TESTCASES = ({\n",
      "    'testcase_name': 'base',\n",
      "    'strategy_fn': create_one_device_strategy\n",
      "}, {\n",
      "    'testcase_name': 'distribute',\n",
      "    'strategy_fn': create_mirrored_strategy\n",
      "})\n",
      "\n",
      "\n",
      "class KerasLayerTest(test.TestCase, parameterized.TestCase):\n",
      "  \"\"\"Test mixed precision with Keras layers.\"\"\"\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_variables_in_float32(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16)\n",
      "        y = layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "        self.assertEqual(y.dtype, dtypes.float16)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(y), 2.)\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_layer_with_non_autocast_variable(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)\n",
      "        y = layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "        self.assertEqual(y.dtype, dtypes.float16)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(y), 2.)\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_layer_regularizer_runs_in_float32(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        # Test on AddLayer\n",
      "        layer = AddLayer(assert_type=dtypes.float16,\n",
      "                         regularizer=IdentityRegularizer())\n",
      "        layer(x)\n",
      "        (regularizer_loss,) = layer.losses\n",
      "        self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n",
      "\n",
      "        # Test on AddLayerWithoutAutoCast\n",
      "        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                        regularizer=IdentityRegularizer())\n",
      "        layer(x)\n",
      "        (regularizer_loss,) = layer.losses\n",
      "        self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_passing_policy_to_layer(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      # Passing a Policy to 'dtype' sets the policy for that layer.\n",
      "      layer = AddLayer(assert_type=dtypes.float16,\n",
      "                       dtype=policy.Policy('infer_float32_vars'))\n",
      "      # layer.dtype refers to the variable dtype\n",
      "      self.assertEqual(layer.dtype, dtypes.float32)\n",
      "      layer(x)\n",
      "      self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        # Passing a Policy to dtype overrides the global Policy\n",
      "        layer = AddLayer(assert_type=dtypes.float16,\n",
      "                         dtype=policy.Policy('infer'))\n",
      "        # layer dtype is not yet known\n",
      "        self.assertEqual(layer.dtype, None)\n",
      "        layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float16)\n",
      "        self.assertEqual(layer.dtype, dtypes.float16)\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_gradient(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope() as strategy:\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16)\n",
      "        def run_fn():\n",
      "          with backprop.GradientTape() as tape:\n",
      "            y = layer(x)\n",
      "            # Divide by num_replicas_in_sync, as the effective total loss is the\n",
      "            # sum of each of the replica's losses.\n",
      "            y /= strategy.num_replicas_in_sync\n",
      "\n",
      "          # Learning rate is small enough that if applied to a float16 variable,\n",
      "          # the variable will not change. So this tests the learning rate is not\n",
      "          # applied to a float16 value, but instead the float32 variable.\n",
      "          opt = gradient_descent.SGD(2 ** -14)\n",
      "          grad = tape.gradient(y, layer.v)\n",
      "          return opt.apply_gradients([(grad, layer.v)])\n",
      "\n",
      "        op = strategy.experimental_run(run_fn)\n",
      "        if not context.executing_eagerly():\n",
      "          self.evaluate(variables.global_variables_initializer())\n",
      "          self.evaluate(op)\n",
      "        # The gradient with respective to the variable is 1. Since the\n",
      "        # variable is initialized with 1 and the learning rate is 2**-14, the\n",
      "        # new variable value should be: init_val - gradient * learning_rate,\n",
      "        # which is  1 - 1 * 2**-14\n",
      "        self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)\n",
      "\n",
      "\n",
      "class KerasModelTest(test.TestCase, parameterized.TestCase):\n",
      "  \"\"\"Test mixed precision with Keras models.\"\"\"\n",
      "\n",
      "  @parameterized.named_parameters({\n",
      "      'testcase_name': 'base',\n",
      "      'strategy_fn': create_one_device_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'distribute',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'operator',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "      'use_operator': True\n",
      "  }, {\n",
      "      'testcase_name': 'regularizer',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "      'use_regularizer': True\n",
      "  })\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):\n",
      "    regularizer = IdentityRegularizer() if use_regularizer else None\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "        layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,\n",
      "                         regularizer=regularizer)\n",
      "        y = layer(x)\n",
      "        y = math_ops.cast(y, dtypes.float32)\n",
      "        model = models.Model(inputs=x, outputs=y)\n",
      "\n",
      "        def loss_fn(y_true, y_pred):\n",
      "          del y_true\n",
      "          return math_ops.reduce_mean(y_pred)\n",
      "\n",
      "        # Learning rate is small enough that if applied to a float16 variable,\n",
      "        # the variable will not change. So this tests the learning rate not\n",
      "        # applied to a float16 value, but instead the float32 variable.\n",
      "        opt = gradient_descent.SGD(2 ** -14)\n",
      "        model.compile(opt, loss=loss_fn)\n",
      "\n",
      "      self.assertEqual(backend.eval(layer.v), 1)\n",
      "      x = np.ones((2, 1))\n",
      "      y = np.ones((2, 1))\n",
      "      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "      model.fit(dataset)\n",
      "      # Variable starts at 1, and should have gradient of 2 ** -14 subtracted\n",
      "      # from it.\n",
      "      expected = 1 - 2 ** -14\n",
      "      if use_regularizer:\n",
      "        # Regularizer adds another 2 ** -14 to the gradient.\n",
      "        expected -= 2 ** -14\n",
      "      self.assertEqual(backend.eval(layer.v), expected)\n",
      "\n",
      "  @parameterized.named_parameters({\n",
      "      'testcase_name': 'base',\n",
      "      'strategy_fn': create_one_device_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'distribute',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'loss_scaling',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "      'use_loss_scaling': True\n",
      "  })\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_advanced_model(self, strategy_fn, use_loss_scaling=False):\n",
      "\n",
      "    # The advanced model tests mixed-precision-related features that would occur\n",
      "    # in a resnet50 model. It tests a model that has:\n",
      "    #  * Multiple layers, some which use auto-cast variables and some which do\n",
      "    #    not\n",
      "    #  * Regularization on some variables and not others.\n",
      "    #  * Loss scaling (if use_loss_scaling is True)\n",
      "\n",
      "    strategy = strategy_fn()\n",
      "    if use_loss_scaling:\n",
      "      loss_scale = 8.\n",
      "    learning_rate = 2 ** -14\n",
      "\n",
      "    with strategy.scope():\n",
      "      with policy.policy_scope(policy.Policy('infer_float32_vars')):\n",
      "        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "        layer1 = AddLayer(assert_type=dtypes.float16,\n",
      "                          regularizer=IdentityRegularizer(), use_operator=True)\n",
      "        layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                         use_operator=True)\n",
      "        layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)\n",
      "        layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                         regularizer=IdentityRegularizer(),\n",
      "                                         use_operator=False)\n",
      "        y = layer1(x)\n",
      "        y = layer2(y)\n",
      "        y = layer3(y)\n",
      "        y = layer4(y)\n",
      "        if use_loss_scaling:\n",
      "          # The gradient of 'y' at this point is 1. With loss scaling, the\n",
      "          # gradient is 'loss_scale'. The DistributionStrategy additionally\n",
      "          # scales the gradient by 1/num_replicas in_sync. We divide by the\n",
      "          # batch size of 2 since the loss is averaged across batch elements.\n",
      "          expected_gradient = loss_scale / strategy.num_replicas_in_sync / 2\n",
      "          identity_with_grad_check_fn = (\n",
      "              mp_test_util.create_identity_with_grad_check_fn(\n",
      "                  expected_dtype=dtypes.float16,\n",
      "                  expected_gradient=[expected_gradient] * 2))\n",
      "          y = core.Lambda(identity_with_grad_check_fn)(y)\n",
      "        y = math_ops.cast(y, dtypes.float32)\n",
      "        model = models.Model(inputs=x, outputs=y)\n",
      "\n",
      "        def loss_fn(y_true, y_pred):\n",
      "          self.assertEqual(y_true.dtype, dtypes.float32)\n",
      "          self.assertEqual(y_pred.dtype, dtypes.float32)\n",
      "          return math_ops.reduce_mean(y_pred)\n",
      "\n",
      "        opt = gradient_descent.SGD(learning_rate)\n",
      "        if use_loss_scaling:\n",
      "          opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)\n",
      "        model.compile(opt, loss=loss_fn)\n",
      "\n",
      "      x = np.ones((2, 1))\n",
      "      y = np.ones((2, 1))\n",
      "      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "      model.fit(dataset)\n",
      "      for layer in (layer1, layer2, layer3, layer4):\n",
      "        if layer.losses:\n",
      "          # Layer has weight regularizer\n",
      "          self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)\n",
      "        else:\n",
      "          # Layer does not have weight regularizer\n",
      "          self.assertEqual(backend.eval(layer.v), 1 - learning_rate)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "  test.main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(origin_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  # Copyright 2019 The TensorFlow Authors. All Rights Reserved.',\n",
       " '  #',\n",
       " '  # Licensed under the Apache License, Version 2.0 (the \"License\");',\n",
       " '  # you may not use this file except in compliance with the License.',\n",
       " '  # You may obtain a copy of the License at',\n",
       " '  #',\n",
       " '  #     http://www.apache.org/licenses/LICENSE-2.0',\n",
       " '  #',\n",
       " '  # Unless required by applicable law or agreed to in writing, software',\n",
       " '  # distributed under the License is distributed on an \"AS IS\" BASIS,',\n",
       " '  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.',\n",
       " '  # See the License for the specific language governing permissions and',\n",
       " '  # limitations under the License.',\n",
       " '  # ==============================================================================',\n",
       " '  \"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"',\n",
       " '  from __future__ import absolute_import',\n",
       " '  from __future__ import division',\n",
       " '  from __future__ import print_function',\n",
       " '  from absl.testing import parameterized',\n",
       " '  import numpy as np',\n",
       " '  from tensorflow.python.data.ops import dataset_ops',\n",
       " '  from tensorflow.python.distribute import mirrored_strategy',\n",
       " '  from tensorflow.python.distribute import one_device_strategy',\n",
       " '  from tensorflow.python.eager import backprop',\n",
       " '  from tensorflow.python.eager import context',\n",
       " '  from tensorflow.python.framework import constant_op',\n",
       " '  from tensorflow.python.framework import dtypes',\n",
       " '  from tensorflow.python.framework import test_util',\n",
       " '  from tensorflow.python.keras import backend',\n",
       " '  from tensorflow.python.keras import layers',\n",
       " '  from tensorflow.python.keras import models',\n",
       " '  from tensorflow.python.keras import regularizers',\n",
       " '  from tensorflow.python.keras.engine import base_layer',\n",
       " '  from tensorflow.python.keras.mixed_precision.experimental import policy',\n",
       " '  from tensorflow.python.keras.optimizer_v2 import gradient_descent',\n",
       " '  from tensorflow.python.ops import array_ops',\n",
       " '  from tensorflow.python.ops import math_ops',\n",
       " '  from tensorflow.python.ops import variables',\n",
       " '  from tensorflow.python.platform import test',\n",
       " '  from tensorflow.python.util import nest',\n",
       " '  class AssertTypeLayer(base_layer.Layer):',\n",
       " '    \"\"\"A layer which asserts it\\'s inputs are a certain type.\"\"\"',\n",
       " '    def __init__(self, assert_type=None, **kwargs):',\n",
       " '      self._assert_type = assert_type',\n",
       " '      super(AssertTypeLayer, self).__init__(**kwargs)',\n",
       " '    def assert_input_types(self, inputs):',\n",
       " '      \"\"\"Asserts `inputs` are of the correct type. Should be called in call().\"\"\"',\n",
       " '      if self._assert_type:',\n",
       " '        inputs_flattened = nest.flatten(inputs)',\n",
       " '        for inp in inputs_flattened:',\n",
       " '          assert inp.dtype.base_dtype == self._assert_type, (',\n",
       " \"              'Input tensor has type %s which does not match assert type %s' %\",\n",
       " '              (inp.dtype.name, self._assert_type.name))',\n",
       " '  class AddLayer(AssertTypeLayer):',\n",
       " '    \"\"\"A layer which adds it\\'s input to a scalar variable.\"\"\"',\n",
       " '    def __init__(self, regularizer=None, use_operator=False, **kwargs):',\n",
       " '      \"\"\"Initializes the AddLayer.',\n",
       " '      Args:',\n",
       " '        regularizer: The regularizer on the scalar variable.',\n",
       " '        use_operator: If True, add using the + operator. If False, add using',\n",
       " '          tf.add.',\n",
       " '        **kwargs: Passed to AssertTypeLayer constructor.',\n",
       " '      \"\"\"',\n",
       " '      self._regularizer = regularizer',\n",
       " '      self._use_operator = use_operator',\n",
       " '      super(AddLayer, self).__init__(**kwargs)',\n",
       " '    def build(self, _):',\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones',\",\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert inputs.dtype == self.v.dtype',\n",
       " '      return self._add(inputs, self.v)',\n",
       " '    def _add(self, x, y):',\n",
       " '      if self._use_operator:',\n",
       " '        return x + y',\n",
       " '      else:',\n",
       " '        return math_ops.add(x, y)',\n",
       " '  class AddLayerWithoutAutoCast(AddLayer):',\n",
       " '    \"\"\"Same as AddLayer, but does not use AutoCastVariables.\"\"\"',\n",
       " '    def build(self, _):',\n",
       " '      dtype = self.dtype',\n",
       " \"      if dtype in ('float16', 'bfloat16'):\",\n",
       " \"        dtype = 'float32'\",\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\",\n",
       " '                               experimental_autocast=False,',\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert self.v.dtype in (dtypes.float32, dtypes.float64)',\n",
       " '      return self._add(inputs, math_ops.cast(self.v, inputs.dtype))',\n",
       " '  class IdentityRegularizer(regularizers.Regularizer):',\n",
       " '    def __call__(self, x):',\n",
       " '      assert x.dtype == dtypes.float32',\n",
       " '      return array_ops.identity(x)',\n",
       " '  def create_one_device_strategy():',\n",
       " \"    return one_device_strategy.OneDeviceStrategy('cpu:0')\",\n",
       " '  def create_mirrored_strategy():',\n",
       " '    if context.num_gpus() >= 1:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\",\n",
       " '    else:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0'])\",\n",
       " '  TESTCASES = ({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy\",\n",
       " '  })',\n",
       " '  class KerasLayerTest(test.TestCase, parameterized.TestCase):',\n",
       " '    \"\"\"Test mixed precision with Keras layers.\"\"\"',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_variables_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_with_non_autocast_variable(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_regularizer_runs_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          # Test on AddLayer',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " '                           regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '          # Test on AddLayerWithoutAutoCast',\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                          regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_passing_policy_to_layer(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        # Passing a Policy to 'dtype' sets the policy for that layer.\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                         dtype=policy.Policy('infer_float32_vars'))\",\n",
       " '        # layer.dtype refers to the variable dtype',\n",
       " '        self.assertEqual(layer.dtype, dtypes.float32)',\n",
       " '        layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          # Passing a Policy to dtype overrides the global Policy',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                           dtype=policy.Policy('infer'))\",\n",
       " '          # layer dtype is not yet known',\n",
       " '          self.assertEqual(layer.dtype, None)',\n",
       " '          layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float16)',\n",
       " '          self.assertEqual(layer.dtype, dtypes.float16)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_gradient(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope() as strategy:',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          def run_fn():',\n",
       " '            with backprop.GradientTape() as tape:',\n",
       " '              y = layer(x)',\n",
       " '              # Divide by num_replicas_in_sync, as the effective total loss is the',\n",
       " \"              # sum of each of the replica's losses.\",\n",
       " '              y /= strategy.num_replicas_in_sync',\n",
       " '            # Learning rate is small enough that if applied to a float16 variable,',\n",
       " '            # the variable will not change. So this tests the learning rate is not',\n",
       " '            # applied to a float16 value, but instead the float32 variable.',\n",
       " '            opt = gradient_descent.SGD(2 ** -14)',\n",
       " '            grad = tape.gradient(y, layer.v)',\n",
       " '            return opt.apply_gradients([(grad, layer.v)])',\n",
       " '          op = strategy.experimental_run(run_fn)',\n",
       " '          if not context.executing_eagerly():',\n",
       " '            self.evaluate(variables.global_variables_initializer())',\n",
       " '            self.evaluate(op)',\n",
       " '          # The gradient with respective to the variable is 1. Since the',\n",
       " '          # variable is initialized with 1 and the learning rate is 2**-14, the',\n",
       " '          # new variable value should be: init_val - gradient * learning_rate,',\n",
       " '          # which is  1 - 1 * 2**-14',\n",
       " '          self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)',\n",
       " '  class KerasModelTest(test.TestCase, parameterized.TestCase):',\n",
       " '    \"\"\"Test mixed precision with Keras models.\"\"\"',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'operator',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_operator': True\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'regularizer',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_regularizer': True\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):',\n",
       " '      regularizer = IdentityRegularizer() if use_regularizer else None',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,',\n",
       " '                           regularizer=regularizer)',\n",
       " '          y = layer(x)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            del y_true',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '          # Learning rate is small enough that if applied to a float16 variable,',\n",
       " '          # the variable will not change. So this tests the learning rate not',\n",
       " '          # applied to a float16 value, but instead the float32 variable.',\n",
       " '          opt = gradient_descent.SGD(2 ** -14)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '        self.assertEqual(backend.eval(layer.v), 1)',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        # Variable starts at 1, and should have gradient of 2 ** -14 subtracted',\n",
       " '        # from it.',\n",
       " '        expected = 1 - 2 ** -14',\n",
       " '        if use_regularizer:',\n",
       " '          # Regularizer adds another 2 ** -14 to the gradient.',\n",
       " '          expected -= 2 ** -14',\n",
       " '        self.assertEqual(backend.eval(layer.v), expected)',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '#   def test_advanced_model(self, strategy_fn):',\n",
       " '?                                            ++++++++++++++++++++++++\\n',\n",
       " '      # The advanced model tests mixed-precision-related features that would occur',\n",
       " '      # in a resnet50 model. It tests a model that has:',\n",
       " '      #  * Multiple layers, some which use auto-cast variables and some which do',\n",
       " '      #    not',\n",
       " '      #  * Regularization on some variables and not others.',\n",
       " '      strategy = strategy_fn()',\n",
       " '      learning_rate = 2 ** -14',\n",
       " '      with strategy.scope():',\n",
       " \"        with policy.policy_scope(policy.Policy('infer_float32_vars')):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer1 = AddLayer(assert_type=dtypes.float16,',\n",
       " '                            regularizer=IdentityRegularizer(), use_operator=True)',\n",
       " '          layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           use_operator=True)',\n",
       " '          layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)',\n",
       " '          layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           regularizer=IdentityRegularizer(),',\n",
       " '                                           use_operator=False)',\n",
       " '          y = layer1(x)',\n",
       " '          y = layer2(y)',\n",
       " '          y = layer3(y)',\n",
       " '          y = layer4(y)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            self.assertEqual(y_true.dtype, dtypes.float32)',\n",
       " '            self.assertEqual(y_pred.dtype, dtypes.float32)',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '          opt = gradient_descent.SGD(learning_rate)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        for layer in (layer1, layer2, layer3, layer4):',\n",
       " '          if layer.losses:',\n",
       " '            # Layer has weight regularizer',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)',\n",
       " '          else:',\n",
       " '            # Layer does not have weight regularizer',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - learning_rate)',\n",
       " \"  if __name__ == '__main__':\",\n",
       " '    test.main()']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_diff_file(origin_file, parent_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提前删docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting astor\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: astor\n",
      "Successfully installed astor-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install astor -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astor\n",
    "\n",
    "def delete_docstring(text):\n",
    "    parsed = ast.parse(text)\n",
    "    for node in ast.walk(parsed):\n",
    "        # let's work only on functions & classes definitions\n",
    "        if not isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n",
    "            continue\n",
    "\n",
    "        if not len(node.body):\n",
    "            continue\n",
    "\n",
    "        if not isinstance(node.body[0], ast.Expr):\n",
    "            continue\n",
    "\n",
    "        if not hasattr(node.body[0], 'value') or not isinstance(node.body[0].value, ast.Str):\n",
    "            continue\n",
    "\n",
    "        # Uncomment lines below if you want print what and where we are removing\n",
    "        print(node)\n",
    "        print(node.body[0].value.s)\n",
    "\n",
    "        node.body = node.body[1:]\n",
    "        return astor.to_source(parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_ast.ClassDef object at 0x7f3e39efc430>\n",
      "A layer which asserts it's inputs are a certain type.\n",
      "\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "from absl.testing import parameterized\n",
      "import numpy as np\n",
      "from tensorflow.python.data.ops import dataset_ops\n",
      "from tensorflow.python.distribute import mirrored_strategy\n",
      "from tensorflow.python.distribute import one_device_strategy\n",
      "from tensorflow.python.eager import backprop\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.framework import dtypes\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.keras import backend\n",
      "from tensorflow.python.keras import layers\n",
      "from tensorflow.python.keras import models\n",
      "from tensorflow.python.keras import regularizers\n",
      "from tensorflow.python.keras.engine import base_layer\n",
      "from tensorflow.python.keras.layers import core\n",
      "from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer\n",
      "from tensorflow.python.keras.mixed_precision.experimental import policy\n",
      "from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util\n",
      "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.ops import variables\n",
      "from tensorflow.python.platform import test\n",
      "from tensorflow.python.util import nest\n",
      "\n",
      "\n",
      "class AssertTypeLayer(base_layer.Layer):\n",
      "\n",
      "    def __init__(self, assert_type=None, **kwargs):\n",
      "        self._assert_type = assert_type\n",
      "        super(AssertTypeLayer, self).__init__(**kwargs)\n",
      "\n",
      "    def assert_input_types(self, inputs):\n",
      "        \"\"\"Asserts `inputs` are of the correct type. Should be called in call().\"\"\"\n",
      "        if self._assert_type:\n",
      "            inputs_flattened = nest.flatten(inputs)\n",
      "            for inp in inputs_flattened:\n",
      "                assert inp.dtype.base_dtype == self._assert_type, 'Input tensor has type %s which does not match assert type %s' % (\n",
      "                    inp.dtype.name, self._assert_type.name)\n",
      "\n",
      "\n",
      "class AddLayer(AssertTypeLayer):\n",
      "    \"\"\"A layer which adds it's input to a scalar variable.\"\"\"\n",
      "\n",
      "    def __init__(self, regularizer=None, use_operator=False, **kwargs):\n",
      "        \"\"\"Initializes the AddLayer.\n",
      "\n",
      "    Args:\n",
      "      regularizer: The regularizer on the scalar variable.\n",
      "      use_operator: If True, add using the + operator. If False, add using\n",
      "        tf.add.\n",
      "      **kwargs: Passed to AssertTypeLayer constructor.\n",
      "    \"\"\"\n",
      "        self._regularizer = regularizer\n",
      "        self._use_operator = use_operator\n",
      "        super(AddLayer, self).__init__(**kwargs)\n",
      "\n",
      "    def build(self, _):\n",
      "        self.v = self.add_weight('v', (), initializer='ones', regularizer=\n",
      "            self._regularizer)\n",
      "        self.built = True\n",
      "\n",
      "    def call(self, inputs):\n",
      "        self.assert_input_types(inputs)\n",
      "        assert inputs.dtype == self.v.dtype\n",
      "        return self._add(inputs, self.v)\n",
      "\n",
      "    def _add(self, x, y):\n",
      "        if self._use_operator:\n",
      "            return x + y\n",
      "        else:\n",
      "            return math_ops.add(x, y)\n",
      "\n",
      "\n",
      "class AddLayerWithoutAutoCast(AddLayer):\n",
      "    \"\"\"Same as AddLayer, but does not use AutoCastVariables.\"\"\"\n",
      "\n",
      "    def build(self, _):\n",
      "        dtype = self.dtype\n",
      "        if dtype in ('float16', 'bfloat16'):\n",
      "            dtype = 'float32'\n",
      "        self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\n",
      "            experimental_autocast=False, regularizer=self._regularizer)\n",
      "        self.built = True\n",
      "\n",
      "    def call(self, inputs):\n",
      "        self.assert_input_types(inputs)\n",
      "        assert self.v.dtype in (dtypes.float32, dtypes.float64)\n",
      "        return self._add(inputs, math_ops.cast(self.v, inputs.dtype))\n",
      "\n",
      "\n",
      "class IdentityRegularizer(regularizers.Regularizer):\n",
      "\n",
      "    def __call__(self, x):\n",
      "        assert x.dtype == dtypes.float32\n",
      "        return array_ops.identity(x)\n",
      "\n",
      "\n",
      "def create_one_device_strategy():\n",
      "    return one_device_strategy.OneDeviceStrategy('cpu:0')\n",
      "\n",
      "\n",
      "def create_mirrored_strategy():\n",
      "    if context.num_gpus() >= 1:\n",
      "        return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\n",
      "    else:\n",
      "        return mirrored_strategy.MirroredStrategy(['cpu:0'])\n",
      "\n",
      "\n",
      "TESTCASES = {'testcase_name': 'base', 'strategy_fn': create_one_device_strategy\n",
      "    }, {'testcase_name': 'distribute', 'strategy_fn': create_mirrored_strategy}\n",
      "\n",
      "\n",
      "class KerasLayerTest(test.TestCase, parameterized.TestCase):\n",
      "    \"\"\"Test mixed precision with Keras layers.\"\"\"\n",
      "\n",
      "    @parameterized.named_parameters(*TESTCASES)\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_variables_in_float32(self, strategy_fn):\n",
      "        x = constant_op.constant([1.0], dtype=dtypes.float16)\n",
      "        with strategy_fn().scope():\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                layer = AddLayer(assert_type=dtypes.float16)\n",
      "                y = layer(x)\n",
      "                self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "                self.assertEqual(y.dtype, dtypes.float16)\n",
      "                self.evaluate(variables.global_variables_initializer())\n",
      "                self.assertEqual(self.evaluate(y), 2.0)\n",
      "\n",
      "    @parameterized.named_parameters(*TESTCASES)\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_layer_with_non_autocast_variable(self, strategy_fn):\n",
      "        x = constant_op.constant([1.0], dtype=dtypes.float16)\n",
      "        with strategy_fn().scope():\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)\n",
      "                y = layer(x)\n",
      "                self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "                self.assertEqual(y.dtype, dtypes.float16)\n",
      "                self.evaluate(variables.global_variables_initializer())\n",
      "                self.assertEqual(self.evaluate(y), 2.0)\n",
      "\n",
      "    @parameterized.named_parameters(*TESTCASES)\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_layer_regularizer_runs_in_float32(self, strategy_fn):\n",
      "        x = constant_op.constant([1.0], dtype=dtypes.float16)\n",
      "        with strategy_fn().scope():\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                layer = AddLayer(assert_type=dtypes.float16, regularizer=\n",
      "                    IdentityRegularizer())\n",
      "                layer(x)\n",
      "                regularizer_loss, = layer.losses\n",
      "                self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "                self.evaluate(variables.global_variables_initializer())\n",
      "                self.assertEqual(self.evaluate(regularizer_loss), 1.0)\n",
      "                layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                    regularizer=IdentityRegularizer())\n",
      "                layer(x)\n",
      "                regularizer_loss, = layer.losses\n",
      "                self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "                self.evaluate(variables.global_variables_initializer())\n",
      "                self.assertEqual(self.evaluate(regularizer_loss), 1.0)\n",
      "\n",
      "    @parameterized.named_parameters(*TESTCASES)\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_passing_policy_to_layer(self, strategy_fn):\n",
      "        x = constant_op.constant([1.0], dtype=dtypes.float16)\n",
      "        with strategy_fn().scope():\n",
      "            layer = AddLayer(assert_type=dtypes.float16, dtype=policy.\n",
      "                Policy('infer_float32_vars'))\n",
      "            self.assertEqual(layer.dtype, dtypes.float32)\n",
      "            layer(x)\n",
      "            self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                layer = AddLayer(assert_type=dtypes.float16, dtype=policy.\n",
      "                    Policy('infer'))\n",
      "                self.assertEqual(layer.dtype, None)\n",
      "                layer(x)\n",
      "                self.assertEqual(layer.v.dtype, dtypes.float16)\n",
      "                self.assertEqual(layer.dtype, dtypes.float16)\n",
      "\n",
      "    @parameterized.named_parameters(*TESTCASES)\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_gradient(self, strategy_fn):\n",
      "        x = constant_op.constant([1.0], dtype=dtypes.float16)\n",
      "        with strategy_fn().scope() as strategy:\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                layer = AddLayer(assert_type=dtypes.float16)\n",
      "\n",
      "                def run_fn():\n",
      "                    with backprop.GradientTape() as tape:\n",
      "                        y = layer(x)\n",
      "                        y /= strategy.num_replicas_in_sync\n",
      "                    opt = gradient_descent.SGD(2 ** -14)\n",
      "                    grad = tape.gradient(y, layer.v)\n",
      "                    return opt.apply_gradients([(grad, layer.v)])\n",
      "                op = strategy.experimental_run(run_fn)\n",
      "                if not context.executing_eagerly():\n",
      "                    self.evaluate(variables.global_variables_initializer())\n",
      "                    self.evaluate(op)\n",
      "                self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)\n",
      "\n",
      "\n",
      "class KerasModelTest(test.TestCase, parameterized.TestCase):\n",
      "    \"\"\"Test mixed precision with Keras models.\"\"\"\n",
      "\n",
      "    @parameterized.named_parameters({'testcase_name': 'base', 'strategy_fn':\n",
      "        create_one_device_strategy}, {'testcase_name': 'distribute',\n",
      "        'strategy_fn': create_mirrored_strategy}, {'testcase_name':\n",
      "        'operator', 'strategy_fn': create_mirrored_strategy, 'use_operator':\n",
      "        True}, {'testcase_name': 'regularizer', 'strategy_fn':\n",
      "        create_mirrored_strategy, 'use_regularizer': True})\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_model(self, strategy_fn, use_operator=False, use_regularizer=False\n",
      "        ):\n",
      "        regularizer = IdentityRegularizer() if use_regularizer else None\n",
      "        with strategy_fn().scope():\n",
      "            with policy.policy_scope('infer_float32_vars'):\n",
      "                x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "                layer = AddLayer(assert_type=dtypes.float16, use_operator=\n",
      "                    use_operator, regularizer=regularizer)\n",
      "                y = layer(x)\n",
      "                y = math_ops.cast(y, dtypes.float32)\n",
      "                model = models.Model(inputs=x, outputs=y)\n",
      "\n",
      "                def loss_fn(y_true, y_pred):\n",
      "                    del y_true\n",
      "                    return math_ops.reduce_mean(y_pred)\n",
      "                opt = gradient_descent.SGD(2 ** -14)\n",
      "                model.compile(opt, loss=loss_fn)\n",
      "            self.assertEqual(backend.eval(layer.v), 1)\n",
      "            x = np.ones((2, 1))\n",
      "            y = np.ones((2, 1))\n",
      "            dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "            model.fit(dataset)\n",
      "            expected = 1 - 2 ** -14\n",
      "            if use_regularizer:\n",
      "                expected -= 2 ** -14\n",
      "            self.assertEqual(backend.eval(layer.v), expected)\n",
      "\n",
      "    @parameterized.named_parameters({'testcase_name': 'base', 'strategy_fn':\n",
      "        create_one_device_strategy}, {'testcase_name': 'distribute',\n",
      "        'strategy_fn': create_mirrored_strategy}, {'testcase_name':\n",
      "        'loss_scaling', 'strategy_fn': create_mirrored_strategy,\n",
      "        'use_loss_scaling': True})\n",
      "    @test_util.run_in_graph_and_eager_modes\n",
      "    def test_advanced_model(self, strategy_fn, use_loss_scaling=False):\n",
      "        strategy = strategy_fn()\n",
      "        if use_loss_scaling:\n",
      "            loss_scale = 8.0\n",
      "        learning_rate = 2 ** -14\n",
      "        with strategy.scope():\n",
      "            with policy.policy_scope(policy.Policy('infer_float32_vars')):\n",
      "                x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "                layer1 = AddLayer(assert_type=dtypes.float16, regularizer=\n",
      "                    IdentityRegularizer(), use_operator=True)\n",
      "                layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                    use_operator=True)\n",
      "                layer3 = AddLayer(assert_type=dtypes.float16, use_operator=\n",
      "                    False)\n",
      "                layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                    regularizer=IdentityRegularizer(), use_operator=False)\n",
      "                y = layer1(x)\n",
      "                y = layer2(y)\n",
      "                y = layer3(y)\n",
      "                y = layer4(y)\n",
      "                if use_loss_scaling:\n",
      "                    expected_gradient = (loss_scale / strategy.\n",
      "                        num_replicas_in_sync / 2)\n",
      "                    identity_with_grad_check_fn = (mp_test_util.\n",
      "                        create_identity_with_grad_check_fn(expected_dtype=\n",
      "                        dtypes.float16, expected_gradient=[\n",
      "                        expected_gradient] * 2))\n",
      "                    y = core.Lambda(identity_with_grad_check_fn)(y)\n",
      "                y = math_ops.cast(y, dtypes.float32)\n",
      "                model = models.Model(inputs=x, outputs=y)\n",
      "\n",
      "                def loss_fn(y_true, y_pred):\n",
      "                    self.assertEqual(y_true.dtype, dtypes.float32)\n",
      "                    self.assertEqual(y_pred.dtype, dtypes.float32)\n",
      "                    return math_ops.reduce_mean(y_pred)\n",
      "                opt = gradient_descent.SGD(learning_rate)\n",
      "                if use_loss_scaling:\n",
      "                    opt = loss_scale_optimizer.LossScaleOptimizer(opt,\n",
      "                        loss_scale)\n",
      "                model.compile(opt, loss=loss_fn)\n",
      "            x = np.ones((2, 1))\n",
      "            y = np.ones((2, 1))\n",
      "            dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "            model.fit(dataset)\n",
      "            for layer in (layer1, layer2, layer3, layer4):\n",
      "                if layer.losses:\n",
      "                    self.assertEqual(backend.eval(layer.v), 1 - 2 *\n",
      "                        learning_rate)\n",
      "                else:\n",
      "                    self.assertEqual(backend.eval(layer.v), 1 - learning_rate)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    test.main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(delete_docstring(origin_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "def create_generator(data):\n",
    "    \"\"\"字符流生成器，在内部构建了一个闭包。\n",
    "    为了节约内存，避免一次性加载文件内容\"\"\"\n",
    "\n",
    "    def generator():\n",
    "        for elem in data:\n",
    "            try:\n",
    "                yield str.encode(elem)\n",
    "            except GeneratorExit:\n",
    "                logging.info(f\"GeneratorExit! EOF. {elem}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logging.info(\"Exception!\" + str(type(e)), str(e))\n",
    "                yield str.encode('')\n",
    "\n",
    "    g = generator()  # 生成器\n",
    "\n",
    "    def next_element():\n",
    "        return next(g)\n",
    "\n",
    "    return next_element  # 迭代器\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Returns 'source' minus comments and docstrings.\n",
    "    \"\"\"\n",
    "#     io_obj = cStringIO.StringIO(source)\n",
    "    out = \"\"\n",
    "    prev_toktype = tokenize.INDENT\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    \n",
    "    data_generator = create_generator(source)\n",
    "    \n",
    "#     tokens_iterator = tokenize.generate_tokens(data_generator)\n",
    "    tokens_iterator = tokenize.tokenize(BytesIO(source.encode('utf-8')).readline)\n",
    "    \n",
    "    for tok in tokens_iterator:\n",
    "        token_type = tok[0]\n",
    "        token_string = tok[1]\n",
    "        start_line, start_col = tok[2]\n",
    "        end_line, end_col = tok[3]\n",
    "        ltext = tok[4]\n",
    "        # The following two conditionals preserve indentation.\n",
    "        # This is necessary because we're not using tokenize.untokenize()\n",
    "        # (because it spits out code with copious amounts of oddly-placed\n",
    "        # whitespace).\n",
    "        if start_line > last_lineno:\n",
    "            last_col = 0\n",
    "        if start_col > last_col:\n",
    "            out += (\" \" * (start_col - last_col))\n",
    "        # Remove comments:\n",
    "#         if token_type == tokenize.COMMENT:\n",
    "#             pass\n",
    "        # This series of conditionals removes docstrings:\n",
    "        elif token_type == tokenize.STRING:\n",
    "            if prev_toktype != tokenize.INDENT:\n",
    "        # This is likely a docstring; double-check we're not inside an operator:\n",
    "                if prev_toktype != tokenize.NEWLINE:\n",
    "                    # Note regarding NEWLINE vs NL: The tokenize module\n",
    "                    # differentiates between newlines that start a new statement\n",
    "                    # and newlines inside of operators such as parens, brackes,\n",
    "                    # and curly braces.  Newlines inside of operators are\n",
    "                    # NEWLINE and newlines that start new code are NL.\n",
    "                    # Catch whole-module docstrings:\n",
    "                    if start_col > 0:\n",
    "                        # Unlabelled indentation means we're inside an operator\n",
    "                        out += token_string\n",
    "                    # Note regarding the INDENT token: The tokenize module does\n",
    "                    # not label indentation inside of an operator (parens,\n",
    "                    # brackets, and curly braces) as actual indentation.\n",
    "                    # For example:\n",
    "                    # def foo():\n",
    "                    #     \"The spaces before this docstring are tokenize.INDENT\"\n",
    "                    #     test = [\n",
    "                    #         \"The spaces before this string do not get a token\"\n",
    "                    #     ]\n",
    "        else:\n",
    "            out += token_string\n",
    "        prev_toktype = token_type\n",
    "        last_col = end_col\n",
    "        last_lineno = end_line\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ==============================================================================\n",
      "\n",
      "from   \n",
      "from   \n",
      "from   \n",
      "\n",
      "from .testing  \n",
      "import   \n",
      "\n",
      "from .python.data.ops  \n",
      "from .python.distribute  \n",
      "from .python.distribute  \n",
      "from .python.eager  \n",
      "from .python.eager  \n",
      "from .python.framework  \n",
      "from .python.framework  \n",
      "from .python.framework  \n",
      "from .python.keras  \n",
      "from .python.keras  \n",
      "from .python.keras  \n",
      "from .python.keras  \n",
      "from .python.keras.engine  \n",
      "from .python.keras.layers  \n",
      "from .python.keras.mixed_precision.experimental  \n",
      "from .python.keras.mixed_precision.experimental  \n",
      "from .python.keras.mixed_precision.experimental    \n",
      "from .python.keras.optimizer_v2  \n",
      "from .python.ops  \n",
      "from .python.ops  \n",
      "from .python.ops  \n",
      "from .python.platform  \n",
      "from .python.util  \n",
      "\n",
      "\n",
      "class (base_layer.Layer):\n",
      "  \n",
      "\n",
      "   (self, =None, kwargs):\n",
      "    self._assert_type  \n",
      "    (AssertTypeLayer, ).__init__(**kwargs)\n",
      "\n",
      "  def (self, ):\n",
      "    \n",
      "     ._assert_type:\n",
      "      inputs_flattened  .flatten(inputs)\n",
      "         :\n",
      "        assert .dtype.base_dtype  ._assert_type, \n",
      "             \n",
      "            inp.dtype.name, ._assert_type.name))\n",
      "\n",
      "\n",
      "class (AssertTypeLayer):\n",
      "  \n",
      "\n",
      "   (self, =None, =False, kwargs):\n",
      "    \n",
      "    ._regularizer  \n",
      "    ._use_operator  \n",
      "    (AddLayer, ).__init__(**kwargs)\n",
      "\n",
      "  def (self, ):\n",
      "    self.v  .add_weight('v', ), ='ones',\n",
      "                             =self._regularizer)\n",
      "    .built  \n",
      "\n",
      "  def (self, ):\n",
      "    self.assert_input_types(inputs)\n",
      "     .dtype  .v.dtype\n",
      "     ._add(inputs, .v)\n",
      "\n",
      "  def (self, , ):\n",
      "    if ._use_operator:\n",
      "      return   \n",
      "    else:\n",
      "      return .add(x, )\n",
      "\n",
      "\n",
      "class (AddLayer):\n",
      "  \n",
      "\n",
      "   (self, ):\n",
      "    dtype  .dtype\n",
      "       'float16', ):\n",
      "      dtype  \n",
      "    self.v  .add_weight('v', ), ='ones', =dtype,\n",
      "                             =False,\n",
      "                             =self._regularizer)\n",
      "    .built  \n",
      "\n",
      "  def (self, ):\n",
      "    self.assert_input_types(inputs)\n",
      "     .v.dtype  dtypes.float32, .float64)\n",
      "     ._add(inputs, .cast(self.v, .dtype))\n",
      "\n",
      "\n",
      "class (regularizers.Regularizer):\n",
      "\n",
      "  def (self, ):\n",
      "    assert .dtype  .float32\n",
      "     .identity(x)\n",
      "\n",
      "\n",
      "def ():\n",
      "  return .OneDeviceStrategy('cpu:0')\n",
      "\n",
      "\n",
      "def ():\n",
      "  if .num_gpus()  :\n",
      "    return .MirroredStrategy(['cpu:0', ])\n",
      "  else:\n",
      "    return .MirroredStrategy(['cpu:0'])\n",
      "\n",
      "\n",
      "TESTCASES  {\n",
      "    : ,\n",
      "    : \n",
      "}, \n",
      "    : ,\n",
      "    : \n",
      "})\n",
      "\n",
      "\n",
      "class (test.TestCase, .TestCase):\n",
      "  \n",
      "\n",
      "  parameterized.named_parameters(*TESTCASES)\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, ):\n",
      "    x  .constant([1.], =dtypes.float16)\n",
      "     ().scope():\n",
      "      with .policy_scope('infer_float32_vars'):\n",
      "        layer  (assert_type=dtypes.float16)\n",
      "          (x)\n",
      "        .assertEqual(layer.v.dtype, .float32)\n",
      "        .assertEqual(y.dtype, .float16)\n",
      "        .evaluate(variables.global_variables_initializer())\n",
      "        .assertEqual(self.evaluate(y), )\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, ):\n",
      "    x  .constant([1.], =dtypes.float16)\n",
      "     ().scope():\n",
      "      with .policy_scope('infer_float32_vars'):\n",
      "        layer  (assert_type=dtypes.float16)\n",
      "          (x)\n",
      "        .assertEqual(layer.v.dtype, .float32)\n",
      "        .assertEqual(y.dtype, .float16)\n",
      "        .evaluate(variables.global_variables_initializer())\n",
      "        .assertEqual(self.evaluate(y), )\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, ):\n",
      "    x  .constant([1.], =dtypes.float16)\n",
      "     ().scope():\n",
      "      with .policy_scope('infer_float32_vars'):\n",
      "        \n",
      "        layer  (assert_type=dtypes.float16,\n",
      "                         =IdentityRegularizer())\n",
      "        (x)\n",
      "        regularizer_loss,)  .losses\n",
      "        .assertEqual(regularizer_loss.dtype, .float32)\n",
      "        .evaluate(variables.global_variables_initializer())\n",
      "        .assertEqual(self.evaluate(regularizer_loss), )\n",
      "\n",
      "        \n",
      "          (assert_type=dtypes.float16,\n",
      "                                        =IdentityRegularizer())\n",
      "        (x)\n",
      "        regularizer_loss,)  .losses\n",
      "        .assertEqual(regularizer_loss.dtype, .float32)\n",
      "        .evaluate(variables.global_variables_initializer())\n",
      "        .assertEqual(self.evaluate(regularizer_loss), )\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, ):\n",
      "    x  .constant([1.], =dtypes.float16)\n",
      "     ().scope():\n",
      "      \n",
      "      layer  (assert_type=dtypes.float16,\n",
      "                       =policy.Policy('infer_float32_vars'))\n",
      "      \n",
      "      .assertEqual(layer.dtype, .float32)\n",
      "      (x)\n",
      "      .assertEqual(layer.v.dtype, .float32)\n",
      "       .policy_scope('infer_float32_vars'):\n",
      "        \n",
      "        layer  (assert_type=dtypes.float16,\n",
      "                         =policy.Policy('infer'))\n",
      "        \n",
      "        .assertEqual(layer.dtype, )\n",
      "        (x)\n",
      "        .assertEqual(layer.v.dtype, .float16)\n",
      "        .assertEqual(layer.dtype, .float16)\n",
      "\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, ):\n",
      "    x  .constant([1.], =dtypes.float16)\n",
      "     ().scope()  :\n",
      "      with .policy_scope('infer_float32_vars'):\n",
      "        layer  (assert_type=dtypes.float16)\n",
      "         ():\n",
      "          with .GradientTape()  :\n",
      "            y  (x)\n",
      "            \n",
      "            \n",
      "              .num_replicas_in_sync\n",
      "\n",
      "          \n",
      "          \n",
      "          \n",
      "          opt  .SGD(2  14)\n",
      "            .gradient(y, .v)\n",
      "           .apply_gradients([(grad, .v)])\n",
      "\n",
      "        op  .experimental_run(run_fn)\n",
      "          .executing_eagerly():\n",
      "          self.evaluate(variables.global_variables_initializer())\n",
      "          .evaluate(op)\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        self.assertEqual(self.evaluate(layer.v),     14)\n",
      "\n",
      "\n",
      "class (test.TestCase, .TestCase):\n",
      "  \n",
      "\n",
      "  parameterized.named_parameters({\n",
      "      : ,\n",
      "      : ,\n",
      "  , \n",
      "      : ,\n",
      "      : ,\n",
      "  , \n",
      "      : ,\n",
      "      : ,\n",
      "      : \n",
      "  , \n",
      "      : ,\n",
      "      : ,\n",
      "      : \n",
      "  )\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, , =False, =False):\n",
      "    regularizer  ()    \n",
      "     ().scope():\n",
      "      with .policy_scope('infer_float32_vars'):\n",
      "        x  .Input(shape=(), =2, =dtypes.float16)\n",
      "          (assert_type=dtypes.float16, =use_operator,\n",
      "                         =regularizer)\n",
      "          (x)\n",
      "          .cast(y, .float32)\n",
      "          .Model(inputs=x, =y)\n",
      "\n",
      "         (y_true, ):\n",
      "          del \n",
      "           .reduce_mean(y_pred)\n",
      "\n",
      "        \n",
      "        \n",
      "        \n",
      "        opt  .SGD(2  14)\n",
      "        .compile(opt, =loss_fn)\n",
      "\n",
      "      self.assertEqual(backend.eval(layer.v), )\n",
      "        .ones((2, ))\n",
      "        .ones((2, ))\n",
      "        .Dataset.from_tensor_slices((x, )).batch(2)\n",
      "      .fit(dataset)\n",
      "      \n",
      "      \n",
      "            14\n",
      "       :\n",
      "        \n",
      "        expected    14\n",
      "      self.assertEqual(backend.eval(layer.v), )\n",
      "\n",
      "  @parameterized.named_parameters({\n",
      "      : ,\n",
      "      : ,\n",
      "  , \n",
      "      : ,\n",
      "      : ,\n",
      "  , \n",
      "      : ,\n",
      "      : ,\n",
      "      : \n",
      "  )\n",
      "  test_util.run_in_graph_and_eager_modes\n",
      "   (self, , =False):\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    strategy  ()\n",
      "     :\n",
      "      loss_scale  \n",
      "    learning_rate    14\n",
      "\n",
      "     .scope():\n",
      "      with .policy_scope(policy.Policy('infer_float32_vars')):\n",
      "        x  .Input(shape=(), =2, =dtypes.float16)\n",
      "          (assert_type=dtypes.float16,\n",
      "                          =IdentityRegularizer(), =True)\n",
      "          (assert_type=dtypes.float16,\n",
      "                                         =True)\n",
      "          (assert_type=dtypes.float16, =False)\n",
      "          (assert_type=dtypes.float16,\n",
      "                                         =IdentityRegularizer(),\n",
      "                                         =False)\n",
      "          (x)\n",
      "          (y)\n",
      "          (y)\n",
      "          (y)\n",
      "         :\n",
      "          \n",
      "          \n",
      "          \n",
      "          \n",
      "          expected_gradient    .num_replicas_in_sync  \n",
      "            \n",
      "              .create_identity_with_grad_check_fn(\n",
      "                  =dtypes.float16,\n",
      "                  =[expected_gradient]  ))\n",
      "            .Lambda(identity_with_grad_check_fn)(y)\n",
      "        y  .cast(y, .float32)\n",
      "          .Model(inputs=x, =y)\n",
      "\n",
      "         (y_true, ):\n",
      "          self.assertEqual(y_true.dtype, .float32)\n",
      "          .assertEqual(y_pred.dtype, .float32)\n",
      "           .reduce_mean(y_pred)\n",
      "\n",
      "        opt  .SGD(learning_rate)\n",
      "         :\n",
      "          opt  .LossScaleOptimizer(opt, )\n",
      "        model.compile(opt, =loss_fn)\n",
      "\n",
      "      x  .ones((2, ))\n",
      "        .ones((2, ))\n",
      "        .Dataset.from_tensor_slices((x, )).batch(2)\n",
      "      .fit(dataset)\n",
      "         layer1, , , ):\n",
      "        if .losses:\n",
      "          \n",
      "          self.assertEqual(backend.eval(layer.v),     )\n",
      "        else:\n",
      "          \n",
      "          self.assertEqual(backend.eval(layer.v),   )\n",
      "\n",
      "\n",
      "if   :\n",
      "  test.main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(remove_comments_and_docstrings(origin_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Copyright 2019 The TensorFlow Authors. All Rights Reserved.',\n",
       " '#',\n",
       " '# Licensed under the Apache License, Version 2.0 (the \"License\");',\n",
       " '# you may not use this file except in compliance with the License.',\n",
       " '# You may obtain a copy of the License at',\n",
       " '#',\n",
       " '#     http://www.apache.org/licenses/LICENSE-2.0',\n",
       " '#',\n",
       " '# Unless required by applicable law or agreed to in writing, software',\n",
       " '# distributed under the License is distributed on an \"AS IS\" BASIS,',\n",
       " '# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.',\n",
       " '# See the License for the specific language governing permissions and',\n",
       " '# limitations under the License.',\n",
       " '# ==============================================================================',\n",
       " '\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"',\n",
       " 'from __future__ import absolute_import',\n",
       " 'from __future__ import division',\n",
       " 'from __future__ import print_function',\n",
       " '',\n",
       " 'from absl.testing import parameterized',\n",
       " 'import numpy as np',\n",
       " '',\n",
       " 'from tensorflow.python.data.ops import dataset_ops',\n",
       " 'from tensorflow.python.distribute import mirrored_strategy',\n",
       " 'from tensorflow.python.distribute import one_device_strategy',\n",
       " 'from tensorflow.python.eager import backprop',\n",
       " 'from tensorflow.python.eager import context',\n",
       " 'from tensorflow.python.framework import constant_op',\n",
       " 'from tensorflow.python.framework import dtypes',\n",
       " 'from tensorflow.python.framework import test_util',\n",
       " 'from tensorflow.python.keras import backend',\n",
       " 'from tensorflow.python.keras import layers',\n",
       " 'from tensorflow.python.keras import models',\n",
       " 'from tensorflow.python.keras import regularizers',\n",
       " 'from tensorflow.python.keras.engine import base_layer',\n",
       " 'from tensorflow.python.keras.layers import core',\n",
       " 'from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer',\n",
       " 'from tensorflow.python.keras.mixed_precision.experimental import policy',\n",
       " 'from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util',\n",
       " 'from tensorflow.python.keras.optimizer_v2 import gradient_descent',\n",
       " 'from tensorflow.python.ops import array_ops',\n",
       " 'from tensorflow.python.ops import math_ops',\n",
       " 'from tensorflow.python.ops import variables',\n",
       " 'from tensorflow.python.platform import test',\n",
       " 'from tensorflow.python.util import nest',\n",
       " '',\n",
       " '',\n",
       " 'class AssertTypeLayer(base_layer.Layer):',\n",
       " '  \"\"\"A layer which asserts it\\'s inputs are a certain type.\"\"\"',\n",
       " '',\n",
       " '  def __init__(self, assert_type=None, **kwargs):',\n",
       " '    self._assert_type = assert_type',\n",
       " '    super(AssertTypeLayer, self).__init__(**kwargs)',\n",
       " '',\n",
       " '  def assert_input_types(self, inputs):',\n",
       " '    \"\"\"Asserts `inputs` are of the correct type. Should be called in call().\"\"\"',\n",
       " '    if self._assert_type:',\n",
       " '      inputs_flattened = nest.flatten(inputs)',\n",
       " '      for inp in inputs_flattened:',\n",
       " '        assert inp.dtype.base_dtype == self._assert_type, (',\n",
       " \"            'Input tensor has type %s which does not match assert type %s' %\",\n",
       " '            (inp.dtype.name, self._assert_type.name))',\n",
       " '',\n",
       " '',\n",
       " 'class AddLayer(AssertTypeLayer):',\n",
       " '  \"\"\"A layer which adds it\\'s input to a scalar variable.\"\"\"',\n",
       " '',\n",
       " '  def __init__(self, regularizer=None, use_operator=False, **kwargs):',\n",
       " '    \"\"\"Initializes the AddLayer.',\n",
       " '',\n",
       " '    Args:',\n",
       " '      regularizer: The regularizer on the scalar variable.',\n",
       " '      use_operator: If True, add using the + operator. If False, add using',\n",
       " '        tf.add.',\n",
       " '      **kwargs: Passed to AssertTypeLayer constructor.',\n",
       " '    \"\"\"',\n",
       " '    self._regularizer = regularizer',\n",
       " '    self._use_operator = use_operator',\n",
       " '    super(AddLayer, self).__init__(**kwargs)',\n",
       " '',\n",
       " '  def build(self, _):',\n",
       " \"    self.v = self.add_weight('v', (), initializer='ones',\",\n",
       " '                             regularizer=self._regularizer)',\n",
       " '    self.built = True',\n",
       " '',\n",
       " '  def call(self, inputs):',\n",
       " '    self.assert_input_types(inputs)',\n",
       " '    assert inputs.dtype == self.v.dtype',\n",
       " '    return self._add(inputs, self.v)',\n",
       " '',\n",
       " '  def _add(self, x, y):',\n",
       " '    if self._use_operator:',\n",
       " '      return x + y',\n",
       " '    else:',\n",
       " '      return math_ops.add(x, y)',\n",
       " '',\n",
       " '',\n",
       " 'class AddLayerWithoutAutoCast(AddLayer):',\n",
       " '  \"\"\"Same as AddLayer, but does not use AutoCastVariables.\"\"\"',\n",
       " '',\n",
       " '  def build(self, _):',\n",
       " '    dtype = self.dtype',\n",
       " \"    if dtype in ('float16', 'bfloat16'):\",\n",
       " \"      dtype = 'float32'\",\n",
       " \"    self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\",\n",
       " '                             experimental_autocast=False,',\n",
       " '                             regularizer=self._regularizer)',\n",
       " '    self.built = True',\n",
       " '',\n",
       " '  def call(self, inputs):',\n",
       " '    self.assert_input_types(inputs)',\n",
       " '    assert self.v.dtype in (dtypes.float32, dtypes.float64)',\n",
       " '    return self._add(inputs, math_ops.cast(self.v, inputs.dtype))',\n",
       " '',\n",
       " '',\n",
       " 'class IdentityRegularizer(regularizers.Regularizer):',\n",
       " '',\n",
       " '  def __call__(self, x):',\n",
       " '    assert x.dtype == dtypes.float32',\n",
       " '    return array_ops.identity(x)',\n",
       " '',\n",
       " '',\n",
       " 'def create_one_device_strategy():',\n",
       " \"  return one_device_strategy.OneDeviceStrategy('cpu:0')\",\n",
       " '',\n",
       " '',\n",
       " 'def create_mirrored_strategy():',\n",
       " '  if context.num_gpus() >= 1:',\n",
       " \"    return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\",\n",
       " '  else:',\n",
       " \"    return mirrored_strategy.MirroredStrategy(['cpu:0'])\",\n",
       " '',\n",
       " '',\n",
       " 'TESTCASES = ({',\n",
       " \"    'testcase_name': 'base',\",\n",
       " \"    'strategy_fn': create_one_device_strategy\",\n",
       " '}, {',\n",
       " \"    'testcase_name': 'distribute',\",\n",
       " \"    'strategy_fn': create_mirrored_strategy\",\n",
       " '})',\n",
       " '',\n",
       " '',\n",
       " 'class KerasLayerTest(test.TestCase, parameterized.TestCase):',\n",
       " '  \"\"\"Test mixed precision with Keras layers.\"\"\"',\n",
       " '',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_variables_in_float32(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '        y = layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '        self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(y), 2.)',\n",
       " '',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_layer_with_non_autocast_variable(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)',\n",
       " '        y = layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '        self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(y), 2.)',\n",
       " '',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_layer_regularizer_runs_in_float32(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        # Test on AddLayer',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " '                         regularizer=IdentityRegularizer())',\n",
       " '        layer(x)',\n",
       " '        (regularizer_loss,) = layer.losses',\n",
       " '        self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '',\n",
       " '        # Test on AddLayerWithoutAutoCast',\n",
       " '        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                        regularizer=IdentityRegularizer())',\n",
       " '        layer(x)',\n",
       " '        (regularizer_loss,) = layer.losses',\n",
       " '        self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_passing_policy_to_layer(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      # Passing a Policy to 'dtype' sets the policy for that layer.\",\n",
       " '      layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                       dtype=policy.Policy('infer_float32_vars'))\",\n",
       " '      # layer.dtype refers to the variable dtype',\n",
       " '      self.assertEqual(layer.dtype, dtypes.float32)',\n",
       " '      layer(x)',\n",
       " '      self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        # Passing a Policy to dtype overrides the global Policy',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                         dtype=policy.Policy('infer'))\",\n",
       " '        # layer dtype is not yet known',\n",
       " '        self.assertEqual(layer.dtype, None)',\n",
       " '        layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float16)',\n",
       " '        self.assertEqual(layer.dtype, dtypes.float16)',\n",
       " '',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_gradient(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope() as strategy:',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '        def run_fn():',\n",
       " '          with backprop.GradientTape() as tape:',\n",
       " '            y = layer(x)',\n",
       " '            # Divide by num_replicas_in_sync, as the effective total loss is the',\n",
       " \"            # sum of each of the replica's losses.\",\n",
       " '            y /= strategy.num_replicas_in_sync',\n",
       " '',\n",
       " '          # Learning rate is small enough that if applied to a float16 variable,',\n",
       " '          # the variable will not change. So this tests the learning rate is not',\n",
       " '          # applied to a float16 value, but instead the float32 variable.',\n",
       " '          opt = gradient_descent.SGD(2 ** -14)',\n",
       " '          grad = tape.gradient(y, layer.v)',\n",
       " '          return opt.apply_gradients([(grad, layer.v)])',\n",
       " '',\n",
       " '        op = strategy.experimental_run(run_fn)',\n",
       " '        if not context.executing_eagerly():',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.evaluate(op)',\n",
       " '        # The gradient with respective to the variable is 1. Since the',\n",
       " '        # variable is initialized with 1 and the learning rate is 2**-14, the',\n",
       " '        # new variable value should be: init_val - gradient * learning_rate,',\n",
       " '        # which is  1 - 1 * 2**-14',\n",
       " '        self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)',\n",
       " '',\n",
       " '',\n",
       " 'class KerasModelTest(test.TestCase, parameterized.TestCase):',\n",
       " '  \"\"\"Test mixed precision with Keras models.\"\"\"',\n",
       " '',\n",
       " '  @parameterized.named_parameters({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'operator',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"      'use_operator': True\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'regularizer',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"      'use_regularizer': True\",\n",
       " '  })',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):',\n",
       " '    regularizer = IdentityRegularizer() if use_regularizer else None',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,',\n",
       " '                         regularizer=regularizer)',\n",
       " '        y = layer(x)',\n",
       " '        y = math_ops.cast(y, dtypes.float32)',\n",
       " '        model = models.Model(inputs=x, outputs=y)',\n",
       " '',\n",
       " '        def loss_fn(y_true, y_pred):',\n",
       " '          del y_true',\n",
       " '          return math_ops.reduce_mean(y_pred)',\n",
       " '',\n",
       " '        # Learning rate is small enough that if applied to a float16 variable,',\n",
       " '        # the variable will not change. So this tests the learning rate not',\n",
       " '        # applied to a float16 value, but instead the float32 variable.',\n",
       " '        opt = gradient_descent.SGD(2 ** -14)',\n",
       " '        model.compile(opt, loss=loss_fn)',\n",
       " '',\n",
       " '      self.assertEqual(backend.eval(layer.v), 1)',\n",
       " '      x = np.ones((2, 1))',\n",
       " '      y = np.ones((2, 1))',\n",
       " '      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '      model.fit(dataset)',\n",
       " '      # Variable starts at 1, and should have gradient of 2 ** -14 subtracted',\n",
       " '      # from it.',\n",
       " '      expected = 1 - 2 ** -14',\n",
       " '      if use_regularizer:',\n",
       " '        # Regularizer adds another 2 ** -14 to the gradient.',\n",
       " '        expected -= 2 ** -14',\n",
       " '      self.assertEqual(backend.eval(layer.v), expected)',\n",
       " '',\n",
       " '  @parameterized.named_parameters({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'loss_scaling',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"      'use_loss_scaling': True\",\n",
       " '  })',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_advanced_model(self, strategy_fn, use_loss_scaling=False):',\n",
       " '',\n",
       " '    # The advanced model tests mixed-precision-related features that would occur',\n",
       " '    # in a resnet50 model. It tests a model that has:',\n",
       " '    #  * Multiple layers, some which use auto-cast variables and some which do',\n",
       " '    #    not',\n",
       " '    #  * Regularization on some variables and not others.',\n",
       " '    #  * Loss scaling (if use_loss_scaling is True)',\n",
       " '',\n",
       " '    strategy = strategy_fn()',\n",
       " '    if use_loss_scaling:',\n",
       " '      loss_scale = 8.',\n",
       " '    learning_rate = 2 ** -14',\n",
       " '',\n",
       " '    with strategy.scope():',\n",
       " \"      with policy.policy_scope(policy.Policy('infer_float32_vars')):\",\n",
       " '        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '        layer1 = AddLayer(assert_type=dtypes.float16,',\n",
       " '                          regularizer=IdentityRegularizer(), use_operator=True)',\n",
       " '        layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                         use_operator=True)',\n",
       " '        layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)',\n",
       " '        layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                         regularizer=IdentityRegularizer(),',\n",
       " '                                         use_operator=False)',\n",
       " '        y = layer1(x)',\n",
       " '        y = layer2(y)',\n",
       " '        y = layer3(y)',\n",
       " '        y = layer4(y)',\n",
       " '        if use_loss_scaling:',\n",
       " \"          # The gradient of 'y' at this point is 1. With loss scaling, the\",\n",
       " \"          # gradient is 'loss_scale'. The DistributionStrategy additionally\",\n",
       " '          # scales the gradient by 1/num_replicas in_sync. We divide by the',\n",
       " '          # batch size of 2 since the loss is averaged across batch elements.',\n",
       " '          expected_gradient = loss_scale / strategy.num_replicas_in_sync / 2',\n",
       " '          identity_with_grad_check_fn = (',\n",
       " '              mp_test_util.create_identity_with_grad_check_fn(',\n",
       " '                  expected_dtype=dtypes.float16,',\n",
       " '                  expected_gradient=[expected_gradient] * 2))',\n",
       " '          y = core.Lambda(identity_with_grad_check_fn)(y)',\n",
       " '        y = math_ops.cast(y, dtypes.float32)',\n",
       " '        model = models.Model(inputs=x, outputs=y)',\n",
       " '',\n",
       " '        def loss_fn(y_true, y_pred):',\n",
       " '          self.assertEqual(y_true.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y_pred.dtype, dtypes.float32)',\n",
       " '          return math_ops.reduce_mean(y_pred)',\n",
       " '',\n",
       " '        opt = gradient_descent.SGD(learning_rate)',\n",
       " '        if use_loss_scaling:',\n",
       " '          opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)',\n",
       " '        model.compile(opt, loss=loss_fn)',\n",
       " '',\n",
       " '      x = np.ones((2, 1))',\n",
       " '      y = np.ones((2, 1))',\n",
       " '      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '      model.fit(dataset)',\n",
       " '      for layer in (layer1, layer2, layer3, layer4):',\n",
       " '        if layer.losses:',\n",
       " '          # Layer has weight regularizer',\n",
       " '          self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)',\n",
       " '        else:',\n",
       " '          # Layer does not have weight regularizer',\n",
       " '          self.assertEqual(backend.eval(layer.v), 1 - learning_rate)',\n",
       " '',\n",
       " '',\n",
       " \"if __name__ == '__main__':\",\n",
       " '  test.main()',\n",
       " '']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_file.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import token\n",
    "\n",
    "def delete_comment_and_docstrings(source):\n",
    "#     source = open(fname)\n",
    "#     mod = open(fname + \",strip\", \"w\")\n",
    "    modified = \"\"\n",
    "\n",
    "    prev_toktype = token.INDENT\n",
    "    first_line = None\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "\n",
    "#     tokgen = tokenize.generate_tokens(source.readline)\n",
    "    tokens_iterator = tokenize.tokenize(BytesIO(source.encode('utf-8')).readline)\n",
    "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokens_iterator:\n",
    "        if 0:   # Change to if 1 to see the tokens fly by.\n",
    "            print(\"%10s %-14s %-20r %r\" % (\n",
    "                tokenize.tok_name.get(toktype, toktype),\n",
    "                \"%d.%d-%d.%d\" % (slineno, scol, elineno, ecol),\n",
    "                ttext, ltext\n",
    "                ))\n",
    "        if slineno > last_lineno:\n",
    "            last_col = 0\n",
    "        if scol > last_col:\n",
    "#             mod.write(\" \" * (scol - last_col))\n",
    "            modified = modified + \" \" * (scol - last_col)\n",
    "        if toktype == token.STRING and (prev_toktype == token.INDENT or prev_toktype == token.NEWLINE):\n",
    "            # Docstring\n",
    "#             mod.write(\"#--\")\n",
    "#             modified = modified + \"#--\"\n",
    "            pass\n",
    "        elif toktype == tokenize.COMMENT:\n",
    "            # Comment\n",
    "#             mod.write(\"##\\n\")\n",
    "#             modified = modified + \"##\\n\"\n",
    "            pass\n",
    "        else:\n",
    "#             mod.write(ttext)\n",
    "            modified = modified + ttext\n",
    "        prev_toktype = toktype\n",
    "        last_col = ecol\n",
    "        last_lineno = elineno\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "striped_modified_file = delete_comment_and_docstrings(origin_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "striped_origin_file = delete_comment_and_docstrings(parent_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比较删除docstring和comments后的origin_file和parent_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  utf-8',\n",
       " '  \"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"',\n",
       " '  from __future__ import absolute_import',\n",
       " '  from __future__ import division',\n",
       " '  from __future__ import print_function',\n",
       " '  from absl.testing import parameterized',\n",
       " '  import numpy as np',\n",
       " '  from tensorflow.python.data.ops import dataset_ops',\n",
       " '  from tensorflow.python.distribute import mirrored_strategy',\n",
       " '  from tensorflow.python.distribute import one_device_strategy',\n",
       " '  from tensorflow.python.eager import backprop',\n",
       " '  from tensorflow.python.eager import context',\n",
       " '  from tensorflow.python.framework import constant_op',\n",
       " '  from tensorflow.python.framework import dtypes',\n",
       " '  from tensorflow.python.framework import test_util',\n",
       " '  from tensorflow.python.keras import backend',\n",
       " '  from tensorflow.python.keras import layers',\n",
       " '  from tensorflow.python.keras import models',\n",
       " '  from tensorflow.python.keras import regularizers',\n",
       " '  from tensorflow.python.keras.engine import base_layer',\n",
       " '  from tensorflow.python.keras.mixed_precision.experimental import policy',\n",
       " '  from tensorflow.python.keras.optimizer_v2 import gradient_descent',\n",
       " '  from tensorflow.python.ops import array_ops',\n",
       " '  from tensorflow.python.ops import math_ops',\n",
       " '  from tensorflow.python.ops import variables',\n",
       " '  from tensorflow.python.platform import test',\n",
       " '  from tensorflow.python.util import nest',\n",
       " '  class AssertTypeLayer(base_layer.Layer):',\n",
       " '    ',\n",
       " '    def __init__(self, assert_type=None, **kwargs):',\n",
       " '      self._assert_type = assert_type',\n",
       " '      super(AssertTypeLayer, self).__init__(**kwargs)',\n",
       " '    def assert_input_types(self, inputs):',\n",
       " '      ',\n",
       " '      if self._assert_type:',\n",
       " '        inputs_flattened = nest.flatten(inputs)',\n",
       " '        for inp in inputs_flattened:',\n",
       " '          assert inp.dtype.base_dtype == self._assert_type, (',\n",
       " \"              'Input tensor has type %s which does not match assert type %s' %\",\n",
       " '              (inp.dtype.name, self._assert_type.name))',\n",
       " '  class AddLayer(AssertTypeLayer):',\n",
       " '    ',\n",
       " '    def __init__(self, regularizer=None, use_operator=False, **kwargs):',\n",
       " '      ',\n",
       " '      self._regularizer = regularizer',\n",
       " '      self._use_operator = use_operator',\n",
       " '      super(AddLayer, self).__init__(**kwargs)',\n",
       " '    def build(self, _):',\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones',\",\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert inputs.dtype == self.v.dtype',\n",
       " '      return self._add(inputs, self.v)',\n",
       " '    def _add(self, x, y):',\n",
       " '      if self._use_operator:',\n",
       " '        return x + y',\n",
       " '      else:',\n",
       " '        return math_ops.add(x, y)',\n",
       " '  class AddLayerWithoutAutoCast(AddLayer):',\n",
       " '    ',\n",
       " '    def build(self, _):',\n",
       " '      dtype = self.dtype',\n",
       " \"      if dtype in ('float16', 'bfloat16'):\",\n",
       " \"        dtype = 'float32'\",\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\",\n",
       " '                               experimental_autocast=False,',\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert self.v.dtype in (dtypes.float32, dtypes.float64)',\n",
       " '      return self._add(inputs, math_ops.cast(self.v, inputs.dtype))',\n",
       " '  class IdentityRegularizer(regularizers.Regularizer):',\n",
       " '    def __call__(self, x):',\n",
       " '      assert x.dtype == dtypes.float32',\n",
       " '      return array_ops.identity(x)',\n",
       " '  def create_one_device_strategy():',\n",
       " \"    return one_device_strategy.OneDeviceStrategy('cpu:0')\",\n",
       " '  def create_mirrored_strategy():',\n",
       " '    if context.num_gpus() >= 1:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\",\n",
       " '    else:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0'])\",\n",
       " '  TESTCASES = ({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy\",\n",
       " '  })',\n",
       " '  class KerasLayerTest(test.TestCase, parameterized.TestCase):',\n",
       " '    ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_variables_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_with_non_autocast_variable(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_regularizer_runs_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          ',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " '                           regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '          ',\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                          regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_passing_policy_to_layer(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " '        ',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                         dtype=policy.Policy('infer_float32_vars'))\",\n",
       " '        ',\n",
       " '        self.assertEqual(layer.dtype, dtypes.float32)',\n",
       " '        layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          ',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                           dtype=policy.Policy('infer'))\",\n",
       " '          ',\n",
       " '          self.assertEqual(layer.dtype, None)',\n",
       " '          layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float16)',\n",
       " '          self.assertEqual(layer.dtype, dtypes.float16)',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_gradient(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope() as strategy:',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          def run_fn():',\n",
       " '            with backprop.GradientTape() as tape:',\n",
       " '              y = layer(x)',\n",
       " '              ',\n",
       " '              ',\n",
       " '              y /= strategy.num_replicas_in_sync',\n",
       " '            ',\n",
       " '            ',\n",
       " '            ',\n",
       " '            opt = gradient_descent.SGD(2 ** -14)',\n",
       " '            grad = tape.gradient(y, layer.v)',\n",
       " '            return opt.apply_gradients([(grad, layer.v)])',\n",
       " '          op = strategy.experimental_run(run_fn)',\n",
       " '          if not context.executing_eagerly():',\n",
       " '            self.evaluate(variables.global_variables_initializer())',\n",
       " '            self.evaluate(op)',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)',\n",
       " '  class KerasModelTest(test.TestCase, parameterized.TestCase):',\n",
       " '    ',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'operator',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_operator': True\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'regularizer',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_regularizer': True\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):',\n",
       " '      regularizer = IdentityRegularizer() if use_regularizer else None',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,',\n",
       " '                           regularizer=regularizer)',\n",
       " '          y = layer(x)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            del y_true',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          opt = gradient_descent.SGD(2 ** -14)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '        self.assertEqual(backend.eval(layer.v), 1)',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        ',\n",
       " '        ',\n",
       " '        expected = 1 - 2 ** -14',\n",
       " '        if use_regularizer:',\n",
       " '          ',\n",
       " '          expected -= 2 ** -14',\n",
       " '        self.assertEqual(backend.eval(layer.v), expected)',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '#   def test_advanced_model(self, strategy_fn):',\n",
       " '?                                            ++++++++++++++++++++++++\\n',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      strategy = strategy_fn()',\n",
       " '      learning_rate = 2 ** -14',\n",
       " '      with strategy.scope():',\n",
       " \"        with policy.policy_scope(policy.Policy('infer_float32_vars')):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer1 = AddLayer(assert_type=dtypes.float16,',\n",
       " '                            regularizer=IdentityRegularizer(), use_operator=True)',\n",
       " '          layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           use_operator=True)',\n",
       " '          layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)',\n",
       " '          layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           regularizer=IdentityRegularizer(),',\n",
       " '                                           use_operator=False)',\n",
       " '          y = layer1(x)',\n",
       " '          y = layer2(y)',\n",
       " '          y = layer3(y)',\n",
       " '          y = layer4(y)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            self.assertEqual(y_true.dtype, dtypes.float32)',\n",
       " '            self.assertEqual(y_pred.dtype, dtypes.float32)',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '          opt = gradient_descent.SGD(learning_rate)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        for layer in (layer1, layer2, layer3, layer4):',\n",
       " '          if layer.losses:',\n",
       " '            ',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)',\n",
       " '          else:',\n",
       " '            ',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - learning_rate)',\n",
       " \"  if __name__ == '__main__':\",\n",
       " '    test.main()']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_diff_file(striped_modified_file, striped_origin_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "striped_modified_lines = striped_modified_file.splitlines()\n",
    "striped_origin_lines = striped_origin_file.splitlines()\n",
    "d = difflib.Differ()\n",
    "diff = d.compare(striped_modified_lines, striped_origin_lines)\n",
    "# print('\\n'.join(list(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = list(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_temp = []\n",
    "for line in temp:\n",
    "    if line.startswith('-'):\n",
    "        temp_temp.append(line.replace('-', '#', 1))\n",
    "    else:\n",
    "        temp_temp.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  utf-8',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  ',\n",
       " '  \"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"',\n",
       " '  from __future__ import absolute_import',\n",
       " '  from __future__ import division',\n",
       " '  from __future__ import print_function',\n",
       " '  ',\n",
       " '  from absl.testing import parameterized',\n",
       " '  import numpy as np',\n",
       " '  ',\n",
       " '  from tensorflow.python.data.ops import dataset_ops',\n",
       " '  from tensorflow.python.distribute import mirrored_strategy',\n",
       " '  from tensorflow.python.distribute import one_device_strategy',\n",
       " '  from tensorflow.python.eager import backprop',\n",
       " '  from tensorflow.python.eager import context',\n",
       " '  from tensorflow.python.framework import constant_op',\n",
       " '  from tensorflow.python.framework import dtypes',\n",
       " '  from tensorflow.python.framework import test_util',\n",
       " '  from tensorflow.python.keras import backend',\n",
       " '  from tensorflow.python.keras import layers',\n",
       " '  from tensorflow.python.keras import models',\n",
       " '  from tensorflow.python.keras import regularizers',\n",
       " '  from tensorflow.python.keras.engine import base_layer',\n",
       " '# from tensorflow.python.keras.layers import core',\n",
       " '# from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer',\n",
       " '  from tensorflow.python.keras.mixed_precision.experimental import policy',\n",
       " '# from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util',\n",
       " '  from tensorflow.python.keras.optimizer_v2 import gradient_descent',\n",
       " '  from tensorflow.python.ops import array_ops',\n",
       " '  from tensorflow.python.ops import math_ops',\n",
       " '  from tensorflow.python.ops import variables',\n",
       " '  from tensorflow.python.platform import test',\n",
       " '  from tensorflow.python.util import nest',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class AssertTypeLayer(base_layer.Layer):',\n",
       " '    ',\n",
       " '  ',\n",
       " '    def __init__(self, assert_type=None, **kwargs):',\n",
       " '      self._assert_type = assert_type',\n",
       " '      super(AssertTypeLayer, self).__init__(**kwargs)',\n",
       " '  ',\n",
       " '    def assert_input_types(self, inputs):',\n",
       " '      ',\n",
       " '      if self._assert_type:',\n",
       " '        inputs_flattened = nest.flatten(inputs)',\n",
       " '        for inp in inputs_flattened:',\n",
       " '          assert inp.dtype.base_dtype == self._assert_type, (',\n",
       " \"              'Input tensor has type %s which does not match assert type %s' %\",\n",
       " '              (inp.dtype.name, self._assert_type.name))',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class AddLayer(AssertTypeLayer):',\n",
       " '    ',\n",
       " '  ',\n",
       " '    def __init__(self, regularizer=None, use_operator=False, **kwargs):',\n",
       " '      ',\n",
       " '      self._regularizer = regularizer',\n",
       " '      self._use_operator = use_operator',\n",
       " '      super(AddLayer, self).__init__(**kwargs)',\n",
       " '  ',\n",
       " '    def build(self, _):',\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones',\",\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '  ',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert inputs.dtype == self.v.dtype',\n",
       " '      return self._add(inputs, self.v)',\n",
       " '  ',\n",
       " '    def _add(self, x, y):',\n",
       " '      if self._use_operator:',\n",
       " '        return x + y',\n",
       " '      else:',\n",
       " '        return math_ops.add(x, y)',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class AddLayerWithoutAutoCast(AddLayer):',\n",
       " '    ',\n",
       " '  ',\n",
       " '    def build(self, _):',\n",
       " '      dtype = self.dtype',\n",
       " \"      if dtype in ('float16', 'bfloat16'):\",\n",
       " \"        dtype = 'float32'\",\n",
       " \"      self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\",\n",
       " '                               experimental_autocast=False,',\n",
       " '                               regularizer=self._regularizer)',\n",
       " '      self.built = True',\n",
       " '  ',\n",
       " '    def call(self, inputs):',\n",
       " '      self.assert_input_types(inputs)',\n",
       " '      assert self.v.dtype in (dtypes.float32, dtypes.float64)',\n",
       " '      return self._add(inputs, math_ops.cast(self.v, inputs.dtype))',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class IdentityRegularizer(regularizers.Regularizer):',\n",
       " '  ',\n",
       " '    def __call__(self, x):',\n",
       " '      assert x.dtype == dtypes.float32',\n",
       " '      return array_ops.identity(x)',\n",
       " '  ',\n",
       " '  ',\n",
       " '  def create_one_device_strategy():',\n",
       " \"    return one_device_strategy.OneDeviceStrategy('cpu:0')\",\n",
       " '  ',\n",
       " '  ',\n",
       " '  def create_mirrored_strategy():',\n",
       " '    if context.num_gpus() >= 1:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\",\n",
       " '    else:',\n",
       " \"      return mirrored_strategy.MirroredStrategy(['cpu:0'])\",\n",
       " '  ',\n",
       " '  ',\n",
       " '  TESTCASES = ({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy\",\n",
       " '  })',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class KerasLayerTest(test.TestCase, parameterized.TestCase):',\n",
       " '    ',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_variables_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_with_non_autocast_variable(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)',\n",
       " '          y = layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(y), 2.)',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_layer_regularizer_runs_in_float32(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          ',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " '                           regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '  ',\n",
       " '          ',\n",
       " '          layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                          regularizer=IdentityRegularizer())',\n",
       " '          layer(x)',\n",
       " '          (regularizer_loss,) = layer.losses',\n",
       " '          self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_passing_policy_to_layer(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope():',\n",
       " '        ',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                         dtype=policy.Policy('infer_float32_vars'))\",\n",
       " '        ',\n",
       " '        self.assertEqual(layer.dtype, dtypes.float32)',\n",
       " '        layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          ',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                           dtype=policy.Policy('infer'))\",\n",
       " '          ',\n",
       " '          self.assertEqual(layer.dtype, None)',\n",
       " '          layer(x)',\n",
       " '          self.assertEqual(layer.v.dtype, dtypes.float16)',\n",
       " '          self.assertEqual(layer.dtype, dtypes.float16)',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters(*TESTCASES)',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_gradient(self, strategy_fn):',\n",
       " '      x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '      with strategy_fn().scope() as strategy:',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '          def run_fn():',\n",
       " '            with backprop.GradientTape() as tape:',\n",
       " '              y = layer(x)',\n",
       " '              ',\n",
       " '              ',\n",
       " '              y /= strategy.num_replicas_in_sync',\n",
       " '  ',\n",
       " '            ',\n",
       " '            ',\n",
       " '            ',\n",
       " '            opt = gradient_descent.SGD(2 ** -14)',\n",
       " '            grad = tape.gradient(y, layer.v)',\n",
       " '            return opt.apply_gradients([(grad, layer.v)])',\n",
       " '  ',\n",
       " '          op = strategy.experimental_run(run_fn)',\n",
       " '          if not context.executing_eagerly():',\n",
       " '            self.evaluate(variables.global_variables_initializer())',\n",
       " '            self.evaluate(op)',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)',\n",
       " '  ',\n",
       " '  ',\n",
       " '  class KerasModelTest(test.TestCase, parameterized.TestCase):',\n",
       " '    ',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'operator',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_operator': True\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'regularizer',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " \"        'use_regularizer': True\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '    def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):',\n",
       " '      regularizer = IdentityRegularizer() if use_regularizer else None',\n",
       " '      with strategy_fn().scope():',\n",
       " \"        with policy.policy_scope('infer_float32_vars'):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,',\n",
       " '                           regularizer=regularizer)',\n",
       " '          y = layer(x)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '  ',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            del y_true',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '  ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          opt = gradient_descent.SGD(2 ** -14)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '  ',\n",
       " '        self.assertEqual(backend.eval(layer.v), 1)',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        ',\n",
       " '        ',\n",
       " '        expected = 1 - 2 ** -14',\n",
       " '        if use_regularizer:',\n",
       " '          ',\n",
       " '          expected -= 2 ** -14',\n",
       " '        self.assertEqual(backend.eval(layer.v), expected)',\n",
       " '  ',\n",
       " '    @parameterized.named_parameters({',\n",
       " \"        'testcase_name': 'base',\",\n",
       " \"        'strategy_fn': create_one_device_strategy,\",\n",
       " '    }, {',\n",
       " \"        'testcase_name': 'distribute',\",\n",
       " \"        'strategy_fn': create_mirrored_strategy,\",\n",
       " '#   }, {',\n",
       " \"#       'testcase_name': 'loss_scaling',\",\n",
       " \"#       'strategy_fn': create_mirrored_strategy,\",\n",
       " \"#       'use_loss_scaling': True\",\n",
       " '    })',\n",
       " '    @test_util.run_in_graph_and_eager_modes',\n",
       " '#   def test_advanced_model(self, strategy_fn, use_loss_scaling=False):',\n",
       " '?                                            ------------------------\\n',\n",
       " '+   def test_advanced_model(self, strategy_fn):',\n",
       " '  ',\n",
       " '#     ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '      ',\n",
       " '  ',\n",
       " '      strategy = strategy_fn()',\n",
       " '+ ',\n",
       " '#     if use_loss_scaling:',\n",
       " '#       loss_scale = 8.',\n",
       " '      learning_rate = 2 ** -14',\n",
       " '  ',\n",
       " '      with strategy.scope():',\n",
       " \"        with policy.policy_scope(policy.Policy('infer_float32_vars')):\",\n",
       " '          x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '          layer1 = AddLayer(assert_type=dtypes.float16,',\n",
       " '                            regularizer=IdentityRegularizer(), use_operator=True)',\n",
       " '          layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           use_operator=True)',\n",
       " '          layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)',\n",
       " '          layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                           regularizer=IdentityRegularizer(),',\n",
       " '                                           use_operator=False)',\n",
       " '          y = layer1(x)',\n",
       " '          y = layer2(y)',\n",
       " '          y = layer3(y)',\n",
       " '          y = layer4(y)',\n",
       " '#         if use_loss_scaling:',\n",
       " '#           ',\n",
       " '#           ',\n",
       " '#           ',\n",
       " '#           ',\n",
       " '#           expected_gradient = loss_scale / strategy.num_replicas_in_sync / 2',\n",
       " '#           identity_with_grad_check_fn = (',\n",
       " '#               mp_test_util.create_identity_with_grad_check_fn(',\n",
       " '#                   expected_dtype=dtypes.float16,',\n",
       " '#                   expected_gradient=[expected_gradient] * 2))',\n",
       " '#           y = core.Lambda(identity_with_grad_check_fn)(y)',\n",
       " '          y = math_ops.cast(y, dtypes.float32)',\n",
       " '          model = models.Model(inputs=x, outputs=y)',\n",
       " '  ',\n",
       " '          def loss_fn(y_true, y_pred):',\n",
       " '            self.assertEqual(y_true.dtype, dtypes.float32)',\n",
       " '            self.assertEqual(y_pred.dtype, dtypes.float32)',\n",
       " '            return math_ops.reduce_mean(y_pred)',\n",
       " '  ',\n",
       " '          opt = gradient_descent.SGD(learning_rate)',\n",
       " '#         if use_loss_scaling:',\n",
       " '#           opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)',\n",
       " '          model.compile(opt, loss=loss_fn)',\n",
       " '  ',\n",
       " '        x = np.ones((2, 1))',\n",
       " '        y = np.ones((2, 1))',\n",
       " '        dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '        model.fit(dataset)',\n",
       " '        for layer in (layer1, layer2, layer3, layer4):',\n",
       " '          if layer.losses:',\n",
       " '            ',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)',\n",
       " '          else:',\n",
       " '            ',\n",
       " '            self.assertEqual(backend.eval(layer.v), 1 - learning_rate)',\n",
       " '  ',\n",
       " '  ',\n",
       " \"  if __name__ == '__main__':\",\n",
       " '    test.main()']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_file(origin_file, modified_file):\n",
    "    origin_file = delete_comment_and_docstrings(origin_file)\n",
    "    modified_file = delete_comment_and_docstrings(modified_file)\n",
    "    \n",
    "    origin_lines = origin_file.splitlines()\n",
    "    modified_lines = modified_file.splitlines()\n",
    "    d = difflib.Differ()\n",
    "    diff = d.compare(origin_lines, modified_lines)\n",
    "    \n",
    "    minus_lines = []\n",
    "    for line in list(diff):\n",
    "        if len(line.lstrip(' ')) == 0 or len(line.lstrip('- ')) == 0 or line.startswith('+') or line.startswith('?'):\n",
    "            continue\n",
    "        if line.startswith('-'):\n",
    "            minus_lines.append('#' + line[2:])\n",
    "        else:\n",
    "            minus_lines.append(line[2:])\n",
    "    return minus_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"\n",
      "from __future__ import absolute_import\n",
      "from __future__ import division\n",
      "from __future__ import print_function\n",
      "from absl.testing import parameterized\n",
      "import numpy as np\n",
      "from tensorflow.python.data.ops import dataset_ops\n",
      "from tensorflow.python.distribute import mirrored_strategy\n",
      "from tensorflow.python.distribute import one_device_strategy\n",
      "from tensorflow.python.eager import backprop\n",
      "from tensorflow.python.eager import context\n",
      "from tensorflow.python.framework import constant_op\n",
      "from tensorflow.python.framework import dtypes\n",
      "from tensorflow.python.framework import test_util\n",
      "from tensorflow.python.keras import backend\n",
      "from tensorflow.python.keras import layers\n",
      "from tensorflow.python.keras import models\n",
      "from tensorflow.python.keras import regularizers\n",
      "from tensorflow.python.keras.engine import base_layer\n",
      "from tensorflow.python.keras.mixed_precision.experimental import policy\n",
      "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
      "from tensorflow.python.ops import array_ops\n",
      "from tensorflow.python.ops import math_ops\n",
      "from tensorflow.python.ops import variables\n",
      "from tensorflow.python.platform import test\n",
      "from tensorflow.python.util import nest\n",
      "class AssertTypeLayer(base_layer.Layer):\n",
      "  def __init__(self, assert_type=None, **kwargs):\n",
      "    self._assert_type = assert_type\n",
      "    super(AssertTypeLayer, self).__init__(**kwargs)\n",
      "  def assert_input_types(self, inputs):\n",
      "    if self._assert_type:\n",
      "      inputs_flattened = nest.flatten(inputs)\n",
      "      for inp in inputs_flattened:\n",
      "        assert inp.dtype.base_dtype == self._assert_type, (\n",
      "            'Input tensor has type %s which does not match assert type %s' %\n",
      "            (inp.dtype.name, self._assert_type.name))\n",
      "class AddLayer(AssertTypeLayer):\n",
      "  def __init__(self, regularizer=None, use_operator=False, **kwargs):\n",
      "    self._regularizer = regularizer\n",
      "    self._use_operator = use_operator\n",
      "    super(AddLayer, self).__init__(**kwargs)\n",
      "  def build(self, _):\n",
      "    self.v = self.add_weight('v', (), initializer='ones',\n",
      "                             regularizer=self._regularizer)\n",
      "    self.built = True\n",
      "  def call(self, inputs):\n",
      "    self.assert_input_types(inputs)\n",
      "    assert inputs.dtype == self.v.dtype\n",
      "    return self._add(inputs, self.v)\n",
      "  def _add(self, x, y):\n",
      "    if self._use_operator:\n",
      "      return x + y\n",
      "    else:\n",
      "      return math_ops.add(x, y)\n",
      "class AddLayerWithoutAutoCast(AddLayer):\n",
      "  def build(self, _):\n",
      "    dtype = self.dtype\n",
      "    if dtype in ('float16', 'bfloat16'):\n",
      "      dtype = 'float32'\n",
      "    self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\n",
      "                             experimental_autocast=False,\n",
      "                             regularizer=self._regularizer)\n",
      "    self.built = True\n",
      "  def call(self, inputs):\n",
      "    self.assert_input_types(inputs)\n",
      "    assert self.v.dtype in (dtypes.float32, dtypes.float64)\n",
      "    return self._add(inputs, math_ops.cast(self.v, inputs.dtype))\n",
      "class IdentityRegularizer(regularizers.Regularizer):\n",
      "  def __call__(self, x):\n",
      "    assert x.dtype == dtypes.float32\n",
      "    return array_ops.identity(x)\n",
      "def create_one_device_strategy():\n",
      "  return one_device_strategy.OneDeviceStrategy('cpu:0')\n",
      "def create_mirrored_strategy():\n",
      "  if context.num_gpus() >= 1:\n",
      "    return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\n",
      "  else:\n",
      "    return mirrored_strategy.MirroredStrategy(['cpu:0'])\n",
      "TESTCASES = ({\n",
      "    'testcase_name': 'base',\n",
      "    'strategy_fn': create_one_device_strategy\n",
      "}, {\n",
      "    'testcase_name': 'distribute',\n",
      "    'strategy_fn': create_mirrored_strategy\n",
      "})\n",
      "class KerasLayerTest(test.TestCase, parameterized.TestCase):\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_variables_in_float32(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16)\n",
      "        y = layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "        self.assertEqual(y.dtype, dtypes.float16)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(y), 2.)\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_layer_with_non_autocast_variable(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)\n",
      "        y = layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "        self.assertEqual(y.dtype, dtypes.float16)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(y), 2.)\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_layer_regularizer_runs_in_float32(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16,\n",
      "                         regularizer=IdentityRegularizer())\n",
      "        layer(x)\n",
      "        (regularizer_loss,) = layer.losses\n",
      "        self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n",
      "        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                        regularizer=IdentityRegularizer())\n",
      "        layer(x)\n",
      "        (regularizer_loss,) = layer.losses\n",
      "        self.assertEqual(regularizer_loss.dtype, dtypes.float32)\n",
      "        self.evaluate(variables.global_variables_initializer())\n",
      "        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_passing_policy_to_layer(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope():\n",
      "      layer = AddLayer(assert_type=dtypes.float16,\n",
      "                       dtype=policy.Policy('infer_float32_vars'))\n",
      "      self.assertEqual(layer.dtype, dtypes.float32)\n",
      "      layer(x)\n",
      "      self.assertEqual(layer.v.dtype, dtypes.float32)\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16,\n",
      "                         dtype=policy.Policy('infer'))\n",
      "        self.assertEqual(layer.dtype, None)\n",
      "        layer(x)\n",
      "        self.assertEqual(layer.v.dtype, dtypes.float16)\n",
      "        self.assertEqual(layer.dtype, dtypes.float16)\n",
      "  @parameterized.named_parameters(*TESTCASES)\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_gradient(self, strategy_fn):\n",
      "    x = constant_op.constant([1.], dtype=dtypes.float16)\n",
      "    with strategy_fn().scope() as strategy:\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        layer = AddLayer(assert_type=dtypes.float16)\n",
      "        def run_fn():\n",
      "          with backprop.GradientTape() as tape:\n",
      "            y = layer(x)\n",
      "            y /= strategy.num_replicas_in_sync\n",
      "          opt = gradient_descent.SGD(2 ** -14)\n",
      "          grad = tape.gradient(y, layer.v)\n",
      "          return opt.apply_gradients([(grad, layer.v)])\n",
      "        op = strategy.experimental_run(run_fn)\n",
      "        if not context.executing_eagerly():\n",
      "          self.evaluate(variables.global_variables_initializer())\n",
      "          self.evaluate(op)\n",
      "        self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)\n",
      "class KerasModelTest(test.TestCase, parameterized.TestCase):\n",
      "  @parameterized.named_parameters({\n",
      "      'testcase_name': 'base',\n",
      "      'strategy_fn': create_one_device_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'distribute',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'operator',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "      'use_operator': True\n",
      "  }, {\n",
      "      'testcase_name': 'regularizer',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "      'use_regularizer': True\n",
      "  })\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "  def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):\n",
      "    regularizer = IdentityRegularizer() if use_regularizer else None\n",
      "    with strategy_fn().scope():\n",
      "      with policy.policy_scope('infer_float32_vars'):\n",
      "        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "        layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,\n",
      "                         regularizer=regularizer)\n",
      "        y = layer(x)\n",
      "        y = math_ops.cast(y, dtypes.float32)\n",
      "        model = models.Model(inputs=x, outputs=y)\n",
      "        def loss_fn(y_true, y_pred):\n",
      "          del y_true\n",
      "          return math_ops.reduce_mean(y_pred)\n",
      "        opt = gradient_descent.SGD(2 ** -14)\n",
      "        model.compile(opt, loss=loss_fn)\n",
      "      self.assertEqual(backend.eval(layer.v), 1)\n",
      "      x = np.ones((2, 1))\n",
      "      y = np.ones((2, 1))\n",
      "      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "      model.fit(dataset)\n",
      "      expected = 1 - 2 ** -14\n",
      "      if use_regularizer:\n",
      "        expected -= 2 ** -14\n",
      "      self.assertEqual(backend.eval(layer.v), expected)\n",
      "  @parameterized.named_parameters({\n",
      "      'testcase_name': 'base',\n",
      "      'strategy_fn': create_one_device_strategy,\n",
      "  }, {\n",
      "      'testcase_name': 'distribute',\n",
      "      'strategy_fn': create_mirrored_strategy,\n",
      "  })\n",
      "  @test_util.run_in_graph_and_eager_modes\n",
      "#  def test_advanced_model(self, strategy_fn):\n",
      "    strategy = strategy_fn()\n",
      "    learning_rate = 2 ** -14\n",
      "    with strategy.scope():\n",
      "      with policy.policy_scope(policy.Policy('infer_float32_vars')):\n",
      "        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)\n",
      "        layer1 = AddLayer(assert_type=dtypes.float16,\n",
      "                          regularizer=IdentityRegularizer(), use_operator=True)\n",
      "        layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                         use_operator=True)\n",
      "        layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)\n",
      "        layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,\n",
      "                                         regularizer=IdentityRegularizer(),\n",
      "                                         use_operator=False)\n",
      "        y = layer1(x)\n",
      "        y = layer2(y)\n",
      "        y = layer3(y)\n",
      "        y = layer4(y)\n",
      "        y = math_ops.cast(y, dtypes.float32)\n",
      "        model = models.Model(inputs=x, outputs=y)\n",
      "        def loss_fn(y_true, y_pred):\n",
      "          self.assertEqual(y_true.dtype, dtypes.float32)\n",
      "          self.assertEqual(y_pred.dtype, dtypes.float32)\n",
      "          return math_ops.reduce_mean(y_pred)\n",
      "        opt = gradient_descent.SGD(learning_rate)\n",
      "        model.compile(opt, loss=loss_fn)\n",
      "      x = np.ones((2, 1))\n",
      "      y = np.ones((2, 1))\n",
      "      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)\n",
      "      model.fit(dataset)\n",
      "      for layer in (layer1, layer2, layer3, layer4):\n",
      "        if layer.losses:\n",
      "          self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)\n",
      "        else:\n",
      "          self.assertEqual(backend.eval(layer.v), 1 - learning_rate)\n",
      "if __name__ == '__main__':\n",
      "  test.main()\n"
     ]
    }
   ],
   "source": [
    "result = get_diff_file(parent_file, origin_file)\n",
    "print('\\n'.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utf-8',\n",
       " '\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"',\n",
       " 'from __future__ import absolute_import',\n",
       " 'from __future__ import division',\n",
       " 'from __future__ import print_function',\n",
       " 'from absl.testing import parameterized',\n",
       " 'import numpy as np',\n",
       " 'from tensorflow.python.data.ops import dataset_ops',\n",
       " 'from tensorflow.python.distribute import mirrored_strategy',\n",
       " 'from tensorflow.python.distribute import one_device_strategy',\n",
       " 'from tensorflow.python.eager import backprop',\n",
       " 'from tensorflow.python.eager import context',\n",
       " 'from tensorflow.python.framework import constant_op',\n",
       " 'from tensorflow.python.framework import dtypes',\n",
       " 'from tensorflow.python.framework import test_util',\n",
       " 'from tensorflow.python.keras import backend',\n",
       " 'from tensorflow.python.keras import layers',\n",
       " 'from tensorflow.python.keras import models',\n",
       " 'from tensorflow.python.keras import regularizers',\n",
       " 'from tensorflow.python.keras.engine import base_layer',\n",
       " '#from tensorflow.python.keras.layers import core',\n",
       " '#from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer',\n",
       " 'from tensorflow.python.keras.mixed_precision.experimental import policy',\n",
       " '#from tensorflow.python.keras.mixed_precision.experimental import test_util as mp_test_util',\n",
       " 'from tensorflow.python.keras.optimizer_v2 import gradient_descent',\n",
       " 'from tensorflow.python.ops import array_ops',\n",
       " 'from tensorflow.python.ops import math_ops',\n",
       " 'from tensorflow.python.ops import variables',\n",
       " 'from tensorflow.python.platform import test',\n",
       " 'from tensorflow.python.util import nest',\n",
       " 'class AssertTypeLayer(base_layer.Layer):',\n",
       " '  def __init__(self, assert_type=None, **kwargs):',\n",
       " '    self._assert_type = assert_type',\n",
       " '    super(AssertTypeLayer, self).__init__(**kwargs)',\n",
       " '  def assert_input_types(self, inputs):',\n",
       " '    if self._assert_type:',\n",
       " '      inputs_flattened = nest.flatten(inputs)',\n",
       " '      for inp in inputs_flattened:',\n",
       " '        assert inp.dtype.base_dtype == self._assert_type, (',\n",
       " \"            'Input tensor has type %s which does not match assert type %s' %\",\n",
       " '            (inp.dtype.name, self._assert_type.name))',\n",
       " 'class AddLayer(AssertTypeLayer):',\n",
       " '  def __init__(self, regularizer=None, use_operator=False, **kwargs):',\n",
       " '    self._regularizer = regularizer',\n",
       " '    self._use_operator = use_operator',\n",
       " '    super(AddLayer, self).__init__(**kwargs)',\n",
       " '  def build(self, _):',\n",
       " \"    self.v = self.add_weight('v', (), initializer='ones',\",\n",
       " '                             regularizer=self._regularizer)',\n",
       " '    self.built = True',\n",
       " '  def call(self, inputs):',\n",
       " '    self.assert_input_types(inputs)',\n",
       " '    assert inputs.dtype == self.v.dtype',\n",
       " '    return self._add(inputs, self.v)',\n",
       " '  def _add(self, x, y):',\n",
       " '    if self._use_operator:',\n",
       " '      return x + y',\n",
       " '    else:',\n",
       " '      return math_ops.add(x, y)',\n",
       " 'class AddLayerWithoutAutoCast(AddLayer):',\n",
       " '  def build(self, _):',\n",
       " '    dtype = self.dtype',\n",
       " \"    if dtype in ('float16', 'bfloat16'):\",\n",
       " \"      dtype = 'float32'\",\n",
       " \"    self.v = self.add_weight('v', (), initializer='ones', dtype=dtype,\",\n",
       " '                             experimental_autocast=False,',\n",
       " '                             regularizer=self._regularizer)',\n",
       " '    self.built = True',\n",
       " '  def call(self, inputs):',\n",
       " '    self.assert_input_types(inputs)',\n",
       " '    assert self.v.dtype in (dtypes.float32, dtypes.float64)',\n",
       " '    return self._add(inputs, math_ops.cast(self.v, inputs.dtype))',\n",
       " 'class IdentityRegularizer(regularizers.Regularizer):',\n",
       " '  def __call__(self, x):',\n",
       " '    assert x.dtype == dtypes.float32',\n",
       " '    return array_ops.identity(x)',\n",
       " 'def create_one_device_strategy():',\n",
       " \"  return one_device_strategy.OneDeviceStrategy('cpu:0')\",\n",
       " 'def create_mirrored_strategy():',\n",
       " '  if context.num_gpus() >= 1:',\n",
       " \"    return mirrored_strategy.MirroredStrategy(['cpu:0', 'gpu:0'])\",\n",
       " '  else:',\n",
       " \"    return mirrored_strategy.MirroredStrategy(['cpu:0'])\",\n",
       " 'TESTCASES = ({',\n",
       " \"    'testcase_name': 'base',\",\n",
       " \"    'strategy_fn': create_one_device_strategy\",\n",
       " '}, {',\n",
       " \"    'testcase_name': 'distribute',\",\n",
       " \"    'strategy_fn': create_mirrored_strategy\",\n",
       " '})',\n",
       " 'class KerasLayerTest(test.TestCase, parameterized.TestCase):',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_variables_in_float32(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '        y = layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '        self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(y), 2.)',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_layer_with_non_autocast_variable(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16)',\n",
       " '        y = layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " '        self.assertEqual(y.dtype, dtypes.float16)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(y), 2.)',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_layer_regularizer_runs_in_float32(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " '                         regularizer=IdentityRegularizer())',\n",
       " '        layer(x)',\n",
       " '        (regularizer_loss,) = layer.losses',\n",
       " '        self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '        layer = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                        regularizer=IdentityRegularizer())',\n",
       " '        layer(x)',\n",
       " '        (regularizer_loss,) = layer.losses',\n",
       " '        self.assertEqual(regularizer_loss.dtype, dtypes.float32)',\n",
       " '        self.evaluate(variables.global_variables_initializer())',\n",
       " '        self.assertEqual(self.evaluate(regularizer_loss), 1.)',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_passing_policy_to_layer(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope():',\n",
       " '      layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                       dtype=policy.Policy('infer_float32_vars'))\",\n",
       " '      self.assertEqual(layer.dtype, dtypes.float32)',\n",
       " '      layer(x)',\n",
       " '      self.assertEqual(layer.v.dtype, dtypes.float32)',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16,',\n",
       " \"                         dtype=policy.Policy('infer'))\",\n",
       " '        self.assertEqual(layer.dtype, None)',\n",
       " '        layer(x)',\n",
       " '        self.assertEqual(layer.v.dtype, dtypes.float16)',\n",
       " '        self.assertEqual(layer.dtype, dtypes.float16)',\n",
       " '  @parameterized.named_parameters(*TESTCASES)',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_gradient(self, strategy_fn):',\n",
       " '    x = constant_op.constant([1.], dtype=dtypes.float16)',\n",
       " '    with strategy_fn().scope() as strategy:',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        layer = AddLayer(assert_type=dtypes.float16)',\n",
       " '        def run_fn():',\n",
       " '          with backprop.GradientTape() as tape:',\n",
       " '            y = layer(x)',\n",
       " '            y /= strategy.num_replicas_in_sync',\n",
       " '          opt = gradient_descent.SGD(2 ** -14)',\n",
       " '          grad = tape.gradient(y, layer.v)',\n",
       " '          return opt.apply_gradients([(grad, layer.v)])',\n",
       " '        op = strategy.experimental_run(run_fn)',\n",
       " '        if not context.executing_eagerly():',\n",
       " '          self.evaluate(variables.global_variables_initializer())',\n",
       " '          self.evaluate(op)',\n",
       " '        self.assertEqual(self.evaluate(layer.v), 1 - 2 ** -14)',\n",
       " 'class KerasModelTest(test.TestCase, parameterized.TestCase):',\n",
       " '  @parameterized.named_parameters({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'operator',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"      'use_operator': True\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'regularizer',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"      'use_regularizer': True\",\n",
       " '  })',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '  def test_model(self, strategy_fn, use_operator=False, use_regularizer=False):',\n",
       " '    regularizer = IdentityRegularizer() if use_regularizer else None',\n",
       " '    with strategy_fn().scope():',\n",
       " \"      with policy.policy_scope('infer_float32_vars'):\",\n",
       " '        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '        layer = AddLayer(assert_type=dtypes.float16, use_operator=use_operator,',\n",
       " '                         regularizer=regularizer)',\n",
       " '        y = layer(x)',\n",
       " '        y = math_ops.cast(y, dtypes.float32)',\n",
       " '        model = models.Model(inputs=x, outputs=y)',\n",
       " '        def loss_fn(y_true, y_pred):',\n",
       " '          del y_true',\n",
       " '          return math_ops.reduce_mean(y_pred)',\n",
       " '        opt = gradient_descent.SGD(2 ** -14)',\n",
       " '        model.compile(opt, loss=loss_fn)',\n",
       " '      self.assertEqual(backend.eval(layer.v), 1)',\n",
       " '      x = np.ones((2, 1))',\n",
       " '      y = np.ones((2, 1))',\n",
       " '      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '      model.fit(dataset)',\n",
       " '      expected = 1 - 2 ** -14',\n",
       " '      if use_regularizer:',\n",
       " '        expected -= 2 ** -14',\n",
       " '      self.assertEqual(backend.eval(layer.v), expected)',\n",
       " '  @parameterized.named_parameters({',\n",
       " \"      'testcase_name': 'base',\",\n",
       " \"      'strategy_fn': create_one_device_strategy,\",\n",
       " '  }, {',\n",
       " \"      'testcase_name': 'distribute',\",\n",
       " \"      'strategy_fn': create_mirrored_strategy,\",\n",
       " '#  }, {',\n",
       " \"#      'testcase_name': 'loss_scaling',\",\n",
       " \"#      'strategy_fn': create_mirrored_strategy,\",\n",
       " \"#      'use_loss_scaling': True\",\n",
       " '  })',\n",
       " '  @test_util.run_in_graph_and_eager_modes',\n",
       " '#  def test_advanced_model(self, strategy_fn, use_loss_scaling=False):',\n",
       " '    strategy = strategy_fn()',\n",
       " '#    if use_loss_scaling:',\n",
       " '#      loss_scale = 8.',\n",
       " '    learning_rate = 2 ** -14',\n",
       " '    with strategy.scope():',\n",
       " \"      with policy.policy_scope(policy.Policy('infer_float32_vars')):\",\n",
       " '        x = layers.Input(shape=(), batch_size=2, dtype=dtypes.float16)',\n",
       " '        layer1 = AddLayer(assert_type=dtypes.float16,',\n",
       " '                          regularizer=IdentityRegularizer(), use_operator=True)',\n",
       " '        layer2 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                         use_operator=True)',\n",
       " '        layer3 = AddLayer(assert_type=dtypes.float16, use_operator=False)',\n",
       " '        layer4 = AddLayerWithoutAutoCast(assert_type=dtypes.float16,',\n",
       " '                                         regularizer=IdentityRegularizer(),',\n",
       " '                                         use_operator=False)',\n",
       " '        y = layer1(x)',\n",
       " '        y = layer2(y)',\n",
       " '        y = layer3(y)',\n",
       " '        y = layer4(y)',\n",
       " '#        if use_loss_scaling:',\n",
       " '#          expected_gradient = loss_scale / strategy.num_replicas_in_sync / 2',\n",
       " '#          identity_with_grad_check_fn = (',\n",
       " '#              mp_test_util.create_identity_with_grad_check_fn(',\n",
       " '#                  expected_dtype=dtypes.float16,',\n",
       " '#                  expected_gradient=[expected_gradient] * 2))',\n",
       " '#          y = core.Lambda(identity_with_grad_check_fn)(y)',\n",
       " '        y = math_ops.cast(y, dtypes.float32)',\n",
       " '        model = models.Model(inputs=x, outputs=y)',\n",
       " '        def loss_fn(y_true, y_pred):',\n",
       " '          self.assertEqual(y_true.dtype, dtypes.float32)',\n",
       " '          self.assertEqual(y_pred.dtype, dtypes.float32)',\n",
       " '          return math_ops.reduce_mean(y_pred)',\n",
       " '        opt = gradient_descent.SGD(learning_rate)',\n",
       " '#        if use_loss_scaling:',\n",
       " '#          opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)',\n",
       " '        model.compile(opt, loss=loss_fn)',\n",
       " '      x = np.ones((2, 1))',\n",
       " '      y = np.ones((2, 1))',\n",
       " '      dataset = dataset_ops.Dataset.from_tensor_slices((x, y)).batch(2)',\n",
       " '      model.fit(dataset)',\n",
       " '      for layer in (layer1, layer2, layer3, layer4):',\n",
       " '        if layer.losses:',\n",
       " '          self.assertEqual(backend.eval(layer.v), 1 - 2 * learning_rate)',\n",
       " '        else:',\n",
       " '          self.assertEqual(backend.eval(layer.v), 1 - learning_rate)',\n",
       " \"if __name__ == '__main__':\",\n",
       " '  test.main()']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_diff_file(origin_file, parent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
